{
  "applicationName": "TURBINE_INTERNAL",
  "jsonSpecification": "{\r\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\r\n    \"title\": \"DQ Test\",\r\n    \"description\": \"DQ test\",\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \"DQ_MODULE_PATH\": {\r\n            \"title\": \"DQ_MODULE_PATH\",\r\n            \"description\": \"DQ_MODULE_PATH\",\r\n            \"default\": \"/unrefined/cloudpanel-test-unref/test/dvm/\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"UNREFINED_PATH\": {\r\n            \"title\": \"UNREFINED_PATH\",\r\n            \"description\": \"UNREFINED_PATH\",\r\n            \"default\": \"/unrefined/cloudpanel-test-unref/test/ADW/adw_30DEC/\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"SRCE_SYS_ID\": {\r\n            \"title\": \"SRCE_SYS_ID\",\r\n            \"description\": \"SRCE_SYS_ID\",\r\n            \"default\": 4,\r\n            \"type\": \"integer\"\r\n        },\r\n        \"TIER2_CATEG_ID\": {\r\n            \"title\": \"TIER2_CATEG_ID\",\r\n            \"description\": \"TIER2_CATEG_ID\",\r\n            \"default\": \"ALL\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CNTRT_ID\": {\r\n            \"title\": \"CNTRT_ID\",\r\n            \"description\": \"CNTRT_ID\",\r\n            \"default\": 615,\r\n            \"type\": \"integer\"\r\n        },\r\n        \"DQ_MODULE_REPORT_PATH\": {\r\n            \"title\": \"DQ_MODULE_REPORT_PATH\",\r\n            \"description\": \"DQ_MODULE_REPORT_PATH\",\r\n            \"default\": \"/unrefined/cloudpanel-test-unref/test/dvm/file_struct/reports/\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ1\": {\r\n            \"title\": \"CHK_DQ1\",\r\n            \"description\": \"CHK_DQ1\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ2\": {\r\n            \"title\": \"CHK_DQ2\",\r\n            \"description\": \"CHK_DQ2\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ3\": {\r\n            \"title\": \"CHK_DQ3\",\r\n            \"description\": \"CHK_DQ3\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ4\": {\r\n            \"title\": \"CHK_DQ4\",\r\n            \"description\": \"CHK_DQ4\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ5\": {\r\n            \"title\": \"CHK_DQ5\",\r\n            \"description\": \"CHK_DQ5\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ6\": {\r\n            \"title\": \"CHK_DQ6\",\r\n            \"description\": \"CHK_DQ6\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ7\": {\r\n            \"title\": \"CHK_DQ7\",\r\n            \"description\": \"CHK_DQ7\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ8\": {\r\n            \"title\": \"CHK_DQ8\",\r\n            \"description\": \"CHK_DQ8\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ9\": {\r\n            \"title\": \"CHK_DQ9\",\r\n            \"description\": \"CHK_DQ9\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ10\": {\r\n            \"title\": \"CHK_DQ10\",\r\n            \"description\": \"CHK_DQ10\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ11\": {\r\n            \"title\": \"CHK_DQ11\",\r\n            \"description\": \"CHK_DQ11\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ12\": {\r\n            \"title\": \"CHK_DQ12\",\r\n            \"description\": \"CHK_DQ12\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ13\": {\r\n            \"title\": \"CHK_DQ13\",\r\n            \"description\": \"CHK_DQ13\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ14\": {\r\n            \"title\": \"CHK_DQ14\",\r\n            \"description\": \"CHK_DQ14\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ15\": {\r\n            \"title\": \"CHK_DQ15\",\r\n            \"description\": \"CHK_DQ15\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ16\": {\r\n            \"title\": \"CHK_DQ16\",\r\n            \"description\": \"CHK_DQ16\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ17\": {\r\n            \"title\": \"CHK_DQ17\",\r\n            \"description\": \"CHK_DQ17\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ18\": {\r\n            \"title\": \"CHK_DQ18\",\r\n            \"description\": \"CHK_DQ18\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ19\": {\r\n            \"title\": \"CHK_DQ19\",\r\n            \"description\": \"CHK_DQ19\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"CHK_DQ20\": {\r\n            \"title\": \"CHK_DQ20\",\r\n            \"description\": \"CHK_DQ20\",\r\n            \"default\": \"false\",\r\n            \"type\": \"string\"\r\n        }\r\n   },\r\n    \"required\": [],\r\n    \"configurable\": [\"DQ_MODULE_PATH\", \"UNREFINED_PATH\", \"SRCE_SYS_ID\",\"TIER2_CATEG_ID\",\"DQ_MODULE_REPORT_PATH\",\"CHK_DQ1\",\"CHK_DQ2\",\"CHK_DQ3\",\"CHK_DQ4\",\"CHK_DQ5\",\"CHK_DQ6\",\"CHK_DQ7\",\"CHK_DQ8\",\"CHK_DQ9\",\"CHK_DQ10\",\"CHK_DQ11\",\"CHK_DQ12\",\"CHK_DQ13\",\"CHK_DQ14\",\"CHK_DQ15\",\"CHK_DQ16\",\"CHK_DQ17\",\"CHK_DQ18\",\"CHK_DQ19\",\"CHK_DQ20\"]\r\n}",
  "nodes": [
    {
      "operationName": "DQ3 - Negative fact values - Step 1",
      "jsonSpecification": "{\n  \"active\": \"<<CHK_DQ3>>\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_mmc_fact = dict_all_dfs['df_fact_extrn'][\\\"df_object\\\"]\\n\\ncols = df_mmc_fact.columns\\n\\nfrom pyspark.sql.functions import lit, col\\ndf_dq3 = spark.createDataFrame(cols,\\\"string\\\").withColumnRenamed(\\\"value\\\", \\\"column\\\").withColumn('kpi_type', lit('Threshold Validation KPI')).withColumn('param_1', lit(0)).withColumn('param_2', lit('')).withColumn('fail_on_error', lit('false')).withColumn('check_description', lit('Negative fact values')).withColumn('target', lit(100))\\n\\ndf_dq3 = df_dq3.select('column', 'kpi_type',  'param_1', 'param_2', 'fail_on_error', 'check_description', 'target').filter(~col('column').like('%id%'))\\n\\ndict_all_dfs['df_dq3'] = {\\\"df_object\\\" :df_dq3}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_fact_extrn\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq3\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "DQ3 - Publishing KPI in CSV - Step 3",
      "predecessorName": "DQ3 - Negative fact values - Step 1",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"unrefined/cloudpanel-test-unref/test/dvm/file_struct/DQ3_Negative_value/<<PROCESS_RUN_KEY>>.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dq3\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ3 - Negative fact values",
      "predecessorName": "DQ3 - Publishing KPI in CSV - Step 3",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"unrefined/cloudpanel-test-unref/test/dvm/file_struct/DQ3_Negative_value/<<PROCESS_RUN_KEY>>.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dq3\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq3_chk\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ2 - Unexpected backdata difference v1",
      "predecessorName": "DQ3 - Negative fact values",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_tier2_dqm_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['df_join_mkt_dim_w_extrn_id'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['df_prod_dim_lkp_vw_w_extrn_id'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\n\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/busi_val/processing_files/mm_dvm_run_strct_lvl_plc/')\\ndf_mm_dvm_run_strct_lvl_plc.createOrReplaceTempView('MM_DVM_RUN_STRCT_LVL_PLC')\\n\\ndf_tier2_dqm_calc_index.createOrReplaceTempView('TIER2_DQM_CALC_INDEX')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\ndf_time_pd = spark.read.format('parquet').load('/mnt/unrefined/cloudpanel-test-unref/test/ADW/adw_30DEC/MM_TIME_PERD_FDIM_VW')\\ndf_time_pd.createOrReplaceTempView('mm_time_perd_fdim')\\n\\nrun_id =<<PROCESS_RUN_KEY>>\\n\\ndq2_query = f\\\"\\\"\\\"SELECT * FROM (\\nWITH fct_detail AS (\\n   SELECT * FROM TIER2_DQM_CALC_INDEX fct\\n       JOIN MM_DVM_RUN_STRCT_LVL_PLC dvm_strct_lvl\\n          ON     fct.prod_lvl = abs(dvm_strct_lvl.strct_lvl_id)\\n             AND fct.run_id = dvm_strct_lvl.run_id\\n\\t\\t\\t and dvm_strct_lvl.run_id = {run_id}\\n   WHERE     PD_TIME_PERD_IND = 'Y'\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPD_SU_PCT) >= IPD_CHECK_VAL)\\n            OR (ABS (IPD_LC_PCT) >= IPD_CHECK_VAL))\\n\\t\\t\\t),\\n fct\\n     AS (SELECT 'IPD' index_type,\\n                'Volume Sales MSU' measure_name,\\n                ipd_su_pct \\n                index_value,\\n                SALES_MSU_QTY_PD Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPD_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IPD_SU_PCT) >= IPD_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IPD' index_type,\\n                'Value Sales MLC' measure_name,\\n                ipd_lc_pct index_value,\\n                SALES_MLC_AMT_PD Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPD_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IPD_LC_PCT) >= IPD_CHECK_VAL)           \\nSELECT \\n\\t   TIME_PERD.TIME_PERD_LONG_NAME,\\n\\t   nvl(PROD.SHORT_PROD_DESC_TXT,PROD.LONG_PROD_DESC_TXT) PROD_LONG_NAME,\\n\\t   mkt_map.MKT_NAME as MKT_NAME,\\n\\t   mkt_map.EXTRN_MKT_ID as EXTRN_MKT_ID,\\n       INDEX_TYPE,\\n       CHECK_VAL TOLERANCE_LEVEL,\\n       MEASURE_NAME,\\n       ROUND(INDEX_VALUE,5) INDEX_VALUE,\\n       PREVIOUS_VALUE,\\n       CURRENT_VALUE\\n  FROM fct\\n       JOIN mm_time_perd_fdim time_perd\\n          ON fct.time_perd_id = time_perd.time_perd_id\\n       JOIN TIER2_MKT_MTRLZ_TBL mkt_map ON mkt_map.mkt_skid = fct.mkt_skid\\n             JOIN TIER2_PROD_MTRLZ_TBL prod on prod.prod_skid = fct.prod_skid\\n\\t\\t\\t )\\\"\\\"\\\"\\n\\ndf_dq2_1 = spark.sql(dq2_query)\\n\\n######### DQ1\\n\\nquery3 = \\\"\\\"\\\" (\\nSELECT fct_val1.*,dvm_strct_lvl1.IPP_CHECK_VAL,dvm_strct_lvl1.IYA_CHECK_VAL\\n  FROM  TIER2_DQM_CALC_INDEX  fct_val1\\n       JOIN MM_DVM_RUN_STRCT_LVL_PLC dvm_strct_lvl1\\n             ON dvm_strct_lvl1.run_id = fct_val1.run_id\\n             and abs(dvm_strct_lvl1.STRCT_LVL_ID) = fct_val1.PROD_LVL\\n WHERE     NEW_TIME_PERD_IND = 'Y' AND fct_val1.run_id = dvm_strct_lvl1.run_id\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPP_SU_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_SU_PCT) >= IYA_CHECK_VAL)\\n            OR (ABS (IPP_LC_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_LC_PCT) >= IYA_CHECK_VAL))  \\n)\\\"\\\"\\\"\\n\\nfct_detail1 = spark.sql(query3)\\nfct_detail1.createOrReplaceTempView('fct_detail1')\\n\\nquery4 = \\\"\\\"\\\"(SELECT 'IPP' index_type,\\n                'Volume Sales MSU' measure_name,\\n                ipp_su_pct index_value,\\n                SALES_MSU_QTY_PP Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IPP_SU_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IPP' index_type,\\n                'Value Sales MLC' measure_name,\\n                ipp_LC_pct,\\n                SALES_MLC_AMT_PP Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IPP_LC_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Volume Sales MSU' measure_name,\\n                iya_su_pct index_value,\\n                SALES_MSU_QTY_YA Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IYA_SU_PCT) >= IYA_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Value Sales MLC' measure_name,\\n                iya_LC_pct index_value,\\n                SALES_MLC_AMT_YA Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IYA_LC_PCT) >= IYA_CHECK_VAL)\\\"\\\"\\\"\\n\\nfct1 = spark.sql(query4)\\nfct1.createOrReplaceTempView('fct1')\\n\\nquery5 = \\\"\\\"\\\"SELECT  index_type, measure_name, index_value, Previous_value, Current_Value, time_perd_id, fct1.mkt_skid, fct1.prod_skid, CHECK_VAL\\n  FROM fct1\\n       JOIN mm_time_perd_fdim time_perd1\\n          ON fct1.time_perd_id = time_perd1.time_perd_id\\n       JOIN TIER2_MKT_MTRLZ_TBL mkt_map1 ON mkt_map1.mkt_skid = fct1.mkt_skid\\n             JOIN TIER2_PROD_MTRLZ_TBL prod1 on prod1.prod_skid = fct1.prod_skid \\\"\\\"\\\"\\ndf_dq1 = spark.sql(query5)\\n\\ndict_all_dfs['df_dq1'] = {\\\"df_object\\\" :df_dq1}\\ndict_all_dfs['df_dq2_1'] = {\\\"df_object\\\" :df_dq2_1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_calc_index\"\n    },\n    {\n      \"name\": \"df_join_mkt_dim_w_extrn_id\"\n    },\n    {\n      \"name\": \"df_prod_dim_lkp_vw_w_extrn_id\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq2_1\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_dq1\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ2 - Unexpected backdata difference v2",
      "predecessorName": "DQ2 - Unexpected backdata difference v1",
      "jsonSpecification": "{\n  \"active\": \"<<CHK_DQ2>>\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_tier2_dqm_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['df_join_mkt_dim_w_extrn_id'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['df_prod_dim_lkp_vw_w_extrn_id'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\n\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/<<PUBLISH_PATH>>/mm_dvm_run_strct_lvl_plc/')\\ndf_mm_dvm_run_strct_lvl_plc.createOrReplaceTempView('MM_DVM_RUN_STRCT_LVL_PLC')\\n\\ndf_tier2_dqm_calc_index.createOrReplaceTempView('TIER2_DQM_CALC_INDEX')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\ndf_time_pd = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_FDIM_VW')\\ndf_time_pd.createOrReplaceTempView('mm_time_perd_fdim')\\n\\nrun_id =<<PROCESS_RUN_KEY>>\\n\\ndq2_query = f\\\"\\\"\\\"SELECT * FROM (\\nWITH fct_detail AS (\\n   SELECT * FROM TIER2_DQM_CALC_INDEX fct\\n       JOIN MM_DVM_RUN_STRCT_LVL_PLC dvm_strct_lvl\\n          ON     fct.prod_lvl = abs(dvm_strct_lvl.strct_lvl_id)\\n             AND fct.run_id = dvm_strct_lvl.run_id\\n\\t\\t\\t and dvm_strct_lvl.run_id = {run_id}\\n   WHERE     PD_TIME_PERD_IND = 'Y'\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPD_SU_PCT) >= IPD_CHECK_VAL)\\n            OR (ABS (IPD_LC_PCT) >= IPD_CHECK_VAL))\\n\\t\\t\\t),\\n fct\\n     AS (SELECT 'IPD' index_type,\\n                'Volume Sales MSU' measure_name,\\n                ipd_su_pct \\n                index_value,\\n                SALES_MSU_QTY_PD Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPD_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IPD_SU_PCT) >= IPD_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IPD' index_type,\\n                'Value Sales MLC' measure_name,\\n                ipd_lc_pct index_value,\\n                SALES_MLC_AMT_PD Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPD_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IPD_LC_PCT) >= IPD_CHECK_VAL)           \\nSELECT \\n\\t   TIME_PERD.TIME_PERD_LONG_NAME as TIME_PERD,\\n\\t   mkt_map.MKT_NAME as MKT_NAME,\\n\\t   nvl(PROD.SHORT_PROD_DESC_TXT,PROD.LONG_PROD_DESC_TXT) PROD_LONG_NAME,\\n\\t   INDEX_TYPE,\\n\\t   CHECK_VAL TOLERANCE_LEVEL,\\n       MEASURE_NAME,\\n       ROUND(INDEX_VALUE,5) INDEX_VALUE,\\n       PREVIOUS_VALUE,\\n       CURRENT_VALUE\\n  FROM fct\\n       JOIN mm_time_perd_fdim time_perd\\n          ON fct.time_perd_id = time_perd.time_perd_id\\n       JOIN TIER2_MKT_MTRLZ_TBL mkt_map ON mkt_map.mkt_skid = fct.mkt_skid\\n             JOIN TIER2_PROD_MTRLZ_TBL prod on prod.prod_skid = fct.prod_skid\\n\\t\\t\\t )\\\"\\\"\\\"\\n\\ndf_dq2_1 = spark.sql(dq2_query)\\n\\ndf_output_dict['df_dq2_1'] = df_dq2_1\\ndict_all_dfs['df_dq2_1'] = {\\\"df_object\\\" :df_dq2_1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_calc_index\"\n    },\n    {\n      \"name\": \"df_join_mkt_dim_w_extrn_id\"\n    },\n    {\n      \"name\": \"df_prod_dim_lkp_vw_w_extrn_id\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq2_1\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "DQ1",
      "predecessorName": "DQ2 - Unexpected backdata difference v2",
      "jsonSpecification": "{\n  \"active\": \"<<CHK_DQ1>>\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_tier2_dqm_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['df_join_mkt_dim_w_extrn_id'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['df_prod_dim_lkp_vw_w_extrn_id'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\n\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/<<PUBLISH_PATH>>/mm_dvm_run_strct_lvl_plc/')\\ndf_mm_dvm_run_strct_lvl_plc.createOrReplaceTempView('MM_DVM_RUN_STRCT_LVL_PLC')\\n\\ndf_tier2_dqm_calc_index.createOrReplaceTempView('TIER2_DQM_CALC_INDEX')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\ndf_time_pd = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_FDIM_VW')\\ndf_time_pd.createOrReplaceTempView('mm_time_perd_fdim')\\n\\nrun_id =<<PROCESS_RUN_KEY>>\\n\\n\\nquery3 = \\\"\\\"\\\" (\\nSELECT fct_val1.*,dvm_strct_lvl1.IPP_CHECK_VAL,dvm_strct_lvl1.IYA_CHECK_VAL\\n  FROM  TIER2_DQM_CALC_INDEX  fct_val1\\n       JOIN MM_DVM_RUN_STRCT_LVL_PLC dvm_strct_lvl1\\n             ON dvm_strct_lvl1.run_id = fct_val1.run_id\\n             and abs(dvm_strct_lvl1.STRCT_LVL_ID) = fct_val1.PROD_LVL\\n WHERE     NEW_TIME_PERD_IND = 'Y' AND fct_val1.run_id = dvm_strct_lvl1.run_id\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPP_SU_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_SU_PCT) >= IYA_CHECK_VAL)\\n            OR (ABS (IPP_LC_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_LC_PCT) >= IYA_CHECK_VAL))\\n)\\\"\\\"\\\"\\n\\nfct_detail1 = spark.sql(query3)\\nfct_detail1.createOrReplaceTempView('fct_detail1')\\n\\nquery4 = \\\"\\\"\\\"(SELECT 'IPP' index_type,\\n                'Volume Sales MSU' MEASURE_NAME,\\n                ipp_su_pct index_value,\\n                SALES_MSU_QTY_PP Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IPP_SU_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IPP' index_type,\\n                'Value Sales MLC' MEASURE_NAME,\\n                ipp_LC_pct,\\n                SALES_MLC_AMT_PP Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IPP_LC_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Volume Sales MSU' MEASURE_NAME,\\n                iya_su_pct index_value,\\n                SALES_MSU_QTY_YA Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IYA_SU_PCT) >= IYA_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Value Sales MLC' MEASURE_NAME,\\n                iya_LC_pct index_value,\\n                SALES_MLC_AMT_YA Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IYA_LC_PCT) >= IYA_CHECK_VAL)\\\"\\\"\\\"\\n\\nfct1 = spark.sql(query4)\\nfct1.createOrReplaceTempView('fct1')\\n\\nquery5 = \\\"\\\"\\\"SELECT  time_perd1.TIME_PERD_LONG_NAME as TIME_PERD, mkt_map1.MKT_NAME as MKT_NAME, nvl(prod1.SHORT_PROD_DESC_TXT,prod1.LONG_PROD_DESC_TXT) PROD_LONG_NAME, INDEX_TYPE, CHECK_VAL TOLERANCE_LEVEL, MEASURE_NAME, ROUND(INDEX_VALUE,5) INDEX_VALUE, PREVIOUS_VALUE, CURRENT_VALUE\\n  FROM fct1\\n       JOIN mm_time_perd_fdim time_perd1\\n          ON fct1.time_perd_id = time_perd1.time_perd_id\\n       JOIN TIER2_MKT_MTRLZ_TBL mkt_map1 ON mkt_map1.mkt_skid = fct1.mkt_skid\\n             JOIN TIER2_PROD_MTRLZ_TBL prod1 on prod1.prod_skid = fct1.prod_skid \\\"\\\"\\\"\\ndf_dq1 = spark.sql(query5)\\n\\ndf_output_dict['df_dq1'] = df_dq1\\ndict_all_dfs['df_dq1'] = {\\\"df_object\\\" :df_dq1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_calc_index\"\n    },\n    {\n      \"name\": \"df_join_mkt_dim_w_extrn_id\"\n    },\n    {\n      \"name\": \"df_prod_dim_lkp_vw_w_extrn_id\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq1\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "[GEN] - Business Validation Reports dataframe creation",
      "predecessorName": "DQ1",
      "jsonSpecification": "{\n  \"active\": \"<<BUSINESS_DVM>>\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"#Create empty dataframe\\nspark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_dq1 = dict_all_dfs['df_dq1'][\\\"df_object\\\"]\\ndf_dq2 = dict_all_dfs['df_dq2_1'][\\\"df_object\\\"]\\ndf_dq3 = dict_all_dfs['df_dq3'][\\\"df_object\\\"]\\n\\nchk_dq1 = ('<<CHK_DQ1>>' == 'true')\\nchk_dq2 = ('<<CHK_DQ2>>' == 'true')\\nchk_dq3 = ('<<CHK_DQ3>>' == 'true')\\n\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.window import Window\\n\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\nif chk_dq1:\\n  df_dq1 = df_dq1.withColumn('DQ', lit('Unexpected change vs previous period or year ago')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq1.select('row_id','DQ','TIME_PERD', 'MKT_NAME', 'PROD_LONG_NAME', 'INDEX_TYPE', 'TOLERANCE_LEVEL', 'MEASURE_NAME', 'INDEX_VALUE', 'PREVIOUS_VALUE', 'CURRENT_VALUE')\\nelse:\\n  df_empty\\n  \\nif chk_dq2:\\n  df_dq2 = df_dq1.withColumn('DQ', lit('Unexpected backdata difference')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq2.select('row_id','DQ','TIME_PERD', 'MKT_NAME', 'PROD_LONG_NAME', 'INDEX_TYPE', 'TOLERANCE_LEVEL', 'MEASURE_NAME', 'INDEX_VALUE', 'PREVIOUS_VALUE', 'CURRENT_VALUE')\\nelse:\\n  df_empty\\n  \\n\\nbvr_elig = chk_dq1 | chk_dq2 | chk_dq3\\n\\nif bvr_elig:\\n  # df_combine = df_dq1.join(df_dq2, df_dq1.row_id == df_dq2.row_id ,'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n  df_combine = df_dq1.unionByName(df_dq2, True).drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\nelse:\\n  df_combine = df_empty\\n\\n# KPI information\\ndq1_columns = ['DQ','TIME_PERD', 'MKT_NAME', 'PROD_LONG_NAME', 'INDEX_TYPE', 'TOLERANCE_LEVEL', 'MEASURE_NAME', 'INDEX_VALUE', 'PREVIOUS_VALUE', 'CURRENT_VALUE']\\ndq2_columns = ['DQ','TIME_PERD', 'MKT_NAME', 'PROD_LONG_NAME', 'INDEX_TYPE', 'TOLERANCE_LEVEL', 'MEASURE_NAME', 'INDEX_VALUE', 'PREVIOUS_VALUE', 'CURRENT_VALUE']\\n\\ncombined_cols = ['row_id']\\ndata = []\\n\\nif chk_dq1 | chk_dq2:\\n  [combined_cols.append(i) for i in dq1_columns]\\n  dq1_val = ('DQ', 'SQL Validation KPI', \\\"DQ IS NULL\\\", '', 'false', 'Unexpected change vs previous period or year ago', 100 )\\n  data.append(dq1_val)\\n# if chk_dq2:\\n#   [combined_cols.append(i) for i in dq2_columns]\\n#   dq2_val = ('DQ', 'SQL Validation KPI', \\\"DQ IS NULL\\\", '', 'false', 'Unexpected backdata difference', 100 )\\n#   data.append(dq2_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\nif bvr_elig:\\n  df_bvr = spark.createDataFrame(data, schema_for_kpi)\\nelse:\\n  df_bvr = df_empty\\n\\n\\n# bvr_elig.write.format('csv').mode('overwrite').save('/mnt/bf/unrefined/cloudpanel-test-unref/test/dvm/KPI/bvr.csv')\\n#BVR Check Eligibility\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(bvr_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"bvr\\\",BooleanType(),True),\\n    StructField(\\\"bvr_elig\\\",StringType(),True)\\n  ])\\n \\ndf_bvr_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_bvr_eligibility = df_bvr_eligibility.withColumn('bvr_elig', when(col('bvr'), lit('true')).otherwise(lit('false')))\\n\\ndf_output_dict['df_combine_bvr'] = df_combine\\ndict_all_dfs['df_combine_bvr'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_bvr'] = df_bvr\\ndict_all_dfs['df_bvr'] = {\\\"df_object\\\" :df_bvr}\\ndf_output_dict['df_bvr_eligibility'] = df_bvr_eligibility\\ndict_all_dfs['df_bvr_eligibility'] = {\\\"df_object\\\" :df_bvr_eligibility}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dq1\"\n    },\n    {\n      \"name\": \"df_dq2_1\"\n    },\n    {\n      \"name\": \"df_dq3\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_bvr\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_bvr\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_bvr_eligibility\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "[FP] - Business Validation KPI",
      "predecessorName": "[GEN] - Business Validation Reports dataframe creation",
      "jsonSpecification": "{\n  \"active\": \"<<BUSINESS_DVM>>\",\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_bvr.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_bvr\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": true
    },
    {
      "operationName": "DQ1 - Step1",
      "predecessorName": "[FP] - Business Validation KPI",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_tier2_dqm_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['df_join_mkt_dim_w_extrn_id'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['df_prod_dim_lkp_vw_w_extrn_id'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\n\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/busi_val/processing_files/mm_dvm_run_strct_lvl_plc/')\\ndf_mm_dvm_run_strct_lvl_plc.createOrReplaceTempView('MM_DVM_RUN_STRCT_LVL_PLC')\\n\\ndf_tier2_dqm_calc_index.createOrReplaceTempView('TIER2_DQM_CALC_INDEX')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\ndf_time_pd = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_FDIM_VW')\\ndf_time_pd.createOrReplaceTempView('mm_time_perd_fdim')\\n\\nrun_id =<<PROCESS_RUN_KEY>>\\n\\n\\nquery3 = \\\"\\\"\\\" (\\nSELECT fct_val1.*,dvm_strct_lvl1.IPP_CHECK_VAL,dvm_strct_lvl1.IYA_CHECK_VAL\\n  FROM  TIER2_DQM_CALC_INDEX  fct_val1\\n       JOIN MM_DVM_RUN_STRCT_LVL_PLC dvm_strct_lvl1\\n             ON dvm_strct_lvl1.run_id = fct_val1.run_id\\n             and abs(dvm_strct_lvl1.STRCT_LVL_ID) = fct_val1.PROD_LVL\\n WHERE     NEW_TIME_PERD_IND = 'Y' AND fct_val1.run_id = dvm_strct_lvl1.run_id\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPP_SU_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_SU_PCT) >= IYA_CHECK_VAL)\\n            OR (ABS (IPP_LC_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_LC_PCT) >= IYA_CHECK_VAL))\\n)\\\"\\\"\\\"\\n\\nfct_detail1 = spark.sql(query3)\\n\\n\\ndict_all_dfs['fct_detail1'] = {\\\"df_object\\\" :fct_detail1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_calc_index\"\n    },\n    {\n      \"name\": \"df_join_mkt_dim_w_extrn_id\"\n    },\n    {\n      \"name\": \"df_prod_dim_lkp_vw_w_extrn_id\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"fct_detail1\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ1 - Step2",
      "predecessorName": "DQ1 - Step1",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\nrun_id =<<PROCESS_RUN_KEY>>\\n\\nfct_detail1 = dict_all_dfs['fct_detail1'][\\\"df_object\\\"]\\nfct_detail1.createOrReplaceTempView('fct_detail1')\\nquery4 = \\\"\\\"\\\"(SELECT 'IPP' index_type,\\n                'Volume Sales MSU' measure_name,\\n                ipp_su_pct index_value,\\n                SALES_MSU_QTY_PP Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IPP_SU_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IPP' index_type,\\n                'Value Sales MLC' measure_name,\\n                ipp_LC_pct,\\n                SALES_MLC_AMT_PP Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IPP_LC_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Volume Sales MSU' measure_name,\\n                iya_su_pct index_value,\\n                SALES_MSU_QTY_YA Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IYA_SU_PCT) >= IYA_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Value Sales MLC' measure_name,\\n                iya_LC_pct index_value,\\n                SALES_MLC_AMT_YA Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail1\\n           WHERE ABS (IYA_LC_PCT) >= IYA_CHECK_VAL)\\\"\\\"\\\"\\n\\nfct1 = spark.sql(query4)\\n\\ndict_all_dfs['fct1'] = {\\\"df_object\\\" :fct1}\\n\\n\\n\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"fct_detail1\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"fct1\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ1 - Step3",
      "predecessorName": "DQ1 - Step2",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_tier2_dqm_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['df_join_mkt_dim_w_extrn_id'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['df_prod_dim_lkp_vw_w_extrn_id'][\\\"df_object\\\"]\\nfct1 = dict_all_dfs['fct1'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\n\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/busi_val/processing_files/mm_dvm_run_strct_lvl_plc/')\\ndf_mm_dvm_run_strct_lvl_plc.createOrReplaceTempView('MM_DVM_RUN_STRCT_LVL_PLC')\\n\\ndf_tier2_dqm_calc_index.createOrReplaceTempView('TIER2_DQM_CALC_INDEX')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\ndf_time_pd = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_FDIM_VW')\\ndf_time_pd.createOrReplaceTempView('mm_time_perd_fdim')\\n\\nrun_id =<<PROCESS_RUN_KEY>>\\n\\n\\n\\nfct1.createOrReplaceTempView('fct1')\\n\\nquery5 = \\\"\\\"\\\"SELECT  index_type, measure_name, index_value, Previous_value, Current_Value, fct1.time_perd_id, fct1.mkt_skid, fct1.prod_skid, CHECK_VAL\\n  FROM fct1\\n       JOIN mm_time_perd_fdim time_perd1\\n          ON fct1.time_perd_id = time_perd1.time_perd_id\\n       JOIN TIER2_MKT_MTRLZ_TBL mkt_map1 ON mkt_map1.mkt_skid = fct1.mkt_skid\\n             JOIN TIER2_PROD_MTRLZ_TBL prod1 on prod1.prod_skid = fct1.prod_skid \\\"\\\"\\\"\\ndf_dq1 = spark.sql(query5)\\n\\ndict_all_dfs['df_dq1'] = {\\\"df_object\\\" :df_dq1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"fct1\"\n    },\n    {\n      \"name\": \"df_calc_index\"\n    },\n    {\n      \"name\": \"df_join_mkt_dim_w_extrn_id\"\n    },\n    {\n      \"name\": \"df_prod_dim_lkp_vw_w_extrn_id\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq1\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ2 - Unexpected backdata difference - Step1",
      "predecessorName": "DQ1 - Step3",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_tier2_dqm_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/busi_val/processing_files/mm_dvm_run_strct_lvl_plc/')\\n\\ndf_dq2 = df_tier2_dqm_calc_index.join(df_mm_dvm_run_strct_lvl_plc, (col('prod_lvl') == abs(col('strct_lvl_id'))) & ( df_tier2_dqm_calc_index.run_id == df_mm_dvm_run_strct_lvl_plc.RUN_ID ) & ( df_mm_dvm_run_strct_lvl_plc.RUN_ID == run_id ) ,\\\"inner\\\").select(df_tier2_dqm_calc_index[\\\"*\\\"], df_mm_dvm_run_strct_lvl_plc[\\\"ABSLT_THSHD_VAL\\\"], df_mm_dvm_run_strct_lvl_plc[\\\"IPP_CHECK_VAL\\\"], df_mm_dvm_run_strct_lvl_plc[\\\"IYA_CHECK_VAL\\\"],df_mm_dvm_run_strct_lvl_plc[\\\"IPD_CHECK_VAL\\\"])\\n\\ndf_dq2 = df_dq2.createOrReplaceTempView(\\\"DQ2_TEMP\\\")\\n\\nquery2 = \\\"\\\"\\\" SELECT * FROM DQ2_TEMP WHERE ( PD_TIME_PERD_IND = 'Y'\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPD_SU_PCT) >= IPD_CHECK_VAL)\\n            OR (ABS (IPD_LC_PCT) >= IPD_CHECK_VAL)) )\\\"\\\"\\\"\\n\\ndf_dq2_1 = spark.sql(query2)\\n\\ndict_all_dfs['df_dq2_1'] = {\\\"df_object\\\" :df_dq2_1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_calc_index\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq2_1\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "update delivery status - phase id 6 status id 2",
      "predecessorName": "DQ2 - Unexpected backdata difference - Step1",
      "jsonSpecification": "{\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\nrows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 6, 1, 2, <<PROCESS_RUN_KEY>>]]\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"jdbc\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"dbtable\\\", \\\"adwgp_mm.MM_DLVRY_RUN_LKP\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").mode(\\\"append\\\").save()\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dummy\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "DQ2 - Unexpected backdata difference",
      "predecessorName": "update delivery status - phase id 6 status id 2",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using form\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dq2_1\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq2_chk\",\n      \"cache\": \"none\"\n    }\n  ],\n  \"kpiTypes\": [\n    {\n      \"KPIName\": \"Data Existence KPI\",\n      \"columnName\": \"run_id\",\n      \"failOnError\": \"false\",\n      \"checkDescription\": \"Redelivered sales data are within the tolerance level of the previously delivered data\",\n      \"target\": \"100\",\n      \"saveDetailedValidationResults\": \"when failed rows occurs\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ1 - Unexpected change vs. previous period or year ago - Step1",
      "predecessorName": "DQ2 - Unexpected backdata difference",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_calc_index = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['df_calc_index'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import lit, col, abs\\n\\ndf_mm_dvm_run_strct_lvl_plc = spark.read.format('delta').load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/busi_val/processing_files/mm_dvm_run_strct_lvl_plc/')\\ndf_time_pd = spark.read.format('parquet').load('/mnt/unrefined/cloudpanel-test-unref/test/ADW/adw_30DEC/MM_TIME_PERD_FDIM_VW')\\n\\ndf_mm_dvm_run_strct_lvl_plc.createOrReplaceTempView('MM_DVM_RUN_STRCT_LVL_PLC')\\ndf_calc_index.createOrReplaceTempView('TIER2_DQM_CALC_INDEX')\\ndf_time_pd.createOrReplaceTempView('mm_time_perd_fdim')\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\nquery3 = \\\"\\\"\\\" (\\nSELECT fct_val.*,dvm_strct_lvl.IPP_CHECK_VAL,dvm_strct_lvl.IYA_CHECK_VAL\\n  FROM  TIER2_DQM_CALC_INDEX  fct_val\\n       JOIN MM_DVM_RUN_STRCT_LVL_PLC dvm_strct_lvl\\n             ON dvm_strct_lvl.run_id = fct_val.run_id\\n             and abs(dvm_strct_lvl.STRCT_LVL_ID) = fct_val.PROD_LVL\\n WHERE     NEW_TIME_PERD_IND = 'Y' AND fct_val.run_id = dvm_strct_lvl.run_id\\n       AND (    ABS (SALES_MSU_QTY) >= ABSLT_THSHD_VAL\\n            AND ABS (SALES_MLC_AMT) >= ABSLT_THSHD_VAL)\\n       AND (   (ABS (IPP_SU_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_SU_PCT) >= IYA_CHECK_VAL)\\n            OR (ABS (IPP_LC_PCT) >= IPP_CHECK_VAL)\\n            OR (ABS (IYA_LC_PCT) >= IYA_CHECK_VAL))  \\n)\\\"\\\"\\\"\\n\\nfct_detail = spark.sql(query3)\\nfct_detail.createOrReplaceTempView('fct_detail')\\n\\nquery4 = \\\"\\\"\\\"(SELECT 'IPP' index_type,\\n                'Volume Sales MSU' measure_name,\\n                ipp_su_pct index_value,\\n                SALES_MSU_QTY_PP Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IPP_SU_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IPP' index_type,\\n                'Value Sales MLC' measure_name,\\n                ipp_LC_pct,\\n                SALES_MLC_AMT_PP Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IPP_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IPP_LC_PCT) >= IPP_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Volume Sales MSU' measure_name,\\n                iya_su_pct index_value,\\n                SALES_MSU_QTY_YA Previous_value,\\n                SALES_MSU_QTY Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IYA_SU_PCT) >= IYA_CHECK_VAL\\n         UNION ALL\\n         SELECT 'IYA' index_type,\\n                'Value Sales MLC' measure_name,\\n                iya_LC_pct index_value,\\n                SALES_MLC_AMT_YA Previous_value,\\n                SALES_MLC_AMT Current_Value,\\n                time_perd_id,\\n                mkt_skid,\\n                prod_skid,\\n                IYA_CHECK_VAL CHECK_VAL\\n           FROM fct_detail \\n           WHERE ABS (IYA_LC_PCT) >= IYA_CHECK_VAL)\\\"\\\"\\\"\\n\\nfct = spark.sql(query4)\\nfct.createOrReplaceTempView('fct')\\n\\nquery5 = \\\"\\\"\\\"SELECT *\\n  FROM fct\\n       JOIN mm_time_perd_fdim time_perd\\n          ON fct.time_perd_id = time_perd.time_perd_id\\n       JOIN TIER2_MKT_MTRLZ_TBL mkt_map ON mkt_map.mkt_skid = fct.mkt_skid\\n             JOIN TIER2_PROD_MTRLZ_TBL prod on prod.prod_skid = fct.prod_skid \\\"\\\"\\\"\\ndf_dq1 = spark.sql(query5)\\n\\ndict_all_dfs['df_dq1'] = {\\\"df_object\\\" :df_dq1}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier2_mkt_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"df_calc_index\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dq1\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ - Business Validations",
      "predecessorName": "DQ1 - Unexpected change vs. previous period or year ago - Step1",
      "jsonSpecification": "{\n  \"active\": \"<<BUSINESS_DVM>>\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_bvr.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_bvr\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_bvr_chk\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": true
    },
    {
      "operationName": "DQ Report - Business Validations",
      "predecessorName": "DQ - Business Validations",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"saveToCSV\": \"true\",\n  \"generateHTMLReport\": \"true\",\n  \"generatePDFReport\": \"false\",\n  \"includeDetailedValidationResults\": \"failed rows only\",\n  \"numberOfRowsToDisplay\": 100,\n  \"reportTemplate\": \"default\"\n}",
      "operationVersionName": "DataQualityReport",
      "overridableIndicator": false
    },
    {
      "operationName": "Stop Calc",
      "predecessorName": "DQ Report - Business Validations",
      "jsonSpecification": "{\n  \"active\": \"<<BUSINESS_DVM>>\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf = dict_all_dfs['df_combine_bvr_chk'][\\\"df_object\\\"]\\n\\nresult = df.columns[4]\\ndf = df.withColumnRenamed(result, 'result')\\n\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\n\\ncnt = df.filter(\\\"result = 'Fail' \\\" ).count()\\n\\nif (cnt>0):\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 12, 1, 4, <<PROCESS_RUN_KEY>>]]\\nelse:\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 13, 1, 3, <<PROCESS_RUN_KEY>>]]\\n\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"jdbc\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"dbtable\\\", \\\"adwgp_mm.MM_DLVRY_RUN_LKP\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").mode(\\\"append\\\").save()\\n\\ndict_all_dfs['df_combine_bvr_chk'] = {\\\"df_object\\\" :df}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_bvr_chk\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_bvr_chk\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "Conditional Stop if DQ Fails",
      "predecessorName": "Stop Calc",
      "jsonSpecification": "{\n  \"active\": \"<<BUSINESS_DVM>>\",\n  \"expression\": \"result = 'Fail'\",\n  \"processStatus\": \"DQ_ISSUE\",\n  \"conditionValue\": \"false\",\n  \"milestone\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_bvr_chk\"\n    }\n  ]\n}",
      "operationVersionName": "ConditionalStop",
      "overridableIndicator": true
    }
  ],
  "graphName": "cdl_t2_business_checks v1"
}