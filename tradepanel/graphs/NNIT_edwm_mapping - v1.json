{
  "applicationName": "TURBINE_INTERNAL",
  "jsonSpecification": "{\r\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\r\n    \"title\": \" Three Files Contract Main Chain Overloading\",\r\n    \"description\": \"Overloading parameters in edwm time mapping\",\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n \r\n         \"IN_FILE_PATH\": {\r\n            \"title\": \"IN_FILE_PATH\",\r\n            \"description\": \"Input File Path\",\r\n            \"default\": \"bf/unrefined/adw-unrefined-bf\",\r\n            \"type\": \"string\"\r\n        },\r\n        \"EDWM_MAP\": {\r\n            \"title\": \"STEP_EDWM_MAP\",\r\n            \"description\": \"EDWM_MAP\",\r\n            \"type\": \"string\"\r\n        },\r\n\t\t \"TIME_MAP\": {\r\n            \"title\": \"TIME_MAP\",\r\n            \"description\": \"TIME_MAP\",\r\n\t\t\t\"enum\": [\r\n                \"MM_TIME_MH\",\r\n                \"MM_TIME_EB\",\r\n\t\t\t\t\"MM_TIME_OB\",\r\n\t\t\t\t\"MM_TIME_MH_LA\",\r\n\t\t\t\t\"MM_TIME_CUSTM\",\r\n\t\t\t\t\"MM_TIME_DACH_SH\",\r\n\t\t\t\t\"MM_TIME_QTR\",\r\n\t\t\t\t\"MM_TIME_WKMS\",\r\n\t\t\t\t\"MM_TIME_WKSS\",\r\n\t\t\t\t\"MM_TIME_BW\",\r\n\t\t\t\t\"MM_TIME_AOS\",\r\n\t\t\t\t\"MM_TIME_WKSS_FLATFILE\",\r\n\t\t\t\t\"MM_TIME_WKSS_FLATFILE2\",\r\n\t\t\t\t\"MM_TIME_FLATFILE\",\r\n\t\t\t\t\"MM_TIME_WEEK_NUM\",\r\n\t\t\t\t\"MM_TIME_MTH_NUM\",\r\n\t\t\t\t\"MM_TIME_P4W_TEST\",\r\n\t\t\t\t\"MM_TIME_BIGDATA_MH\",\r\n\t\t\t\t\"MM_TIME_BIGDATA_WK\"\r\n            ],\r\n            \"type\": \"string\"\r\n        }\r\n   },\r\n    \"required\": [\"IN_FILE_PATH\",\"STEP_EDWM_MAP\",\"TIME_MAP\"],\r\n    \"configurable\": [\"IN_FILE_PATH\",\"EDWM_MAP\",\"TIME_MAP\"]\r\n}",
  "nodes": [
    {
      "operationName": "LOAD FDIM",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"fileType\": \"parquet\",\n  \"inferSchema\": \"false\",\n  \"path\": \"refined/NNIT/tradepanel/prod-tp-lightrefined/MM_TIME_PERD_FDIM_VW/\",\n  \"addInputFileName\": \"false\",\n  \"semaphoreOption\": \"shared\",\n  \"createIfNotExist\": \"true\",\n  \"mergeSchema\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fdim2\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": true
    },
    {
      "operationName": "release MM_TIME_PERD_FDIM_VW",
      "predecessorName": "LOAD FDIM",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"actionType\": \"release\",\n  \"itemType\": \"path\",\n  \"itemPath\": \"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_TIME_PERD_FDIM_VW/\"\n}",
      "operationVersionName": "SemaphoreOperation",
      "overridableIndicator": false
    },
    {
      "operationName": "LOAD DataFrame",
      "predecessorName": "release MM_TIME_PERD_FDIM_VW",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"fileType\": \"csv\",\n  \"semaphoreOption\": \"none\",\n  \"createIfNotExist\": \"false\",\n  \"columnSelectionType\": \"name\",\n  \"path\": \"/unrefined/cloudpanel-test-unref/test/edwm/All_mm_time_mh.csv\",\n  \"addInputFileName\": \"false\",\n  \"milestone\": \"false\",\n  \"header\": \"true\",\n  \"delimiter\": \",\",\n  \"quote\": \"\\\"\",\n  \"escape\": \"\\\\\",\n  \"parserLib\": \"commons\",\n  \"mode\": \"PERMISSIVE\",\n  \"charset\": \"UTF-8\",\n  \"inferSchema\": \"true\",\n  \"comment\": \"#\",\n  \"nullValue\": \"null\",\n  \"dateFormat\": \"yyyy-MM-dd\",\n  \"saveOutputDfsToTempTable\": \"true\",\n  \"columns\": [\n    {\n      \"type\": \"string\",\n      \"sourceName\": \"extrn_time_perd_id\",\n      \"targetName\": \"extrn_time_perd_id\",\n      \"nullable\": \"true\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": true
    },
    {
      "operationName": "EDWM Time mapping",
      "predecessorName": "LOAD DataFrame",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\n\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_mh'):\\n  df_raw2 = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"__-___\\\"), to_date(col('extrn_time_perd_id'), 'y-MMM'))\\n                            .when(col('extrn_time_perd_id').like(\\\"ACCP____\\\"), to_date(col('extrn_time_perd_id')[5:10], 'MMy' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"___ __ - _ W/E __/__/__\\\"), to_date(col('extrn_time_perd_id')[1:6], 'MMM y' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"______\\\"), to_date(col('extrn_time_perd_id'), 'yyyyMM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"____-__\\\"),  to_date(col('extrn_time_perd_id'), 'yyyy-MM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"M ____.__\\\"), to_date(col('extrn_time_perd_id')[3:7], 'yyyy.MM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"MTH______\\\"), to_date(col('extrn_time_perd_id')[4:7], 'yyyyMM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"P%M\\\"), lit('Null') )\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"),lit('Null') )\\n                            .otherwise(to_date(col('extrn_time_perd_id'), 'MMMy' ))).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' ,when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY')\\n                            .otherwise('MH') )\\nelif(edwm_map.lower() == 'mm_time_week_num'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/<<IN_FILE_PATH>>/time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WEEK_NUM').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,concat(df_raw.extrn_time_perd_id[1:5],df_raw.extrn_time_perd_id[-2:20]) == concat(map_rules.extrn_time_perd_id[1:5],map_rules.extrn_time_perd_id[-2:20]), \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  \\nelif(edwm_map.lower() == 'mm_time_wk_uk'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/<<IN_FILE_PATH>>/time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WK_UK').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  \\nelif(edwm_map.lower() == 'mm_time_wk_ca_niq'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/<<IN_FILE_PATH>>/time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WK_CA_NIQ').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  \\nelif(edwm_map.lower() == 'mm_time_qtr'):\\n  df_raw2 = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"__-___\\\"), to_date(col('extrn_time_perd_id'), 'y-MMM'))\\n                            .when(col('extrn_time_perd_id').like(\\\"QTR%\\\"), add_months(to_date( concat( col('extrn_time_perd_id')[-6:4],lit('-'), col('extrn_time_perd_id')[-2:200], lit('-01') ), 'yyyy-MM-dd'),-2) ) \\n                            .when(col('extrn_time_perd_id').like(\\\"P%M\\\"), lit('Null'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"),lit('Null') )\\n                            .otherwise(to_date( concat(col('extrn_time_perd_id')[1:3],col('extrn_time_perd_id')[-2:200] ), 'MMMyy' ))).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise( lit('Null') )).withColumn('time_pd_type_cd' ,when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"__-___\\\"), lit('MH') )\\n                            .when(col('extrn_time_perd_id').like(\\\"QTR%\\\"), lit('QR') )\\n                            .otherwise(lit('MH') ))\\nelif(edwm_map.lower() == 'mm_time_eb'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/<<IN_FILE_PATH>>/time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_EB').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw2 = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"____-_________\\\"), to_date(concat(col('extrn_time_perd_id')[1:3], col('extrn_time_perd_id')[-2:20]), 'MMMy'))\\n                            .when(col('date_str').isNotNull(), to_date(col('date_str'), 'MMddyy') )\\n                            .when(col('extrn_time_perd_id')[1:2]=='JF', to_date(concat(lit('JAN'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='EF', to_date(concat(lit('JAN'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='MA', to_date(concat(lit('MAR'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='MJ', to_date(concat(lit('MAY'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='JA', to_date(concat(lit('JUL'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='SO', to_date(concat(lit('SEP'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='ND', to_date(concat(lit('NOV'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .otherwise(lit('Null') ) ).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' ,when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"____-_________\\\"), 'EB')\\n                            .otherwise('EB') )\\nelif(edwm_map.lower() == 'mm_time_wkss'):\\n  df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate', when(col('extrn_time_perd_id').like('1wk%'), to_date(concat( col('extrn_time_perd_id')[-4:20],col('extrn_time_perd_id')[5:3],col('extrn_time_perd_id')[-7:2] ), 'yyyyMMMdd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"%1 W/E%\\\"), to_date(col('extrn_time_perd_id')[-8:20], 'MM/dd/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"%1 w/e%\\\"), to_date(col('extrn_time_perd_id')[-8:20], 'MM/dd/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"WE%\\\"), to_date(col('extrn_time_perd_id')[-9:200], 'MMM dd yy') )\\n                            .otherwise(to_date(col('extrn_time_perd_id')[-8:20], 'yyyyMMdd'))).withColumn('time_pd_type_cd' , lit('WKSS') )\\nelif(edwm_map.lower() == 'mm_time_wkms'):\\n  df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate', when(length(col('extrn_time_perd_id'))==7, to_date(col('extrn_time_perd_id'), 'MMMddyy'))\\n                            .when(col('extrn_time_perd_id').like('1wk%'), to_date(concat( col('extrn_time_perd_id')[-4:20],col('extrn_time_perd_id')[5:3],col('extrn_time_perd_id')[-7:2] ), 'yyyyMMMdd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"%1 W/E%\\\"), to_date(col('extrn_time_perd_id')[-8:20], 'MM/dd/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"W/E%\\\"), to_date(col('extrn_time_perd_id')[-10:200], 'dd/MM/yyyy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"W %\\\"), to_date(col('extrn_time_perd_id')[-8:200], 'dd/MM/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"Week Ending%-%-%\\\"), to_date(col('extrn_time_perd_id')[-8:200], 'MM-dd-yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"Week Ending%\\\"), to_date(col('extrn_time_perd_id')[-8:200], 'dd/MM/yy') )\\n                            .otherwise(to_date(col('extrn_time_perd_id')[-8:20], 'yyyyMMdd'))).withColumn('time_pd_type_cd' , lit('WKMS') )\\nelif(edwm_map.lower() == 'mm_time_ob'):\\n  df_raw2 = df_raw.withColumn('sdate', when(col('extrn_time_perd_id')[1:2]=='DF', add_months(to_date(concat(lit('DEC'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='FM', add_months(to_date(concat(lit('FEB'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='AM', add_months(to_date(concat(lit('APR'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='JJ', add_months(to_date(concat(lit('JUN'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='AS', add_months(to_date(concat(lit('AUG'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='ON', add_months(to_date(concat(lit('OCT'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .otherwise(lit('Null') ) ).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), add_months(reg_sal(col('extrn_time_perd_id')), -12) )\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' , when(col('extrn_time_perd_id').like(\\\"P%M\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), 'FYTD' )\\n                            .otherwise(lit('OB')) )\\nelif(edwm_map.lower() == 'mm_time_mh_la'):\\n  df_raw2 = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id')[1:3]=='ENE', (to_date(concat(lit('JAN'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='FEB', (to_date(concat(lit('FEB'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='MAR', (to_date(concat(lit('MAR'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                               .when(col('extrn_time_perd_id')[1:3]=='ABR', (to_date(concat(lit('APR'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='MAY', (to_date(concat(lit('MAY'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='JUN', (to_date(concat(lit('JUN'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                            .when(col('extrn_time_perd_id')[1:3]=='JUL', (to_date(concat(lit('JUL'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='AGO', (to_date(concat(lit('AUG'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='SEP', (to_date(concat(lit('SEP'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                               .when(col('extrn_time_perd_id')[1:3]=='OCT', (to_date(concat(lit('OCT'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='NOV', (to_date(concat(lit('NOV'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='DIC', (to_date(concat(lit('DEC'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .otherwise(lit('Null') ) ).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' , when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY' )\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY' )\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY' )\\n                            .otherwise(lit('MH')) )\\nelif(edwm_map.lower() == 'mm_time_dach_sh'):\\n  df_raw2 = df_raw.withColumn('sdate', when(( (length(col('extrn_time_perd_id'))==10) & (col('extrn_time_perd_id')[8:2]=='Q1')), next_day(date_sub(to_date( concat(lit('01'),lit('04'),col('extrn_time_perd_id')[4:4]-1 ), 'ddMMyy'),7), 'Sat')).when(( (length(col('extrn_time_perd_id'))==10) & (col('extrn_time_perd_id')[8:2]=='Q2')), next_day(date_sub(to_date( concat(lit('01'),lit('07'),col('extrn_time_perd_id')[4:4]-1 ), 'ddMMyy'),7), 'Sat')).when(( (length(col('extrn_time_perd_id'))==10) & (col('extrn_time_perd_id')[8:2]=='Q3')), next_day(date_sub(to_date( concat(lit('01'),lit('10'),col('extrn_time_perd_id')[4:4]-1 ), 'ddMMyy'),7), 'Sat')).when(length(col('extrn_time_perd_id'))==6, next_day(date_sub(to_date( concat(lit('01 01 20'),col('extrn_time_perd_id')[4:2] ), 'dd MM yy'),7), 'Sat')).otherwise(None)).withColumn('edate', lit(None) ).withColumn('time_pd_type_cd' , lit('P52W') )\\nelif(edwm_map.lower() == 'mm_time_flatfile'):\\n  df_raw2 = df_raw.withColumn('edate', when(( (col('extrn_time_perd_id').like('________1')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\") ), to_date( col('extrn_time_perd_id')[1:7], 'ddMMMyy')).when(( (col('extrn_time_perd_id').like('________4')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), last_day(to_date( col('extrn_time_perd_id')[1:7], 'ddMMMyy')-7))\\n                             .when(( (col('extrn_time_perd_id').like('________5')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), last_day(to_date( col('extrn_time_perd_id')[1:7], 'ddMMMyy')-7))\\n                             .otherwise(lit(None))).withColumn('sdate', lit(None) ).withColumn('time_pd_type_cd', when(( (col('extrn_time_perd_id').like('________1')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\") ), lit('WKSS') )\\n                             .when(( (col('extrn_time_perd_id').like('________4')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), lit('MH'))\\n                             .when(( (col('extrn_time_perd_id').like('________5')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), lit('MH'))\\n                             .otherwise(lit(None)))\\nelif(edwm_map.lower() == 'mm_time_custm'):\\n  df_raw2 = df_raw.withColumn('time_pd_type_cd', when((col('extrn_time_perd_id').like(\\\"CURR1.M%\\\")), lit('P1M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO1.M%\\\")), lit('P1M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR3.M%\\\")), lit('P3M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO3.M%\\\")), lit('P3M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR6.M%\\\")), lit('P6M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO6.M%\\\")), lit('P6M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR12.M%\\\")), lit('P12M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO12.M%\\\")), lit('P12M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURRFY.M%\\\")), lit('FYTD'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGOF.M%%\\\")), lit('FY'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"2YAGOF.M%\\\")), lit('FY'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"4 Weeks Ending%\\\")), lit('P4W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%VAP%\\\")), lit('P52W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%\\\")), lit('P52W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - W/E%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - W/E%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - w/e%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - w/e%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FYTD\\\")), lit('FYTD'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"P%M%\\\")), when(upper(col('extrn_time_perd_id')).like(\\\"%YA\\\"), \\n                                    concat(lit(\\\"P\\\"),expr(\\\"substring(extrn_time_perd_id,2, instr(extrn_time_perd_id,'M' )-2 )\\\"),lit('M')) ).otherwise(concat(lit(\\\"P\\\"),expr(\\\"substring(extrn_time_perd_id,2, instr(extrn_time_perd_id,'M' )-2 )\\\"),lit('M'))))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FY___/__\\\")), lit('FY'))\\n                                    .otherwise(lit('MH'))).withColumn('sdate', when((col('extrn_time_perd_id').like(\\\"CURR1.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO1.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR3.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO3.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR6.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO6.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR12.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO12.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURRFY.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGOF.M%%\\\")), to_date(lit('2012-07-01'), 'yyyy-MM-dd') )\\n                                    .when((col('extrn_time_perd_id').like(\\\"2YAGOF.M%\\\")), to_date(lit('2011-07-01'), 'yyyy-MM-dd'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"4 Weeks Ending%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%VAP%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - W/E%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - W/E%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - w/e%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - w/e%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FYTD\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"P%M%\\\")), when(upper(col('extrn_time_perd_id')).like(\\\"%YA\\\"), \\n                                    lit(None) ).otherwise(lit(None)))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FY___/__\\\")), to_date(concat(col('extrn_time_perd_id')[4:2],lit(\\\"-07-01\\\") ),'yy-MM-dd'))\\n                                    .when(col('extrn_time_perd_id')[8:1]<1, add_months(to_date(concat(col('extrn_time_perd_id')[6:2], col('extrn_time_perd_id')[2:4] ), 'MMyyyy'), -1) )\\n                                    .when(col('extrn_time_perd_id')[8:1]>0,to_date(concat(col('extrn_time_perd_id')[6:2], col('extrn_time_perd_id')[2:4] ), 'MMyyyy') )\\n                                    .otherwise(lit(None) )).withColumn('edate', when((col('extrn_time_perd_id').like(\\\"CURR1.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO1.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR3.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO3.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR6.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO6.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR12.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO12.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURRFY.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGOF.M%%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"2YAGOF.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"4 Weeks Ending%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM-dd-yy')-1)\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%VAP%\\\")), to_date(col('extrn_time_perd_id')[-14:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - W/E%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - W/E%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - w/e%\\\")),to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - w/e%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FYTD\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"P%M%\\\")), when(upper(col('extrn_time_perd_id')).like(\\\"%YA\\\"), \\n                                    add_months(reg_sal('extrn_time_perd_id'),-12) ).otherwise(reg_sal('extrn_time_perd_id')))\\n                                    .otherwise(lit(None) ))\\nelif(edwm_map.lower() == 'mm_time_aos'):\\n  df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate', to_date(col('extrn_time_perd_id')[-8:200], 'yyyyMMdd')).withColumn('time_pd_type_cd' , lit('MHSS') )\\nelse:\\n   df_raw2 = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',lit('Null') ).withColumn('time_pd_type_cd' , lit('Null') )\\n\\ncols1 = df_raw2.columns\\ndf_raw3 = df_raw2.join(df_fdim2, ((df_raw2.time_pd_type_cd==df_fdim2.time_perd_type_code) & (( when( (df_raw2.edate=='Null'), df_raw2.sdate ==df_fdim2.time_perd_start_date).when( (df_raw2.sdate=='Null'), df_raw2.edate ==df_fdim2.time_perd_end_date ).otherwise( ( df_raw2.sdate ==df_fdim2.time_perd_start_date ) & (df_raw2.edate ==df_fdim2.time_perd_end_date) ) ))), 'left')\\ncols1.append('time_perd_id')\\ndf_raw_out = df_raw3.select(cols1 )\\n\\n\\ndict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw_out}\\ndf_output_dict['df_raw'] = df_raw_out\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    },\n    {\n      \"name\": \"df_fdim2\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": true
    },
    {
      "operationName": "mm_time_mh",
      "predecessorName": "EDWM Time mapping",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\n\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_mh'):\\n  df_raw = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"__-___\\\"), to_date(col('extrn_time_perd_id'), 'y-MMM'))\\n                            .when(col('extrn_time_perd_id').like(\\\"ACCP____\\\"), to_date(col('extrn_time_perd_id')[5:10], 'MMy' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"___ __ - _ W/E __/__/__\\\"), to_date(col('extrn_time_perd_id')[1:6], 'MMM y' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"______\\\"), to_date(col('extrn_time_perd_id'), 'yyyyMM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"____-__\\\"),  to_date(col('extrn_time_perd_id'), 'yyyy-MM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"M ____.__\\\"), to_date(col('extrn_time_perd_id')[3:7], 'yyyy.MM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"MTH______\\\"), to_date(col('extrn_time_perd_id')[4:7], 'yyyyMM' ))\\n                            .when(col('extrn_time_perd_id').like(\\\"P%M\\\"), lit('Null') )\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"),lit('Null') )\\n                            .otherwise(to_date(col('extrn_time_perd_id'), 'MMMy' ))).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' ,when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY')\\n                            .otherwise('MH') )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_mth_num",
      "predecessorName": "mm_time_mh",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_mth_num'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_MTH_NUM').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,concat(df_raw.extrn_time_perd_id[1:5],df_raw.extrn_time_perd_id[-2:20]) == concat(map_rules.extrn_time_perd_id[1:5],map_rules.extrn_time_perd_id[-2:20]), \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('edate', lit('Null') ).withColumn('sdate',to_date(col('date_str'), 'MMyyyy')).withColumn('time_pd_type_cd' , lit('MH') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_week_num",
      "predecessorName": "mm_time_mth_num",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_week_num'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WEEK_NUM').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,concat(df_raw.extrn_time_perd_id[1:5],df_raw.extrn_time_perd_id[-2:20]) == concat(map_rules.extrn_time_perd_id[1:5],map_rules.extrn_time_perd_id[-2:20]), \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_wk_uk",
      "predecessorName": "mm_time_week_num",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_wk_uk'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WK_UK').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "MM_TIME_WEEK_NUM_YR_LAST",
      "predecessorName": "mm_time_wk_uk",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_week_num_yr_last'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WEEK_NUM_YR_LAST').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "MM_TIME_BIGDATA_WK",
      "predecessorName": "MM_TIME_WEEK_NUM_YR_LAST",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_bigdata_wk'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_BIGDATA_WK').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "MM_TIME_BIGDATA_MH",
      "predecessorName": "MM_TIME_BIGDATA_WK",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_bigdata_mh'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_BIGDATA_MH').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMMyy')).withColumn('time_pd_type_cd' , lit('MH') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_wk_ca_niq",
      "predecessorName": "MM_TIME_BIGDATA_MH",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_wk_ca_niq'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_WK_CA_NIQ').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',to_date(col('date_str'), 'MMddyy')).withColumn('time_pd_type_cd' , lit('WKSS') ).drop(\\\"date_str\\\")\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_qtr",
      "predecessorName": "mm_time_wk_ca_niq",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_qtr'):\\n  df_raw = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"__-___\\\"), to_date(col('extrn_time_perd_id'), 'y-MMM'))\\n                            .when(col('extrn_time_perd_id').like(\\\"QTR%\\\"), add_months(to_date( concat( col('extrn_time_perd_id')[-6:4],lit('-'), col('extrn_time_perd_id')[-2:200], lit('-01') ), 'yyyy-MM-dd'),-2) ) \\n                            .when(col('extrn_time_perd_id').like(\\\"P%M\\\"), lit('Null'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"),lit('Null') )\\n                            .otherwise(to_date( concat(col('extrn_time_perd_id')[1:3],col('extrn_time_perd_id')[-2:200] ), 'MMMyy' ))).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise( lit('Null') )).withColumn('time_pd_type_cd' ,when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"__-___\\\"), lit('MH') )\\n                            .when(col('extrn_time_perd_id').like(\\\"QTR%\\\"), lit('QR') )\\n                            .otherwise(lit('MH') ))\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_eb",
      "predecessorName": "mm_time_qtr",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_eb'):\\n  from pyspark.sql.functions import regexp_replace\\n  map_rules = spark.read.format('csv').option('header', True).load('/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined//time_map_rule.csv').filter(upper(col('time_map_dftn'))=='MM_TIME_EB').withColumn('date_str',regexp_replace('date_str', \\\"'\\\", \\\"\\\"))\\n  df_raw = df_raw.join(map_rules,['extrn_time_perd_id'], \\\"left\\\").select(df_raw[\\\"*\\\"],map_rules.date_str)\\n  df_raw = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"____-_________\\\"), to_date(concat(col('extrn_time_perd_id')[1:3], col('extrn_time_perd_id')[-2:20]), 'MMMy'))\\n                            .when(col('date_str').isNotNull(), to_date(col('date_str'), 'MMddyy') )\\n                            .when(col('extrn_time_perd_id')[1:2]=='JF', to_date(concat(lit('JAN'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='EF', to_date(concat(lit('JAN'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='MA', to_date(concat(lit('MAR'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='MJ', to_date(concat(lit('MAY'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='JA', to_date(concat(lit('JUL'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='SO', to_date(concat(lit('SEP'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .when(col('extrn_time_perd_id')[1:2]=='ND', to_date(concat(lit('NOV'),col('extrn_time_perd_id')[-2:20]),'MMMyy') )\\n                             .otherwise(lit('Null') ) ).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' ,when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY')\\n                            .when(col('extrn_time_perd_id').like(\\\"____-_________\\\"), 'EB')\\n                            .otherwise('EB') )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_wkss",
      "predecessorName": "mm_time_eb",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_wkss'):\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate', when(col('extrn_time_perd_id').like('1wk%'), to_date(concat( col('extrn_time_perd_id')[-4:20],col('extrn_time_perd_id')[5:3],col('extrn_time_perd_id')[-7:2] ), 'yyyyMMMdd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"%1 W/E%\\\"), to_date(col('extrn_time_perd_id')[-8:20], 'MM/dd/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"%1 w/e%\\\"), to_date(col('extrn_time_perd_id')[-8:20], 'MM/dd/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"WE%\\\"), to_date(col('extrn_time_perd_id')[-9:200], 'MMM dd yy') )\\n                            .otherwise(to_date(col('extrn_time_perd_id')[-8:20], 'yyyyMMdd'))).withColumn('time_pd_type_cd' , lit('WKSS') )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_wkms",
      "predecessorName": "mm_time_wkss",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_wkms'):\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate', when(length(col('extrn_time_perd_id'))==7, to_date(col('extrn_time_perd_id'), 'MMMddyy'))\\n                            .when(col('extrn_time_perd_id').like('1wk%'), to_date(concat( col('extrn_time_perd_id')[-4:20],col('extrn_time_perd_id')[5:3],col('extrn_time_perd_id')[-7:2] ), 'yyyyMMMdd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"%1 W/E%\\\"), to_date(col('extrn_time_perd_id')[-8:20], 'MM/dd/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"W/E%\\\"), to_date(col('extrn_time_perd_id')[-10:200], 'dd/MM/yyyy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"W %\\\"), to_date(col('extrn_time_perd_id')[-8:200], 'dd/MM/yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"Week Ending%-%-%\\\"), to_date(col('extrn_time_perd_id')[-8:200], 'MM-dd-yy') )\\n                            .when(col('extrn_time_perd_id').like(\\\"Week Ending%\\\"), to_date(col('extrn_time_perd_id')[-8:200], 'dd/MM/yy') )\\n                            .otherwise(to_date(col('extrn_time_perd_id')[-8:20], 'yyyyMMdd'))).withColumn('time_pd_type_cd' , lit('WKMS') )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_ob",
      "predecessorName": "mm_time_wkms",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_ob'):\\n  df_raw = df_raw.withColumn('sdate', when(col('extrn_time_perd_id')[1:2]=='DF', add_months(to_date(concat(lit('DEC'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='FM', add_months(to_date(concat(lit('FEB'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='AM', add_months(to_date(concat(lit('APR'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='JJ', add_months(to_date(concat(lit('JUN'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='AS', add_months(to_date(concat(lit('AUG'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .when(col('extrn_time_perd_id')[1:2]=='ON', add_months(to_date(concat(lit('OCT'),col('extrn_time_perd_id')[-4:20]),'MMMyyyy'),-12) )\\n                             .otherwise(lit('Null') ) ).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), add_months(reg_sal(col('extrn_time_perd_id')), -12) )\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' , when(col('extrn_time_perd_id').like(\\\"P%M\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD YA\\\"), 'FYTD' )\\n                            .otherwise(lit('OB')) )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_mth_la",
      "predecessorName": "mm_time_ob",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_mth_la'):\\n  df_raw = df_raw.withColumn('sdate', when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), to_date(concat(lit(\\\"20\\\"),col('extrn_time_perd_id')[3:2],lit(\\\"-07-01\\\")), 'y-MM-dd'))\\n                            .when(col('extrn_time_perd_id')[1:3]=='ENE', (to_date(concat(lit('JAN'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='FEB', (to_date(concat(lit('FEB'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='MAR', (to_date(concat(lit('MAR'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                               .when(col('extrn_time_perd_id')[1:3]=='ABR', (to_date(concat(lit('APR'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='MAY', (to_date(concat(lit('MAY'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='JUN', (to_date(concat(lit('JUN'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                            .when(col('extrn_time_perd_id')[1:3]=='JUL', (to_date(concat(lit('JUL'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='AGO', (to_date(concat(lit('AUG'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='SEP', (to_date(concat(lit('SEP'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                               .when(col('extrn_time_perd_id')[1:3]=='OCT', (to_date(concat(lit('OCT'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='NOV', (to_date(concat(lit('NOV'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .when(col('extrn_time_perd_id')[1:3]=='DIC', (to_date(concat(lit('DEC'),col('extrn_time_perd_id')[-4:200]),'MMMyyyy')) )\\n                             .otherwise(lit('Null') ) ).withColumn('edate', when(col('extrn_time_perd_id').like(\\\"P%M\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), reg_sal(col('extrn_time_perd_id')))\\n                            .otherwise(lit('Null') )).withColumn('time_pd_type_cd' , when(col('extrn_time_perd_id').like(\\\"P%M\\\"), col('extrn_time_perd_id'))\\n                            .when(col('extrn_time_perd_id').like(\\\"FYTD\\\"), 'FYTD')\\n                            .when(col('extrn_time_perd_id').like(\\\"FY08-09\\\"), 'FY' )\\n                            .when(col('extrn_time_perd_id').like(\\\"FY09-10\\\"), 'FY' )\\n                            .when(col('extrn_time_perd_id').like(\\\"FY10-11\\\"), 'FY' )\\n                            .otherwise(lit('MH')) )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_dach_sh",
      "predecessorName": "mm_time_mth_la",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_dach_sh'):\\n  df_raw = df_raw.withColumn('sdate', when(( (length(col('extrn_time_perd_id'))==10) & (col('extrn_time_perd_id')[8:2]=='Q1')), next_day(date_sub(to_date( concat(lit('01'),lit('04'),col('extrn_time_perd_id')[4:4]-1 ), 'ddMMyy'),7), 'Sat')).when(( (length(col('extrn_time_perd_id'))==10) & (col('extrn_time_perd_id')[8:2]=='Q2')), next_day(date_sub(to_date( concat(lit('01'),lit('07'),col('extrn_time_perd_id')[4:4]-1 ), 'ddMMyy'),7), 'Sat')).when(( (length(col('extrn_time_perd_id'))==10) & (col('extrn_time_perd_id')[8:2]=='Q3')), next_day(date_sub(to_date( concat(lit('01'),lit('10'),col('extrn_time_perd_id')[4:4]-1 ), 'ddMMyy'),7), 'Sat')).when(length(col('extrn_time_perd_id'))==6, next_day(date_sub(to_date( concat(lit('01 01 20'),col('extrn_time_perd_id')[4:2] ), 'dd MM yy'),7), 'Sat')).otherwise(None)).withColumn('edate', lit(None) ).withColumn('time_pd_type_cd' , lit('P52W') )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_flatfile",
      "predecessorName": "mm_time_dach_sh",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_flatfile'):\\n  df_raw = df_raw.withColumn('edate', when(( (col('extrn_time_perd_id').like('________1')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\") ), to_date( col('extrn_time_perd_id')[1:7], 'ddMMMyy')).when(( (col('extrn_time_perd_id').like('________4')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), last_day(to_date( col('extrn_time_perd_id')[1:7], 'ddMMMyy')-7))\\n                             .when(( (col('extrn_time_perd_id').like('________5')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), last_day(to_date( col('extrn_time_perd_id')[1:7], 'ddMMMyy')-7))\\n                             .otherwise(lit(None))).withColumn('sdate', lit(None) ).withColumn('time_pd_type_cd', when(( (col('extrn_time_perd_id').like('________1')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\") ), lit('WKSS') )\\n                             .when(( (col('extrn_time_perd_id').like('________4')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), lit('MH'))\\n                             .when(( (col('extrn_time_perd_id').like('________5')) & (col('extrn_time_perd_id')[8:1]==\\\"_\\\")), lit('MH'))\\n                             .otherwise(lit(None)))\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_custm",
      "predecessorName": "mm_time_flatfile",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\ndef date_convert(date_to_convert):\\n  if (datetime.today().month>1):\\n      return datetime.today().replace(datetime.today().year,datetime.today().month-1,monthrange(datetime.today().year, datetime.today().month-1)[1])\\n  else:\\n      return datetime.today().replace(datetime.today().year-1,datetime.today().month+11,monthrange(datetime.today().year-1, datetime.today().month+11)[1])\\n\\nreg_sal = udf(lambda q: date_convert(q), DateType())\\n\\nif (edwm_map.lower() == 'mm_time_custm'):\\n  df_raw = df_raw.withColumn('time_pd_type_cd', when((col('extrn_time_perd_id').like(\\\"CURR1.M%\\\")), lit('P1M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO1.M%\\\")), lit('P1M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR3.M%\\\")), lit('P3M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO3.M%\\\")), lit('P3M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR6.M%\\\")), lit('P6M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO6.M%\\\")), lit('P6M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR12.M%\\\")), lit('P12M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO12.M%\\\")), lit('P12M'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURRFY.M%\\\")), lit('FYTD'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGOF.M%%\\\")), lit('FY'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"2YAGOF.M%\\\")), lit('FY'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"4 Weeks Ending%\\\")), lit('P4W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%VAP%\\\")), lit('P52W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%\\\")), lit('P52W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - W/E%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - W/E%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - w/e%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - w/e%\\\")), lit('P26W'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FYTD\\\")), lit('FYTD'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"P%M%\\\")), when(upper(col('extrn_time_perd_id')).like(\\\"%YA\\\"), \\n                                    concat(lit(\\\"P\\\"),expr(\\\"substring(extrn_time_perd_id,2, instr(extrn_time_perd_id,'M' )-2 )\\\"),lit('M')) ).otherwise(concat(lit(\\\"P\\\"),expr(\\\"substring(extrn_time_perd_id,2, instr(extrn_time_perd_id,'M' )-2 )\\\"),lit('M'))))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FY___/__\\\")), lit('FY'))\\n                                    .otherwise(lit('MH'))).withColumn('sdate', when((col('extrn_time_perd_id').like(\\\"CURR1.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO1.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR3.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO3.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR6.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO6.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR12.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO12.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURRFY.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGOF.M%%\\\")), to_date(lit('2012-07-01'), 'yyyy-MM-dd') )\\n                                    .when((col('extrn_time_perd_id').like(\\\"2YAGOF.M%\\\")), to_date(lit('2011-07-01'), 'yyyy-MM-dd'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"4 Weeks Ending%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%VAP%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - W/E%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - W/E%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - w/e%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - w/e%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FYTD\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"P%M%\\\")), when(upper(col('extrn_time_perd_id')).like(\\\"%YA\\\"), \\n                                    lit(None) ).otherwise(lit(None)))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FY___/__\\\")), to_date(concat(col('extrn_time_perd_id')[4:2],lit(\\\"-07-01\\\") ),'yy-MM-dd'))\\n                                    .when(col('extrn_time_perd_id')[8:1]<1, add_months(to_date(concat(col('extrn_time_perd_id')[6:2], col('extrn_time_perd_id')[2:4] ), 'MMyyyy'), -1) )\\n                                    .when(col('extrn_time_perd_id')[8:1]>0,to_date(concat(col('extrn_time_perd_id')[6:2], col('extrn_time_perd_id')[2:4] ), 'MMyyyy') )\\n                                    .otherwise(lit(None) )).withColumn('edate', when((col('extrn_time_perd_id').like(\\\"CURR1.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO1.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR3.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO3.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR6.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO6.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURR12.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGO12.M%\\\")), add_months(reg_sal('extrn_time_perd_id'),-12))\\n                                    .when((col('extrn_time_perd_id').like(\\\"CURRFY.M%\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"YAGOF.M%%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"2YAGOF.M%\\\")), lit(None))\\n                                    .when((col('extrn_time_perd_id').like(\\\"4 Weeks Ending%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM-dd-yy')-1)\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%VAP%\\\")), to_date(col('extrn_time_perd_id')[-14:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"52 WEEKS ENDING%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - W/E%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - W/E%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks - w/e%\\\")),to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"Latest 26 Wks YA - w/e%\\\")), to_date(col('extrn_time_perd_id')[-8:8],'MM/dd/yy'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"FYTD\\\")), reg_sal('extrn_time_perd_id'))\\n                                    .when((col('extrn_time_perd_id').like(\\\"P%M%\\\")), when(upper(col('extrn_time_perd_id')).like(\\\"%YA\\\"), \\n                                    add_months(reg_sal('extrn_time_perd_id'),-12) ).otherwise(reg_sal('extrn_time_perd_id')))\\n                                    .otherwise(lit(None) ))\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "mm_time_aos",
      "predecessorName": "mm_time_custm",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nif (edwm_map.lower() == 'mm_time_aos'):\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate', to_date(col('extrn_time_perd_id')[-8:200], 'yyyyMMdd')).withColumn('time_pd_type_cd' , lit('MHSS') )\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\\nelse:\\n  dict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw}\\n  df_output_dict['df_raw'] = df_raw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "time_perd_id derivation",
      "predecessorName": "mm_time_aos",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_raw = dict_all_dfs['df_raw'][\\\"df_object\\\"]\\ndf_fdim2 = dict_all_dfs['df_fdim2'][\\\"df_object\\\"]\\n\\nedwm_map = '<<TIME_MAP>>'\\n\\nfrom pyspark.sql.functions import col, date_format, dayofmonth, date_sub, add_months, length, next_day, last_day, instr, substring, locate\\nfrom pyspark.sql.types import StringType\\nfrom pyspark.sql.functions import expr, col, when, to_date, upper, substring, concat, to_timestamp, udf, lit, current_date, round\\nfrom datetime import datetime, time\\nfrom pyspark.sql.types import DateType, datetime\\nfrom datetime import datetime\\nfrom datetime import date\\nfrom calendar import calendar, monthrange\\n\\nlst_time_map = ['mm_time_mh', 'mm_time_week_num', 'mm_time_wk_uk', 'mm_time_wk_ca_niq', 'mm_time_qtr', 'mm_time_eb', 'mm_time_wkss', 'mm_time_wkms', 'mm_time_ob', 'mm_time_mh_la', 'mm_time_dach_sh', 'mm_time_flatfile', 'mm_time_custm', 'mm_time_aos', 'mm_time_week_num_yr_last', 'mm_time_bigdata_mh', 'mm_time_bigdata_wk', 'mm_time_mth_num','mm_time_mth_la']\\n\\nif (edwm_map.lower() not in  lst_time_map):\\n  df_raw = df_raw.withColumn('sdate', lit('Null') ).withColumn('edate',lit('Null') ).withColumn('time_pd_type_cd' , lit('Null') )\\n\\ndf_raw2 = df_raw \\ncols1 = df_raw2.columns\\ndf_raw3 = df_raw2.join(df_fdim2, ((df_raw2.time_pd_type_cd==df_fdim2.time_perd_type_code) & (( when( (df_raw2.edate=='Null'), df_raw2.sdate ==df_fdim2.time_perd_start_date).when( (df_raw2.sdate=='Null'), df_raw2.edate ==df_fdim2.time_perd_end_date ).otherwise( ( df_raw2.sdate ==df_fdim2.time_perd_start_date ) & (df_raw2.edate ==df_fdim2.time_perd_end_date) ) ))), 'left')\\ncols1.append('time_perd_id')\\ndf_raw_out = df_raw3.select(cols1 )\\n\\n\\ndict_all_dfs['df_raw'] = {\\\"df_object\\\" :df_raw_out}\\ndf_output_dict['df_raw'] = df_raw_out\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    },\n    {\n      \"name\": \"df_fdim2\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Return Time Period Id",
      "predecessorName": "time_perd_id derivation",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"expression\": \"1=1\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_raw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_raw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Filter",
      "overridableIndicator": false
    }
  ],
  "graphName": "NNIT_edwm_mapping - v1"
}