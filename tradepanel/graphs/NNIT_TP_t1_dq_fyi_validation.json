{
  "applicationName": "TURBINE_INTERNAL",
  "nodes": [
    {
      "operationName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"manualSchema\": \"true\",\n  \"transformations\": [\n    {\n      \"columnType\": \"string\",\n      \"columnName\": \"test\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "CreateSchema",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI Validations",
      "predecessorName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = 1\\ntier1_srce_sys_id = 3\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = 165761\\ntier1_cntry_id = 'PL'\\ntier1_categ_id = \\\"SHP\\\"\\n\\n\\n# Dataframes from Prior Steps\\n\\nmm_prod_xref = dict_all_dfs['mm_prod_xref'][\\\"df_object\\\"]\\ntier1_measr_mtrlz_tbl = dict_all_dfs['tier1_measr_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_time_mtrlz_tbl = dict_all_dfs['tier1_time_mtrlz_tbl'][\\\"df_object\\\"]\\nmm_run_measr_plc = dict_all_dfs['mm_run_measr_plc'][\\\"df_object\\\"]\\ntier1_mkt_dsdim = dict_all_dfs['tier1_mkt_dsdim'][\\\"df_object\\\"]\\ntier1_prod_dsdim = dict_all_dfs['tier1_prod_dsdim'][\\\"df_object\\\"]\\nmm_time_perd_assoc_type = dict_all_dfs['mm_time_perd_assoc_type'][\\\"df_object\\\"]\\nmm_time_perd_assoc = dict_all_dfs['mm_time_perd_assoc'][\\\"df_object\\\"]\\nmm_run_mkt_plc = dict_all_dfs['mm_run_mkt_plc'][\\\"df_object\\\"]\\n\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\ntier1_measr_mtrlz_tbl.createOrReplaceTempView('tier1_measr_mtrlz_tbl')\\ntier1_time_mtrlz_tbl.createOrReplaceTempView('tier1_time_mtrlz_tbl')\\nmm_run_measr_plc.createOrReplaceTempView('mm_run_measr_plc')\\ntier1_mkt_dsdim.createOrReplaceTempView('tier1_mkt_dsdim')\\ntier1_prod_dsdim.createOrReplaceTempView('tier1_prod_dsdim')\\nmm_time_perd_assoc_type.createOrReplaceTempView('mm_time_perd_assoc_type')\\nmm_time_perd_assoc.createOrReplaceTempView('mm_time_perd_assoc')\\nmm_run_mkt_plc.createOrReplaceTempView('mm_run_mkt_plc')\\n\\n\\n# tables from Postgres\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\n# mm_measr_id_lkp\\nmm_measr_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_id_lkp.createOrReplaceTempView('mm_measr_id_lkp')\\n\\n# mm_measr_lkp\\nmm_measr_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_lkp.createOrReplaceTempView('mm_measr_lkp')\\n\\n# mm_cntrt_lkp\\nmm_cntrt_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_cntrt_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_cntrt_lkp.createOrReplaceTempView('mm_cntrt_lkp')\\n\\n# mm_time_perd_id_lkp\\nmm_time_perd_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_id_lkp.createOrReplaceTempView('mm_time_perd_id_lkp')\\n\\n# mm_time_perd_fdim\\nmm_time_perd_fdim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_fdim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_fdim.createOrReplaceTempView('mm_time_perd_fdim')\\n\\n# mm_run_prttn_plc\\nmm_run_prttn_plc = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_run_prttn_plc\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\n\\n# mm_mkt_dim\\nmm_mkt_dim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_mkt_dim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_mkt_dim.createOrReplaceTempView('mm_mkt_dim')\\n\\n# dpf_all_run_vw\\ndpf_all_run_vw = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select run_id, process_status as run_sttus_id, cntrt_id as prcsg_id from adwgp_mm.mm_process_run_lkp_vw\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load() #change\\ndpf_all_run_vw.createOrReplaceTempView('dpf_all_run_vw')\\n\\n\\n#Validations\\n\\n#Measure description from the input file is different from expected\\n\\nquery1 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT LINE_NUM, EXTRN_CODE, EXTRN_NAME, MEASR_NAME, MEASR_DESC, mil.MEASR_ID  FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}  \\n),\\ninput_msr AS (\\n  SELECT DISTINCT srce.EXTRN_CODE FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}\\n),\\nmsr_last_existing_run AS (\\nSELECT EXTRN_MEASR_ID, EXTRN_MEASR_NAME, RUN_ID FROM (\\nSELECT rp.*,\\n  row_number() over(PARTITION BY extrn_measr_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_measr_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = rp.run_id\\n  JOIN mm_cntrt_lkp c\\n  ON c.prcsg_id = r.prcsg_id\\n  WHERE r.run_sttus_id = 4 AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_msrs AS (\\n  SELECT input_msr.EXTRN_CODE FROM input_msr\\n  LEFT JOIN msr_last_existing_run ON input_msr.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID\\n  WHERE msr_last_existing_run.EXTRN_MEASR_ID IS NULL\\n)\\nSELECT DISTINCT LINE_NUM as dq1_line_num, EXTRN_CODE as dq1_extrn_code, EXTRN_NAME AS dq1_dscr, msr_last_existing_run.EXTRN_MEASR_NAME dq1_measr_name, MEASR_DESC as dq1_measr_desc \\nFROM srce \\nJOIN msr_last_existing_run ON srce.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID \\nWHERE srce.EXTRN_NAME <> msr_last_existing_run.EXTRN_MEASR_NAME\\nUNION ALL\\nSELECT DISTINCT LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME AS DSCR, 'N/A' AS MEASR_NAME, MEASR_DESC\\nFROM srce\\nJOIN new_msrs ON srce.EXTRN_CODE = new_msrs.EXTRN_CODE\\\"\\\"\\\"\\ndf_inp_diff_exp = spark.sql(query1)\\n\\n\\n# Modified market description\\n\\nquery2 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT input.LINE_NUM, input.EXTRN_CODE, input.EXTRN_NAME, input.MKT_SKID_1 as mkt_skid, mkt.MKT_DESC FROM tier1_mkt_dsdim input             \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}' \\n),\\ninput_mkt AS (\\n  SELECT DISTINCT input.EXTRN_CODE FROM tier1_mkt_dsdim input  \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}'\\n),\\nmkt_last_existing_run AS (\\nSELECT EXTRN_MKT_ID, EXTRN_MKT_NAME, RUN_ID FROM (\\nSELECT rp.*,row_number() over(PARTITION BY extrn_mkt_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_mkt_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = rp.run_id\\n  JOIN mm_cntrt_lkp c\\n  ON c.prcsg_id = r.prcsg_id\\n  WHERE r.run_sttus_id = 4 AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_mkt AS (\\n  SELECT input_mkt.EXTRN_CODE FROM input_mkt \\n  LEFT JOIN mkt_last_existing_run ON input_mkt.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID\\n  WHERE mkt_last_existing_run.EXTRN_MKT_ID IS NULL\\n)\\nSELECT DISTINCT LINE_NUM as dq2_line_num, EXTRN_CODE as dq2_extrn_code, EXTRN_NAME as dq2_extrn_name, mkt_last_existing_run.extrn_mkt_name as dq2_mkt_name, mkt_desc as dq2_mkt_desc\\nFROM srce \\nJOIN mkt_last_existing_run ON srce.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID \\nWHERE srce.EXTRN_NAME <> mkt_last_existing_run.EXTRN_MKT_NAME\\nUNION ALL\\nSELECT DISTINCT LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME, 'N/A' AS mkt_name, mkt_desc \\nFROM srce\\nJOIN new_mkt ON srce.EXTRN_CODE = new_mkt.EXTRN_CODE\\\"\\\"\\\"\\n\\ndf_mod_mkt_desc = spark.sql(query2)\\n\\n\\n# Create mm_time_perd_assoc_tier1_vw\\nquery = \\\"\\\"\\\"with CAL_TYPE_2 as (\\nSELECT   2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_b,\\n          tb.time_perd_start_date time_perd_start_date_b,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_a,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          ta.time_perd_end_date time_perd_end_date_a,\\n          ta.time_perd_start_date time_perd_start_date_a\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n     WHERE assoc.CAL_TYPE_ID=2),\\nBIMTH_2_MTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_B TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_A TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          tb.time_perd_end_date time_perd_end_date_A,\\n          tb.time_perd_start_date time_perd_start_date_A\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\nMTH_2_BIMTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A ,\\n          assoc.TIME_PERD_ID_B ,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_A,\\n          ta.time_perd_end_date time_perd_end_date_A,\\n          ta.time_perd_start_date time_perd_start_date_A,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_B,\\n          tb.time_perd_start_date time_perd_start_date_B\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\n\\nBW_2_EB AS (SELECT  2 CAL_TYPE_ID,\\n          ta.EVEN_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.EVEN_BIMTH_TIME_PERD_CLASS_COD TIME_PERD_CLASS_CODE_A,\\n          ta.EVEN_BIMTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.EVEN_BIMTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.EVEN_BIMTH_START_DATE TIME_PERD_START_DATE_A\\n     FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='EB'\\n    ),\\n\\nBW_2_MTH AS (SELECT  2 CAL_TYPE_ID,\\n          ta.ODD_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.MTH_TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.MTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.MTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.MTH_START_DATE TIME_PERD_START_DATE_A\\n               FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='MTH'\\n    )\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM CAL_TYPE_2\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BIMTH_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_EB\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID, CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM MTH_2_BIMTH\\\"\\\"\\\"\\n\\ndf = spark.sql(query)\\n\\ndf.createOrReplaceTempView(\\\"mm_time_perd_assoc_tier1_vw\\\")\\n\\n# Missing product\\nquery3 = f\\\"\\\"\\\"WITH curr_run\\n     AS (SELECT /*+ materialize */ DISTINCT CASE \\n       WHEN tf.time_perd_class_code IN ('MTH') THEN tf.time_perd_end_date \\n       ELSE ta.time_perd_end_date_a END time_perd_end_date,\\n       3 srce_Sys_id,\\n       {tier1_cntrt_id} cntrt_id,\\n       '{tier1_fact_type_code}' fact_type_code,\\n       '{tier1_categ_id}' prod_prttn_code\\n           FROM tier1_time_mtrlz_tbl st JOIN mm_time_perd_id_lkp l ON \\n           l.extrn_time_perd_id = st.extrn_code and l.vendr_id={tier1_vendr_id}\\n           JOIN mm_time_perd_fdim tf on tf.time_perd_id = l.time_perd_id\\n           LEFT OUTER JOIN mm_time_perd_assoc_tier1_vw ta ON ta.time_perd_id_b = tf.time_perd_id\\n           AND ta.time_perd_type_code_b = tf.time_perd_type_code\\n           AND ta.time_perd_type_code_a = 'MH'\\n           \\n           ),\\n     all_finished_run\\n     AS (SELECT *\\n           FROM mm_run_prttn_plc plc\\n                JOIN dpf_all_run_vw run ON run.run_id = plc.run_id\\n          WHERE     plc.cntrt_id = {tier1_cntrt_id}\\n                AND plc.time_perd_class_code = 'MTH'\\n                AND run.run_sttus_id = 4),\\n     time_perd_before\\n     AS (SELECT MAX (mm_time_perd_end_date) mm_time_perd_end_date,\\n                MAX (srce_sys_id) srce_sys_id,\\n                MAX (cntrt_id) cntrt_id,\\n                MAX (fact_Type_code) fact_Type_code,\\n                MAX (prod_prttn_code) prod_prttn_code\\n           FROM all_finished_run\\n          WHERE mm_time_perd_end_date <\\n                   (SELECT MIN (time_perd_end_date)\\n                      FROM curr_run \\n                                ) \\n                                ),\\n     time_perds \\n     AS (  SELECT /*+ materialize */ * FROM (\\n                 SELECT time_perd_end_date AS mm_time_perd_end_date, \\n                        srce_sys_id,\\n                        cntrt_id,\\n                        fact_type_code,\\n                        prod_prttn_code\\n                 FROM curr_run\\n               UNION ALL\\n                 SELECT mm_time_perd_end_date,\\n                        srce_sys_id, \\n                        cntrt_id,\\n                        fact_type_code, \\n                        prod_prttn_code  \\n                 FROM time_perd_before\\n            )\\n          ),\\n     loaded_prod\\n     AS (SELECT /*+ materialize parallel(8) */\\n                DISTINCT prod_skid\\n           FROM mm_tp_mth_fct fct\\n                JOIN time_perds tp\\n                   ON     fct.mm_time_perd_end_date =\\n                             tp.mm_time_perd_end_date\\n                      AND fct.srce_sys_id = tp.srce_sys_id\\n                      AND fct.cntrt_id = tp.cntrt_id\\n                      AND fct.fact_Type_code = tp.fact_Type_code\\n                      AND fct.prod_prttn_code = tp.prod_prttn_code),\\n     xref\\n     AS (SELECT mm_prod_xref.*\\n           FROM mm_prod_xref inner join (SELECT DISTINCT srce_sys_id, cntrt_id FROM curr_run) distinct_curr_run on \\n          mm_prod_xref.srce_sys_id = distinct_curr_run.srce_sys_id and mm_prod_xref.cntrt_id = distinct_curr_run.cntrt_id\\n         )\\nSELECT xref.extrn_prod_id as dq3_extrn_prod_id , xref.extrn_prod_name as dq3_extrn_prod_name , xref.extrn_prod_attr_val_list as dq3_extrn_prod_attr_val_list , xref.prod_match_attr_list as dq3_prod_match_attr_list , xref.prod_prttn_code as dq3_prod_prttn_code , xref.prod_skid as dq3_prod_skid , xref.srce_sys_id as dq3_srce_sys_id , xref.cntrt_id as dq3_cntrt_id\\n  FROM xref\\n       JOIN loaded_prod ON xref.prod_skid = loaded_prod.prod_skid\\n       LEFT OUTER JOIN tier1_prod_dsdim prod_cur\\n          ON loaded_prod.prod_skid = prod_cur.prod_skid\\n       WHERE prod_cur.prod_skid IS  NULL\\n\\t   LIMIT 1\\\"\\\"\\\"\\ndf_miss_prod = spark.sql(query3)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\ndf_inp_diff_exp = df_inp_diff_exp.withColumn('DQ1', lit('Measure description from the input file is different from expected')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ndf_mod_mkt_desc = df_mod_mkt_desc.withColumn('DQ2', lit('Modified market description')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ndf_miss_prod = df_miss_prod.withColumn('DQ3', lit('Missing product')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n\\n# Combine the dataframes\\ndf_combine = df_inp_diff_exp.join(df_mod_mkt_desc,['row_id'] , 'full').join(df_miss_prod,['row_id'] , 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\n# KPI information\\ndq1_columns = ['DQ1','dq1_line_num', 'dq1_extrn_code', 'dq1_dscr','dq1_measr_name','dq1_measr_desc']\\ndq2_columns = ['DQ2','dq2_line_num', 'dq2_extrn_code', 'dq2_extrn_name', 'dq2_mkt_name', 'dq2_mkt_desc']\\ndq3_columns = ['DQ3','dq3_extrn_prod_id', 'dq3_extrn_prod_name', 'dq3_extrn_prod_attr_val_list', 'dq3_prod_match_attr_list', 'dq3_prod_prttn_code', 'dq3_prod_skid', 'dq3_srce_sys_id', 'dq3_cntrt_id']\\n\\ncombined_cols = ['row_id']\\ndata = []\\n[combined_cols.append(i) for i in dq1_columns]\\n[combined_cols.append(i) for i in dq2_columns]\\n[combined_cols.append(i) for i in dq3_columns]\\n\\n#NR = 0 failed\\n#NR > 0 Passed\\n#CASE WHEN ret.NR = 0 THEN 'PASSED' ELSE 'FAILED' END AS RESULT\\n\\ndq1_val = ('dq1_line_num', 'SQL Validation KPI', \\\"dq1_line_num IS NULL\\\", '', 'false', 'Measure description from the input file is different from expected', 100 )\\ndata.append(dq1_val)\\ndq2_val = ('dq2_line_num', 'SQL Validation KPI', \\\"dq2_line_num IS NULL\\\", '', 'false', 'Modified market description', 100 )\\ndata.append(dq2_val)\\ndq3_val = ('dq3_extrn_prod_id', 'SQL Validation KPI', \\\"dq3_extrn_prod_id IS NULL\\\", '', 'false', 'Missing product', 100 )\\ndata.append(dq3_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\ndf_fyi = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\ndict_all_dfs['df_fyi'] = {\\\"df_object\\\" :df_fyi}\\ndf_output_dict['df_fyi'] = df_fyi\\n\\ndict_all_dfs['df_combine_fyi'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_fyi'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mm_prod_xref\"\n    },\n    {\n      \"name\": \"tier1_measr_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_time_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"mm_run_measr_plc\"\n    },\n    {\n      \"name\": \"tier1_mkt_dsdim\"\n    },\n    {\n      \"name\": \"tier1_prod_dsdim\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc_type\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc\"\n    },\n    {\n      \"name\": \"mm_run_mkt_plc\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fyi\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_fyi\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI Validations - v1",
      "predecessorName": "FYI Validations",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = <<VENDOR_ID>>\\ntier1_srce_sys_id = <<SRCE_SYS_ID>>\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = <<CNTRT_ID>>\\ntier1_cntry_id = '<<ISO_CNTRY_CODE>>'\\ntier1_categ_id = \\\"<<CATEGORY_ID>>\\\"\\n\\n\\n# Dataframes from Prior Steps\\n\\nmm_prod_xref = dict_all_dfs['mm_prod_xref'][\\\"df_object\\\"]\\ntier1_measr_mtrlz_tbl = dict_all_dfs['tier1_measr_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_time_mtrlz_tbl = dict_all_dfs['tier1_time_mtrlz_tbl'][\\\"df_object\\\"]\\nmm_run_measr_plc = dict_all_dfs['mm_run_measr_plc'][\\\"df_object\\\"]\\ntier1_mkt_dsdim = dict_all_dfs['tier1_mkt_dsdim'][\\\"df_object\\\"]\\ntier1_prod_dsdim = dict_all_dfs['tier1_prod_dsdim'][\\\"df_object\\\"]\\nmm_time_perd_assoc_type = dict_all_dfs['mm_time_perd_assoc_type'][\\\"df_object\\\"]\\nmm_time_perd_assoc = dict_all_dfs['mm_time_perd_assoc'][\\\"df_object\\\"]\\nmm_run_mkt_plc = dict_all_dfs['mm_run_mkt_plc'][\\\"df_object\\\"]\\n\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\ntier1_measr_mtrlz_tbl.createOrReplaceTempView('tier1_measr_mtrlz_tbl')\\ntier1_time_mtrlz_tbl.createOrReplaceTempView('tier1_time_mtrlz_tbl')\\nmm_run_measr_plc.createOrReplaceTempView('mm_run_measr_plc')\\ntier1_mkt_dsdim.createOrReplaceTempView('tier1_mkt_dsdim')\\ntier1_prod_dsdim.createOrReplaceTempView('tier1_prod_dsdim')\\nmm_time_perd_assoc_type.createOrReplaceTempView('mm_time_perd_assoc_type')\\nmm_time_perd_assoc.createOrReplaceTempView('mm_time_perd_assoc')\\nmm_run_mkt_plc.createOrReplaceTempView('mm_run_mkt_plc')\\n\\n\\n# tables from Postgres\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\n# mm_measr_id_lkp\\nmm_measr_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_id_lkp.createOrReplaceTempView('mm_measr_id_lkp')\\n\\n# mm_measr_lkp\\nmm_measr_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_lkp.createOrReplaceTempView('mm_measr_lkp')\\n\\n# mm_cntrt_lkp\\nmm_cntrt_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_cntrt_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_cntrt_lkp.createOrReplaceTempView('mm_cntrt_lkp')\\n\\n# mm_time_perd_id_lkp\\nmm_time_perd_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_id_lkp.createOrReplaceTempView('mm_time_perd_id_lkp')\\n\\n# mm_time_perd_fdim\\nmm_time_perd_fdim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_fdim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_fdim.createOrReplaceTempView('mm_time_perd_fdim')\\n\\n# mm_run_prttn_plc\\nmm_run_prttn_plc = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_run_prttn_plc\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\n\\n# mm_mkt_dim\\nmm_mkt_dim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_mkt_dim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_mkt_dim.createOrReplaceTempView('mm_mkt_dim')\\n\\n# dpf_all_run_vw\\ndpf_all_run_vw = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select run_id, process_status as run_sttus_id, cntrt_id as prcsg_id from adwgp_mm.mm_process_run_lkp_vw\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load() #change\\ndpf_all_run_vw.createOrReplaceTempView('dpf_all_run_vw')\\n\\n\\n#Validations\\n\\n#Measure description from the input file is different from expected\\n\\nquery1 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT LINE_NUM, EXTRN_CODE, EXTRN_NAME, MEASR_NAME, MEASR_DESC, mil.MEASR_ID  FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}  \\n),\\ninput_msr AS (\\n  SELECT DISTINCT srce.EXTRN_CODE FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}\\n),\\nmsr_last_existing_run AS (\\nSELECT EXTRN_MEASR_ID, EXTRN_MEASR_NAME, RUN_ID FROM (\\nSELECT rp.*,\\n  row_number() over(PARTITION BY extrn_measr_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_measr_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = rp.run_id\\n  JOIN mm_cntrt_lkp c\\n  ON c.prcsg_id = r.prcsg_id\\n  WHERE r.run_sttus_id = 4 AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_msrs AS (\\n  SELECT input_msr.EXTRN_CODE FROM input_msr\\n  LEFT JOIN msr_last_existing_run ON input_msr.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID\\n  WHERE msr_last_existing_run.EXTRN_MEASR_ID IS NULL\\n)\\nSELECT DISTINCT (1) as DQ, LINE_NUM , EXTRN_CODE , EXTRN_NAME AS DSCR, msr_last_existing_run.EXTRN_MEASR_NAME measr_name, MEASR_DESC\\nFROM srce \\nJOIN msr_last_existing_run ON srce.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID \\nWHERE srce.EXTRN_NAME <> msr_last_existing_run.EXTRN_MEASR_NAME\\nUNION ALL\\nSELECT DISTINCT (1) as DQ, LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME AS DSCR, 'N/A' AS MEASR_NAME, MEASR_DESC\\nFROM srce\\nJOIN new_msrs ON srce.EXTRN_CODE = new_msrs.EXTRN_CODE\\\"\\\"\\\"\\ndf_inp_diff_exp = spark.sql(query1)\\n\\n\\n# Modified market description\\n\\nquery2 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT input.LINE_NUM, input.EXTRN_CODE, input.EXTRN_NAME, input.MKT_SKID_1 as mkt_skid, mkt.MKT_DESC FROM tier1_mkt_dsdim input             \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}' \\n),\\ninput_mkt AS (\\n  SELECT DISTINCT input.EXTRN_CODE FROM tier1_mkt_dsdim input  \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}'\\n),\\nmkt_last_existing_run AS (\\nSELECT EXTRN_MKT_ID, EXTRN_MKT_NAME, RUN_ID FROM (\\nSELECT rp.*,row_number() over(PARTITION BY extrn_mkt_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_mkt_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = rp.run_id\\n  JOIN mm_cntrt_lkp c\\n  ON c.prcsg_id = r.prcsg_id\\n  WHERE r.run_sttus_id = 4 AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_mkt AS (\\n  SELECT input_mkt.EXTRN_CODE FROM input_mkt \\n  LEFT JOIN mkt_last_existing_run ON input_mkt.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID\\n  WHERE mkt_last_existing_run.EXTRN_MKT_ID IS NULL\\n)\\nSELECT DISTINCT (2) as DQ, LINE_NUM , EXTRN_CODE , EXTRN_NAME , mkt_last_existing_run.extrn_mkt_name as mkt_name, mkt_desc as mkt_desc\\nFROM srce \\nJOIN mkt_last_existing_run ON srce.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID \\nWHERE srce.EXTRN_NAME <> mkt_last_existing_run.EXTRN_MKT_NAME\\nUNION ALL\\nSELECT DISTINCT (2) as DQ, LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME, 'N/A' AS mkt_name, mkt_desc \\nFROM srce\\nJOIN new_mkt ON srce.EXTRN_CODE = new_mkt.EXTRN_CODE\\\"\\\"\\\"\\n\\ndf_mod_mkt_desc = spark.sql(query2)\\n\\n\\n# Create mm_time_perd_assoc_tier1_vw\\nquery = \\\"\\\"\\\"with CAL_TYPE_2 as (\\nSELECT   2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_b,\\n          tb.time_perd_start_date time_perd_start_date_b,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_a,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          ta.time_perd_end_date time_perd_end_date_a,\\n          ta.time_perd_start_date time_perd_start_date_a\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n     WHERE assoc.CAL_TYPE_ID=2),\\nBIMTH_2_MTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_B TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_A TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          tb.time_perd_end_date time_perd_end_date_A,\\n          tb.time_perd_start_date time_perd_start_date_A\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\nMTH_2_BIMTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A ,\\n          assoc.TIME_PERD_ID_B ,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_A,\\n          ta.time_perd_end_date time_perd_end_date_A,\\n          ta.time_perd_start_date time_perd_start_date_A,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_B,\\n          tb.time_perd_start_date time_perd_start_date_B\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\n\\nBW_2_EB AS (SELECT  2 CAL_TYPE_ID,\\n          ta.EVEN_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.EVEN_BIMTH_TIME_PERD_CLASS_COD TIME_PERD_CLASS_CODE_A,\\n          ta.EVEN_BIMTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.EVEN_BIMTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.EVEN_BIMTH_START_DATE TIME_PERD_START_DATE_A\\n     FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='EB'\\n    ),\\n\\nBW_2_MTH AS (SELECT  2 CAL_TYPE_ID,\\n          ta.ODD_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.MTH_TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.MTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.MTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.MTH_START_DATE TIME_PERD_START_DATE_A\\n               FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='MTH'\\n    )\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM CAL_TYPE_2\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BIMTH_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_EB\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID, CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM MTH_2_BIMTH\\\"\\\"\\\"\\n\\ndf = spark.sql(query)\\n\\ndf.createOrReplaceTempView(\\\"mm_time_perd_assoc_tier1_vw\\\")\\n\\n# Missing product\\nquery3 = f\\\"\\\"\\\"WITH curr_run\\n     AS (SELECT /*+ materialize */ DISTINCT CASE \\n       WHEN tf.time_perd_class_code IN ('MTH') THEN tf.time_perd_end_date \\n       ELSE ta.time_perd_end_date_a END time_perd_end_date,\\n       3 srce_Sys_id,\\n       {tier1_cntrt_id} cntrt_id,\\n       '{tier1_fact_type_code}' fact_type_code,\\n       '{tier1_categ_id}' prod_prttn_code\\n           FROM tier1_time_mtrlz_tbl st JOIN mm_time_perd_id_lkp l ON \\n           l.extrn_time_perd_id = st.extrn_code and l.vendr_id={tier1_vendr_id}\\n           JOIN mm_time_perd_fdim tf on tf.time_perd_id = l.time_perd_id\\n           LEFT OUTER JOIN mm_time_perd_assoc_tier1_vw ta ON ta.time_perd_id_b = tf.time_perd_id\\n           AND ta.time_perd_type_code_b = tf.time_perd_type_code\\n           AND ta.time_perd_type_code_a = 'MH'\\n           \\n           ),\\n     all_finished_run\\n     AS (SELECT *\\n           FROM mm_run_prttn_plc plc\\n                JOIN dpf_all_run_vw run ON run.run_id = plc.run_id\\n          WHERE     plc.cntrt_id = {tier1_cntrt_id}\\n                AND plc.time_perd_class_code = 'MTH'\\n                AND run.run_sttus_id = 4),\\n     time_perd_before\\n     AS (SELECT MAX (mm_time_perd_end_date) mm_time_perd_end_date,\\n                MAX (srce_sys_id) srce_sys_id,\\n                MAX (cntrt_id) cntrt_id,\\n                MAX (fact_Type_code) fact_Type_code,\\n                MAX (prod_prttn_code) prod_prttn_code\\n           FROM all_finished_run\\n          WHERE mm_time_perd_end_date <\\n                   (SELECT MIN (time_perd_end_date)\\n                      FROM curr_run \\n                                ) \\n                                ),\\n     time_perds \\n     AS (  SELECT /*+ materialize */ * FROM (\\n                 SELECT time_perd_end_date AS mm_time_perd_end_date, \\n                        srce_sys_id,\\n                        cntrt_id,\\n                        fact_type_code,\\n                        prod_prttn_code\\n                 FROM curr_run\\n               UNION ALL\\n                 SELECT mm_time_perd_end_date,\\n                        srce_sys_id, \\n                        cntrt_id,\\n                        fact_type_code, \\n                        prod_prttn_code  \\n                 FROM time_perd_before\\n            )\\n          ),\\n     loaded_prod\\n     AS (SELECT /*+ materialize parallel(8) */\\n                DISTINCT prod_skid\\n           FROM mm_tp_mth_fct fct\\n                JOIN time_perds tp\\n                   ON     fct.mm_time_perd_end_date =\\n                             tp.mm_time_perd_end_date\\n                      AND fct.srce_sys_id = tp.srce_sys_id\\n                      AND fct.cntrt_id = tp.cntrt_id\\n                      AND fct.fact_Type_code = tp.fact_Type_code\\n                      AND fct.prod_prttn_code = tp.prod_prttn_code),\\n     xref\\n     AS (SELECT mm_prod_xref.*\\n           FROM mm_prod_xref inner join (SELECT DISTINCT srce_sys_id, cntrt_id FROM curr_run) distinct_curr_run on \\n          mm_prod_xref.srce_sys_id = distinct_curr_run.srce_sys_id and mm_prod_xref.cntrt_id = distinct_curr_run.cntrt_id\\n         )\\nSELECT (3) as DQ, xref.extrn_prod_id as extrn_prod_id , xref.extrn_prod_name as extrn_prod_name , xref.extrn_prod_attr_val_list as extrn_prod_attr_val_list , xref.prod_match_attr_list as prod_match_attr_list , xref.prod_prttn_code as prod_prttn_code , xref.prod_skid as prod_skid , xref.srce_sys_id as srce_sys_id , xref.cntrt_id as cntrt_id\\n  FROM xref\\n       JOIN loaded_prod ON xref.prod_skid = loaded_prod.prod_skid\\n       LEFT OUTER JOIN tier1_prod_dsdim prod_cur\\n          ON loaded_prod.prod_skid = prod_cur.prod_skid\\n       WHERE prod_cur.prod_skid IS  NULL\\n\\t   LIMIT 1\\\"\\\"\\\"\\ndf_miss_prod = spark.sql(query3)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\ndq1 = df_inp_diff_exp\\ndq2 = df_mod_mkt_desc\\ndq3 = df_miss_prod\\n\\ndf_combine = dq1.unionByName(dq2, True).unionByName(dq3, True)\\n\\ndata = []\\n\\ndq1_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 1\\\", '', 'false', 'Measure description from the input file is different from expected', 100 )\\ndata.append(dq1_val)\\ndq2_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 2\\\", '', 'false', 'Modified market description', 100 )\\ndata.append(dq2_val)\\ndq3_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 3\\\", '', 'false', 'Missing product', 100 )\\ndata.append(dq3_val)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\ndf_fyi = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\ndict_all_dfs['df_fyi'] = {\\\"df_object\\\" :df_fyi}\\ndf_output_dict['df_fyi'] = df_fyi\\n\\ndict_all_dfs['df_combine_fyi'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_fyi'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mm_prod_xref\"\n    },\n    {\n      \"name\": \"tier1_measr_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_time_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"mm_run_measr_plc\"\n    },\n    {\n      \"name\": \"tier1_mkt_dsdim\"\n    },\n    {\n      \"name\": \"tier1_prod_dsdim\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc_type\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc\"\n    },\n    {\n      \"name\": \"mm_run_mkt_plc\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fyi\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_fyi\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI Validations - v2",
      "predecessorName": "FYI Validations - v1",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = <<VENDOR_ID>>\\ntier1_srce_sys_id = <<SRCE_SYS_ID>>\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = <<CNTRT_ID>>\\ntier1_cntry_id = '<<ISO_CNTRY_CODE>>'\\ntier1_categ_id = \\\"<<CATEGORY_ID>>\\\"\\n\\n\\n# Dataframes from Prior Steps\\n\\nmm_prod_xref = dict_all_dfs['mm_prod_xref'][\\\"df_object\\\"]\\ntier1_measr_mtrlz_tbl = dict_all_dfs['tier1_measr_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_time_mtrlz_tbl = dict_all_dfs['tier1_time_mtrlz_tbl'][\\\"df_object\\\"]\\nmm_run_measr_plc = dict_all_dfs['mm_run_measr_plc'][\\\"df_object\\\"]\\ntier1_mkt_dsdim = dict_all_dfs['tier1_mkt_dsdim'][\\\"df_object\\\"]\\ntier1_prod_dsdim = dict_all_dfs['tier1_prod_dsdim'][\\\"df_object\\\"]\\nmm_time_perd_assoc_type = dict_all_dfs['mm_time_perd_assoc_type'][\\\"df_object\\\"]\\nmm_time_perd_assoc = dict_all_dfs['mm_time_perd_assoc'][\\\"df_object\\\"]\\nmm_run_mkt_plc = dict_all_dfs['mm_run_mkt_plc'][\\\"df_object\\\"]\\n\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\ntier1_measr_mtrlz_tbl.createOrReplaceTempView('tier1_measr_mtrlz_tbl')\\ntier1_time_mtrlz_tbl.createOrReplaceTempView('tier1_time_mtrlz_tbl')\\nmm_run_measr_plc.createOrReplaceTempView('mm_run_measr_plc')\\ntier1_mkt_dsdim.createOrReplaceTempView('tier1_mkt_dsdim')\\ntier1_prod_dsdim.createOrReplaceTempView('tier1_prod_dsdim')\\nmm_time_perd_assoc_type.createOrReplaceTempView('mm_time_perd_assoc_type')\\nmm_time_perd_assoc.createOrReplaceTempView('mm_time_perd_assoc')\\nmm_run_mkt_plc.createOrReplaceTempView('mm_run_mkt_plc')\\n\\n\\n# tables from Postgres\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\n# mm_measr_id_lkp\\nmm_measr_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_id_lkp.createOrReplaceTempView('mm_measr_id_lkp')\\n\\n# mm_measr_lkp\\nmm_measr_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_lkp.createOrReplaceTempView('mm_measr_lkp')\\n\\n# mm_cntrt_lkp\\nmm_cntrt_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_cntrt_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_cntrt_lkp.createOrReplaceTempView('mm_cntrt_lkp')\\n\\n# mm_time_perd_id_lkp\\nmm_time_perd_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_id_lkp.createOrReplaceTempView('mm_time_perd_id_lkp')\\n\\n# mm_time_perd_fdim\\nmm_time_perd_fdim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_fdim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_fdim.createOrReplaceTempView('mm_time_perd_fdim')\\n\\n# mm_run_prttn_plc\\nmm_run_prttn_plc = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_run_prttn_plc\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\n\\n# mm_mkt_dim\\nmm_mkt_dim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_mkt_dim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_mkt_dim.createOrReplaceTempView('mm_mkt_dim')\\n\\n# dpf_all_run_vw\\ndpf_all_run_vw = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select run_id, process_status as run_sttus_id, cntrt_id as prcsg_id from adwgp_mm.mm_process_run_lkp_vw\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load() #change\\ndpf_all_run_vw.createOrReplaceTempView('dpf_all_run_vw')\\n\\n\\n#Validations\\n\\n#Measure description from the input file is different from expected\\n\\nquery1 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT LINE_NUM, EXTRN_CODE, EXTRN_NAME, MEASR_NAME, MEASR_DESC, mil.MEASR_ID  FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}  \\n),\\ninput_msr AS (\\n  SELECT DISTINCT srce.EXTRN_CODE FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}\\n),\\nmsr_last_existing_run AS (\\nSELECT EXTRN_MEASR_ID, EXTRN_MEASR_NAME, RUN_ID FROM (\\nSELECT rp.*,\\n  row_number() over(PARTITION BY extrn_measr_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_measr_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = rp.run_id\\n  JOIN mm_cntrt_lkp c\\n  ON c.prcsg_id = r.prcsg_id\\n  WHERE r.run_sttus_id = 4 AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_msrs AS (\\n  SELECT input_msr.EXTRN_CODE FROM input_msr\\n  LEFT JOIN msr_last_existing_run ON input_msr.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID\\n  WHERE msr_last_existing_run.EXTRN_MEASR_ID IS NULL\\n)\\nSELECT DISTINCT ('Measure description from the input file is different from expected') as DQ, LINE_NUM , EXTRN_CODE , EXTRN_NAME AS DSCR, msr_last_existing_run.EXTRN_MEASR_NAME measr_name, MEASR_DESC\\nFROM srce \\nJOIN msr_last_existing_run ON srce.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID \\nWHERE srce.EXTRN_NAME <> msr_last_existing_run.EXTRN_MEASR_NAME\\nUNION ALL\\nSELECT DISTINCT ('Measure description from the input file is different from expected') as DQ, LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME AS DSCR, 'N/A' AS MEASR_NAME, MEASR_DESC\\nFROM srce\\nJOIN new_msrs ON srce.EXTRN_CODE = new_msrs.EXTRN_CODE\\\"\\\"\\\"\\ndf_inp_diff_exp = spark.sql(query1)\\n\\n\\n# Modified market description\\n\\nquery2 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT input.LINE_NUM, input.EXTRN_CODE, input.EXTRN_NAME, input.MKT_SKID_1 as mkt_skid, mkt.MKT_DESC FROM tier1_mkt_dsdim input             \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}' \\n),\\ninput_mkt AS (\\n  SELECT DISTINCT input.EXTRN_CODE FROM tier1_mkt_dsdim input  \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}'\\n),\\nmkt_last_existing_run AS (\\nSELECT EXTRN_MKT_ID, EXTRN_MKT_NAME, RUN_ID FROM (\\nSELECT rp.*,row_number() over(PARTITION BY extrn_mkt_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_mkt_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = rp.run_id\\n  JOIN mm_cntrt_lkp c\\n  ON c.prcsg_id = r.prcsg_id\\n  WHERE r.run_sttus_id = 4 AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_mkt AS (\\n  SELECT input_mkt.EXTRN_CODE FROM input_mkt \\n  LEFT JOIN mkt_last_existing_run ON input_mkt.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID\\n  WHERE mkt_last_existing_run.EXTRN_MKT_ID IS NULL\\n)\\nSELECT DISTINCT ('Modified market description') as DQ, LINE_NUM , EXTRN_CODE , EXTRN_NAME , mkt_last_existing_run.extrn_mkt_name as mkt_name, mkt_desc as mkt_desc\\nFROM srce \\nJOIN mkt_last_existing_run ON srce.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID \\nWHERE srce.EXTRN_NAME <> mkt_last_existing_run.EXTRN_MKT_NAME\\nUNION ALL\\nSELECT DISTINCT ('Modified market description') as DQ, LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME, 'N/A' AS mkt_name, mkt_desc \\nFROM srce\\nJOIN new_mkt ON srce.EXTRN_CODE = new_mkt.EXTRN_CODE\\\"\\\"\\\"\\n\\ndf_mod_mkt_desc = spark.sql(query2)\\n\\n\\n# Create mm_time_perd_assoc_tier1_vw\\nquery = \\\"\\\"\\\"with CAL_TYPE_2 as (\\nSELECT   2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_b,\\n          tb.time_perd_start_date time_perd_start_date_b,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_a,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          ta.time_perd_end_date time_perd_end_date_a,\\n          ta.time_perd_start_date time_perd_start_date_a\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n     WHERE assoc.CAL_TYPE_ID=2),\\nBIMTH_2_MTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_B TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_A TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          tb.time_perd_end_date time_perd_end_date_A,\\n          tb.time_perd_start_date time_perd_start_date_A\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\nMTH_2_BIMTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A ,\\n          assoc.TIME_PERD_ID_B ,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_A,\\n          ta.time_perd_end_date time_perd_end_date_A,\\n          ta.time_perd_start_date time_perd_start_date_A,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_B,\\n          tb.time_perd_start_date time_perd_start_date_B\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\n\\nBW_2_EB AS (SELECT  2 CAL_TYPE_ID,\\n          ta.EVEN_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.EVEN_BIMTH_TIME_PERD_CLASS_COD TIME_PERD_CLASS_CODE_A,\\n          ta.EVEN_BIMTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.EVEN_BIMTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.EVEN_BIMTH_START_DATE TIME_PERD_START_DATE_A\\n     FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='EB'\\n    ),\\n\\nBW_2_MTH AS (SELECT  2 CAL_TYPE_ID,\\n          ta.ODD_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.MTH_TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.MTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.MTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.MTH_START_DATE TIME_PERD_START_DATE_A\\n               FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='MTH'\\n    )\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM CAL_TYPE_2\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BIMTH_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_EB\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID, CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM MTH_2_BIMTH\\\"\\\"\\\"\\n\\ndf = spark.sql(query)\\n\\ndf.createOrReplaceTempView(\\\"mm_time_perd_assoc_tier1_vw\\\")\\n\\n# Missing product\\nquery3 = f\\\"\\\"\\\"WITH curr_run\\n     AS (SELECT /*+ materialize */ DISTINCT CASE \\n       WHEN tf.time_perd_class_code IN ('MTH') THEN tf.time_perd_end_date \\n       ELSE ta.time_perd_end_date_a END time_perd_end_date,\\n       3 srce_Sys_id,\\n       {tier1_cntrt_id} cntrt_id,\\n       '{tier1_fact_type_code}' fact_type_code,\\n       '{tier1_categ_id}' prod_prttn_code\\n           FROM tier1_time_mtrlz_tbl st JOIN mm_time_perd_id_lkp l ON \\n           l.extrn_time_perd_id = st.extrn_code and l.vendr_id={tier1_vendr_id}\\n           JOIN mm_time_perd_fdim tf on tf.time_perd_id = l.time_perd_id\\n           LEFT OUTER JOIN mm_time_perd_assoc_tier1_vw ta ON ta.time_perd_id_b = tf.time_perd_id\\n           AND ta.time_perd_type_code_b = tf.time_perd_type_code\\n           AND ta.time_perd_type_code_a = 'MH'\\n           \\n           ),\\n     all_finished_run\\n     AS (SELECT *\\n           FROM mm_run_prttn_plc plc\\n                JOIN dpf_all_run_vw run ON run.run_id = plc.run_id\\n          WHERE     plc.cntrt_id = {tier1_cntrt_id}\\n                AND plc.time_perd_class_code = 'MTH'\\n                AND run.run_sttus_id = 4),\\n     time_perd_before\\n     AS (SELECT MAX (mm_time_perd_end_date) mm_time_perd_end_date,\\n                MAX (srce_sys_id) srce_sys_id,\\n                MAX (cntrt_id) cntrt_id,\\n                MAX (fact_Type_code) fact_Type_code,\\n                MAX (prod_prttn_code) prod_prttn_code\\n           FROM all_finished_run\\n          WHERE mm_time_perd_end_date <\\n                   (SELECT MIN (time_perd_end_date)\\n                      FROM curr_run \\n                                ) \\n                                ),\\n     time_perds \\n     AS (  SELECT /*+ materialize */ * FROM (\\n                 SELECT time_perd_end_date AS mm_time_perd_end_date, \\n                        srce_sys_id,\\n                        cntrt_id,\\n                        fact_type_code,\\n                        prod_prttn_code\\n                 FROM curr_run\\n               UNION ALL\\n                 SELECT mm_time_perd_end_date,\\n                        srce_sys_id, \\n                        cntrt_id,\\n                        fact_type_code, \\n                        prod_prttn_code  \\n                 FROM time_perd_before\\n            )\\n          ),\\n     loaded_prod\\n     AS (SELECT /*+ materialize parallel(8) */\\n                DISTINCT prod_skid\\n           FROM mm_tp_mth_fct fct\\n                JOIN time_perds tp\\n                   ON     fct.mm_time_perd_end_date =\\n                             tp.mm_time_perd_end_date\\n                      AND fct.srce_sys_id = tp.srce_sys_id\\n                      AND fct.cntrt_id = tp.cntrt_id\\n                      AND fct.fact_Type_code = tp.fact_Type_code\\n                      AND fct.prod_prttn_code = tp.prod_prttn_code),\\n     xref\\n     AS (SELECT mm_prod_xref.*\\n           FROM mm_prod_xref inner join (SELECT DISTINCT srce_sys_id, cntrt_id FROM curr_run) distinct_curr_run on \\n          mm_prod_xref.srce_sys_id = distinct_curr_run.srce_sys_id and mm_prod_xref.cntrt_id = distinct_curr_run.cntrt_id\\n         )\\nSELECT ('Missing product') as DQ, xref.extrn_prod_id as extrn_prod_id , xref.extrn_prod_name as extrn_prod_name , xref.extrn_prod_attr_val_list as extrn_prod_attr_val_list , xref.prod_match_attr_list as prod_match_attr_list , xref.prod_prttn_code as prod_prttn_code , xref.prod_skid as prod_skid , xref.srce_sys_id as srce_sys_id , xref.cntrt_id as cntrt_id\\n  FROM xref\\n       JOIN loaded_prod ON xref.prod_skid = loaded_prod.prod_skid\\n       LEFT OUTER JOIN tier1_prod_dsdim prod_cur\\n          ON loaded_prod.prod_skid = prod_cur.prod_skid\\n       WHERE prod_cur.prod_skid IS  NULL\\n\\t   LIMIT 1\\\"\\\"\\\"\\ndf_miss_prod = spark.sql(query3)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\ndq1 = df_inp_diff_exp\\ndq2 = df_mod_mkt_desc\\ndq3 = df_miss_prod\\n\\ndf_combine = dq1.unionByName(dq2, True).unionByName(dq3, True)\\n\\ndata = []\\n\\ndq1_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'Measure description from the input file is different from expected' \\\", '', 'false', 'Measure description from the input file is different from expected', 100 )\\ndata.append(dq1_val)\\ndq2_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'Modified market description' \\\", '', 'false', 'Modified market description', 100 )\\ndata.append(dq2_val)\\ndq3_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'Missing product' \\\", '', 'false', 'Missing product', 100 )\\ndata.append(dq3_val)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\ndf_fyi = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\ndict_all_dfs['df_fyi'] = {\\\"df_object\\\" :df_fyi}\\ndf_output_dict['df_fyi'] = df_fyi\\n\\ndict_all_dfs['df_combine_fyi'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_fyi'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mm_prod_xref\"\n    },\n    {\n      \"name\": \"tier1_measr_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_time_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"mm_run_measr_plc\"\n    },\n    {\n      \"name\": \"tier1_mkt_dsdim\"\n    },\n    {\n      \"name\": \"tier1_prod_dsdim\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc_type\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc\"\n    },\n    {\n      \"name\": \"mm_run_mkt_plc\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fyi\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_fyi\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Load Fact",
      "predecessorName": "FYI Validations - v2",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"fileType\": \"parquet\",\n  \"inferSchema\": \"false\",\n  \"path\": \"<@@PATH1@@>MM_TP_<<TIME_PERD_CLASS_CODE>>_FCT/part_srce_sys_id=<<SRCE_SYS_ID>>/part_cntrt_id=<<CNTRT_ID>>/\",\n  \"addInputFileName\": \"false\",\n  \"semaphoreOption\": \"exclusive\",\n  \"createIfNotExist\": \"true\",\n  \"mergeSchema\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fact\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": false
    },
    {
      "operationName": "Read Schema",
      "predecessorName": "Load Fact",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\ndf_fct_schema = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"dbtable\\\", \\\"adwgp_mm.mm_tp_fct_schema\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\n    \\ndf_output_dict['df_fct_schema'] = df_fct_schema\\ndict_all_dfs['df_fct_schema'] = {\\\"df_object\\\" :df_fct_schema}\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dummy\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fct_schema\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "CC -Fact",
      "predecessorName": "Read Schema",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_fact= dict_all_dfs['df_fact'][\\\"df_object\\\"]\\ndf_fct_schema = dict_all_dfs['df_fct_schema'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import col\\n\\nlkp_cols = df_fact.columns\\nsdim_cols = df_fct_schema.columns\\n\\nfrom pyspark.sql.functions import lit\\nadd_cols = list(set(sdim_cols)-set(lkp_cols))\\nfor i in add_cols:\\n  df_fact = df_fact.withColumn(i,lit(None).cast('string'))\\n\\ndf_fact = df_fact.select(*sdim_cols)\\ncols = df_fact.columns\\n\\nfor j in cols:\\n  if dict(df_fact.dtypes)[j] != dict(df_fct_schema.dtypes)[j]:\\n    df_fact = df_fact.withColumn(j, col(j).cast(dict(df_fct_schema.dtypes)[j]))\\n\\ndict_all_dfs['df_fact'] = {\\\"df_object\\\" :df_fact}\\ndf_output_dict['df_fact'] = df_fact\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_fact\"\n    },\n    {\n      \"name\": \"df_fct_schema\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fact\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Release Semaphore - Fact",
      "predecessorName": "CC -Fact",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"actionType\": \"release\",\n  \"itemType\": \"path\",\n  \"itemPath\": \"/mnt/<@@PATH1@@>MM_TP_<<TIME_PERD_CLASS_CODE>>_FCT/part_srce_sys_id=<<SRCE_SYS_ID>>/part_cntrt_id=<<CNTRT_ID>>/\"\n}",
      "operationVersionName": "SemaphoreOperation",
      "overridableIndicator": false
    },
    {
      "operationName": "Load FDIM",
      "predecessorName": "Release Semaphore - Fact",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"fileType\": \"parquet\",\n  \"inferSchema\": \"false\",\n  \"path\": \"<@@PATH1@@>MM_TIME_PERD_FDIM_VW/\",\n  \"addInputFileName\": \"false\",\n  \"semaphoreOption\": \"exclusive\",\n  \"createIfNotExist\": \"true\",\n  \"mergeSchema\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"outputDataframes\": [\n    {\n      \"name\": \"mm_time_perd_fdim\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": false
    },
    {
      "operationName": "Release FDIM",
      "predecessorName": "Load FDIM",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"actionType\": \"release\",\n  \"itemType\": \"path\",\n  \"itemPath\": \"/mnt/<@@PATH1@@>MM_TIME_PERD_FDIM_VW/\"\n}",
      "operationVersionName": "SemaphoreOperation",
      "overridableIndicator": false
    },
    {
      "operationName": "Load MM_RUN_PRTTN_PLC",
      "predecessorName": "Release FDIM",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"fileType\": \"parquet\",\n  \"inferSchema\": \"false\",\n  \"path\": \"<@@PATH1@@>MM_RUN_PRTTN_PLC/\",\n  \"addInputFileName\": \"false\",\n  \"semaphoreOption\": \"exclusive\",\n  \"createIfNotExist\": \"true\",\n  \"mergeSchema\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"outputDataframes\": [\n    {\n      \"name\": \"mm_run_prttn_plc\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": false
    },
    {
      "operationName": "Release MM_RUN_PRTTN_PLC",
      "predecessorName": "Load MM_RUN_PRTTN_PLC",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"actionType\": \"release\",\n  \"itemType\": \"path\",\n  \"itemPath\": \"/mnt/<@@PATH1@@>MM_RUN_PRTTN_PLC/\"\n}",
      "operationVersionName": "SemaphoreOperation",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI Validations and Report Generation",
      "predecessorName": "Release MM_RUN_PRTTN_PLC",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = <<VENDOR_ID>>\\ntier1_srce_sys_id = <<SRCE_SYS_ID>>\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = <<CNTRT_ID>>\\ntier1_cntry_id = '<<ISO_CNTRY_CODE>>'\\ntier1_categ_id = \\\"<<CATEGORY_ID>>\\\"\\ntime_perd_class_code = '<<TIME_PERD_CLASS_CODE>>'\\ntime_perd_type_code =  '<<TIME_PERD_TYPE_CODE>>'\\n\\n#Summary of Report\\ndf_file_struct_summary = dict_all_dfs['df_file_struct_summary'][\\\"df_object\\\"]\\ndf_ref_vendors_summary = dict_all_dfs['df_ref_vendors_summary'][\\\"df_object\\\"]\\n\\n# Dataframes from Prior Steps\\n\\nmm_run_prttn_plc = dict_all_dfs['mm_run_prttn_plc'][\\\"df_object\\\"]\\nmm_time_perd_fdim = dict_all_dfs['mm_time_perd_fdim'][\\\"df_object\\\"]\\ndf_vld_dtls = dict_all_dfs['df_vld_dtls'][\\\"df_object\\\"]\\n\\nmm_tp_fct = dict_all_dfs['df_fact'][\\\"df_object\\\"]\\nmm_prod_xref = dict_all_dfs['mm_prod_xref'][\\\"df_object\\\"]\\ntier1_measr_mtrlz_tbl = dict_all_dfs['tier1_measr_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_time_mtrlz_tbl = dict_all_dfs['tier1_time_mtrlz_tbl'][\\\"df_object\\\"]\\nmm_run_measr_plc = dict_all_dfs['mm_run_measr_plc'][\\\"df_object\\\"]\\ntier1_mkt_dsdim = dict_all_dfs['tier1_mkt_dsdim'][\\\"df_object\\\"]\\ntier1_prod_dsdim = dict_all_dfs['tier1_prod_dsdim'][\\\"df_object\\\"]\\nmm_time_perd_assoc_type = dict_all_dfs['mm_time_perd_assoc_type'][\\\"df_object\\\"]\\nmm_time_perd_assoc = dict_all_dfs['mm_time_perd_assoc'][\\\"df_object\\\"]\\nmm_run_mkt_plc = dict_all_dfs['mm_run_mkt_plc'][\\\"df_object\\\"]\\ntier1_mkt_mtrlz_tbl = dict_all_dfs['tier1_mkt_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_prod_mtrlz_tbl = dict_all_dfs['tier1_prod_mtrlz_tbl'][\\\"df_object\\\"]\\n\\ntier1_prod_gav = dict_all_dfs['tier1_prod_gav'][\\\"df_object\\\"]\\ntier1_prod_gav.createOrReplaceTempView('tier1_prod_gav')\\n\\nmm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\nmm_time_perd_fdim.createOrReplaceTempView('mm_time_perd_fdim')\\nmm_tp_fct.createOrReplaceTempView('mm_tp_mth_fct')\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\ntier1_measr_mtrlz_tbl.createOrReplaceTempView('tier1_measr_mtrlz_tbl')\\ntier1_time_mtrlz_tbl.createOrReplaceTempView('tier1_time_mtrlz_tbl')\\nmm_run_measr_plc.createOrReplaceTempView('mm_run_measr_plc')\\ntier1_mkt_dsdim.createOrReplaceTempView('tier1_mkt_dsdim')\\ntier1_prod_dsdim.createOrReplaceTempView('tier1_prod_dsdim')\\nmm_time_perd_assoc_type.createOrReplaceTempView('mm_time_perd_assoc_type')\\nmm_time_perd_assoc.createOrReplaceTempView('mm_time_perd_assoc')\\nmm_run_mkt_plc.createOrReplaceTempView('mm_run_mkt_plc')\\n\\ntier1_mkt_mtrlz_tbl.createOrReplaceTempView('tier1_mkt_mtrlz_tbl')\\ntier1_prod_mtrlz_tbl.createOrReplaceTempView('tier1_prod_mtrlz_tbl')\\n\\n# tables from Postgres\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\n#MM_COMB_RULE_PRC_VLDN_VW\\nMM_COMB_RULE_PRC_VLDN_VW = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.MM_COMB_RULE_PRC_VLDN_VW\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nMM_COMB_RULE_PRC_VLDN_VW.createOrReplaceTempView('MM_COMB_RULE_PRC_VLDN_VW')\\n\\n# mm_measr_id_lkp\\nmm_measr_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_id_lkp.createOrReplaceTempView('mm_measr_id_lkp')\\n\\n# mm_measr_lkp\\nmm_measr_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_measr_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_measr_lkp.createOrReplaceTempView('mm_measr_lkp')\\n\\n# mm_cntrt_lkp\\nmm_cntrt_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_cntrt_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_cntrt_lkp.createOrReplaceTempView('mm_cntrt_lkp')\\n\\n# mm_time_perd_id_lkp\\nmm_time_perd_id_lkp = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_time_perd_id_lkp\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_time_perd_id_lkp.createOrReplaceTempView('mm_time_perd_id_lkp')\\n\\n# mm_time_perd_fdim\\n#mm_time_perd_fdim = spark.read.format(\\\"parquet\\\").load('/mnt/prod-tp-lightrefined/MM_TIME_PERD_FDIM_VW/')\\n#mm_time_perd_fdim.createOrReplaceTempView('mm_time_perd_fdim')\\n\\n# mm_run_prttn_plc\\n#mm_run_prttn_plc = spark.read.format(\\\"parquet\\\").load('/mnt/prod-tp-lightrefined/MM_RUN_PRTTN_PLC/')\\n#mm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\n\\n# mm_mkt_dim\\nmm_mkt_dim = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_mkt_dim\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_mkt_dim.createOrReplaceTempView('mm_mkt_dim')\\n\\n# dpf_all_run_vw\\ndpf_all_run_vw = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select run_id, process_status as run_sttus_id, prcsg_id as prcsg_id from adwgp_mm.dpf_all_run_lref_vw\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load() #change\\ndpf_all_run_vw.createOrReplaceTempView('dpf_all_run_vw')\\n\\n\\n#Validations\\n\\n#Measure description from the input file is different from expected\\n\\nquery1 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT LINE_NUM, EXTRN_CODE, EXTRN_NAME, MEASR_NAME, MEASR_DESC, mil.MEASR_ID  FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE  and mil.fact_type_code in ('TP', '[U]')\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}  \\n),\\ninput_msr AS (\\n  SELECT DISTINCT srce.EXTRN_CODE FROM tier1_measr_mtrlz_tbl srce\\n  JOIN MM_MEASR_ID_LKP mil ON mil.EXTRN_MEASR_ID = srce.EXTRN_CODE  and mil.fact_type_code in ('TP', '[U]')\\n  JOIN MM_MEASR_LKP ml ON mil.MEASR_ID = ml.MEASR_ID AND ml.use_ind='Y' AND mil.VENDR_ID = {tier1_vendr_id}\\n),\\nmsr_last_existing_run AS (\\nSELECT EXTRN_MEASR_ID, EXTRN_MEASR_NAME, RUN_ID FROM (\\nSELECT rp.*,\\n  row_number() over(PARTITION BY extrn_measr_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_measr_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = cast(rp.run_id as decimal(38,10))\\n  JOIN mm_cntrt_lkp c\\n  ON c.cntrt_id = r.prcsg_id\\n  WHERE r.run_sttus_id in('COMPLETED', 'FINISHED') AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_msrs AS (\\n  SELECT input_msr.EXTRN_CODE FROM input_msr\\n  LEFT JOIN msr_last_existing_run ON input_msr.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID\\n  WHERE msr_last_existing_run.EXTRN_MEASR_ID IS NULL\\n)\\nSELECT DISTINCT ('Measure description from the input file is different from expected') as DQ, LINE_NUM , EXTRN_CODE , EXTRN_NAME AS DSCR, msr_last_existing_run.EXTRN_MEASR_NAME measr_name, MEASR_DESC\\nFROM srce \\nJOIN msr_last_existing_run ON srce.EXTRN_CODE = msr_last_existing_run.EXTRN_MEASR_ID \\nWHERE srce.EXTRN_NAME <> msr_last_existing_run.EXTRN_MEASR_NAME\\nUNION ALL\\nSELECT DISTINCT ('Measure description from the input file is different from expected') as DQ, LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME AS DSCR, 'N/A' AS MEASR_NAME, MEASR_DESC\\nFROM srce\\nJOIN new_msrs ON srce.EXTRN_CODE = new_msrs.EXTRN_CODE\\\"\\\"\\\"\\ndf_inp_diff_exp = spark.sql(query1)\\n\\n\\n# Modified market description\\n\\nquery2 = f\\\"\\\"\\\"WITH srce AS (\\n  SELECT input.LINE_NUM, input.EXTRN_CODE, input.EXTRN_NAME, input.MKT_SKID_1 as mkt_skid, mkt.MKT_DESC FROM tier1_mkt_dsdim input             \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}' \\n),\\ninput_mkt AS (\\n  SELECT DISTINCT input.EXTRN_CODE FROM tier1_mkt_dsdim input  \\n  JOIN mm_mkt_dim mkt ON mkt.mkt_skid = input.mkt_skid_1\\n  AND mkt.srce_sys_id = input.srce_sys_id\\n  AND (mkt.cntrt_id =0) AND 'TP' = '{tier1_fact_type_code}'\\n),\\nmkt_last_existing_run AS (\\nSELECT EXTRN_MKT_ID, EXTRN_MKT_NAME, RUN_ID FROM (\\nSELECT rp.*,row_number() over(PARTITION BY extrn_mkt_id ORDER BY rp.run_id DESC) rn\\n  FROM mm_run_mkt_plc rp\\n  JOIN dpf_all_run_vw r\\n  ON r.run_id = cast(rp.run_id as decimal(38,10))\\n  JOIN mm_cntrt_lkp c\\n  ON c.cntrt_id = r.prcsg_id\\n  WHERE r.run_sttus_id IN ('COMPLETED', 'FINISHED') AND c.cntrt_id =  {tier1_cntrt_id}\\n  )\\nWHERE rn = 1\\n),\\nnew_mkt AS (\\n  SELECT input_mkt.EXTRN_CODE FROM input_mkt \\n  LEFT JOIN mkt_last_existing_run ON input_mkt.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID\\n  WHERE mkt_last_existing_run.EXTRN_MKT_ID IS NULL\\n)\\nSELECT DISTINCT ('Modified market description') as DQ, LINE_NUM , EXTRN_CODE , EXTRN_NAME , mkt_last_existing_run.extrn_mkt_name as mkt_name, mkt_desc as mkt_desc\\nFROM srce \\nJOIN mkt_last_existing_run ON srce.EXTRN_CODE = mkt_last_existing_run.EXTRN_MKT_ID \\nWHERE srce.EXTRN_NAME <> mkt_last_existing_run.EXTRN_MKT_NAME\\nUNION ALL\\nSELECT DISTINCT ('Modified market description') as DQ, LINE_NUM, srce.EXTRN_CODE, EXTRN_NAME, 'N/A' AS mkt_name, mkt_desc \\nFROM srce\\nJOIN new_mkt ON srce.EXTRN_CODE = new_mkt.EXTRN_CODE\\\"\\\"\\\"\\n\\ndf_mod_mkt_desc = spark.sql(query2)\\n\\n\\n# Create mm_time_perd_assoc_tier1_vw\\nquery = \\\"\\\"\\\"with CAL_TYPE_2 as (\\nSELECT   2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_b,\\n          tb.time_perd_start_date time_perd_start_date_b,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_a,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          ta.time_perd_end_date time_perd_end_date_a,\\n          ta.time_perd_start_date time_perd_start_date_a\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n     WHERE assoc.CAL_TYPE_ID=2),\\nBIMTH_2_MTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_B TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_A TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          tb.time_perd_end_date time_perd_end_date_A,\\n          tb.time_perd_start_date time_perd_start_date_A\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\nMTH_2_BIMTH AS (\\n SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A ,\\n          assoc.TIME_PERD_ID_B ,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_A,\\n          ta.time_perd_end_date time_perd_end_date_A,\\n          ta.time_perd_start_date time_perd_start_date_A,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_B,\\n          tb.time_perd_start_date time_perd_start_date_B\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\n),\\n\\nBW_2_EB AS (SELECT  2 CAL_TYPE_ID,\\n          ta.EVEN_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.EVEN_BIMTH_TIME_PERD_CLASS_COD TIME_PERD_CLASS_CODE_A,\\n          ta.EVEN_BIMTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.EVEN_BIMTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.EVEN_BIMTH_START_DATE TIME_PERD_START_DATE_A\\n     FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='EB'\\n    ),\\n\\nBW_2_MTH AS (SELECT  2 CAL_TYPE_ID,\\n          ta.ODD_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.MTH_TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.MTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.MTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.MTH_START_DATE TIME_PERD_START_DATE_A\\n               FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='MTH'\\n    )\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM CAL_TYPE_2\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BIMTH_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_EB\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID, CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM MTH_2_BIMTH\\\"\\\"\\\"\\n\\ndf = spark.sql(query)\\n\\ndf.createOrReplaceTempView(\\\"mm_time_perd_assoc_tier1_vw\\\")\\n\\n# Missing product\\nquery3 = f\\\"\\\"\\\"WITH curr_run\\n     AS (SELECT /*+ materialize */ DISTINCT CASE \\n       WHEN tf.time_perd_class_code IN ('{time_perd_class_code}') THEN tf.time_perd_end_date \\n       ELSE ta.time_perd_end_date_a END time_perd_end_date,\\n       3 srce_Sys_id,\\n       {tier1_cntrt_id} cntrt_id,\\n       '{tier1_fact_type_code}' fact_type_code,\\n       '{tier1_categ_id}' prod_prttn_code\\n           FROM tier1_time_mtrlz_tbl st JOIN mm_time_perd_id_lkp l ON \\n           l.extrn_time_perd_id = st.extrn_code and l.vendr_id={tier1_vendr_id}\\n           JOIN mm_time_perd_fdim tf on tf.time_perd_id = l.time_perd_id\\n           LEFT OUTER JOIN mm_time_perd_assoc_tier1_vw ta ON ta.time_perd_id_b = tf.time_perd_id\\n           AND ta.time_perd_type_code_b = tf.time_perd_type_code\\n           AND ta.time_perd_type_code_a = '{time_perd_type_code}'\\n           \\n           ),\\n     all_finished_run\\n     AS (SELECT *\\n           FROM mm_run_prttn_plc plc\\n                JOIN dpf_all_run_vw run ON cast(run.run_id  as decimal(38,10)) = cast(plc.run_id as decimal(38,10))\\n          WHERE     plc.cntrt_id = {tier1_cntrt_id}\\n                AND plc.time_perd_class_code = '{time_perd_class_code}'\\n                AND run.run_sttus_id IN ('COMPLETED', 'FINISHED')),\\n     time_perd_before\\n     AS (SELECT MAX (mm_time_perd_end_date) mm_time_perd_end_date,\\n                MAX (srce_sys_id) srce_sys_id,\\n                MAX (cntrt_id) cntrt_id,\\n                MAX (fact_Type_code) fact_Type_code,\\n                MAX (prod_prttn_code) prod_prttn_code\\n           FROM all_finished_run\\n          WHERE mm_time_perd_end_date <\\n                   (SELECT MIN (time_perd_end_date)\\n                      FROM curr_run \\n                                ) \\n                                ),\\n     time_perds \\n     AS (  SELECT /*+ materialize */ * FROM (\\n                 SELECT time_perd_end_date AS mm_time_perd_end_date, \\n                        srce_sys_id,\\n                        cntrt_id,\\n                        fact_type_code,\\n                        prod_prttn_code\\n                 FROM curr_run\\n               UNION ALL\\n                 SELECT mm_time_perd_end_date,\\n                        srce_sys_id, \\n                        cntrt_id,\\n                        fact_type_code, \\n                        prod_prttn_code  \\n                 FROM time_perd_before\\n            )\\n          ),\\n     loaded_prod\\n     AS (SELECT /*+ materialize parallel(8) */\\n                DISTINCT prod_skid\\n           FROM mm_tp_mth_fct fct\\n                JOIN time_perds tp\\n                   ON     fct.mm_time_perd_end_date =\\n                             tp.mm_time_perd_end_date\\n                      AND fct.srce_sys_id = tp.srce_sys_id\\n                      AND fct.cntrt_id = tp.cntrt_id\\n                      AND fct.fact_Type_code = tp.fact_Type_code\\n                      AND fct.prod_prttn_code = tp.prod_prttn_code),\\n     xref\\n     AS (SELECT mm_prod_xref.*\\n           FROM mm_prod_xref inner join (SELECT DISTINCT srce_sys_id, cntrt_id FROM curr_run) distinct_curr_run on \\n          mm_prod_xref.srce_sys_id = distinct_curr_run.srce_sys_id and mm_prod_xref.cntrt_id = distinct_curr_run.cntrt_id\\n         )\\nSELECT ('Missing product') as DQ, xref.extrn_prod_id as extrn_prod_id , xref.extrn_prod_name as extrn_prod_name , xref.extrn_prod_attr_val_list as extrn_prod_attr_val_list , xref.prod_match_attr_list as prod_match_attr_list , xref.prod_prttn_code as prod_prttn_code , xref.prod_skid as prod_skid , xref.srce_sys_id as srce_sys_id , xref.cntrt_id as cntrt_id\\n  FROM xref\\n       JOIN loaded_prod ON xref.prod_skid = loaded_prod.prod_skid\\n       LEFT OUTER JOIN tier1_prod_dsdim prod_cur\\n          ON loaded_prod.prod_skid = prod_cur.prod_skid\\n       WHERE prod_cur.prod_skid IS  NULL\\n\\t   \\\"\\\"\\\"\\ndf_miss_prod = spark.sql(query3)\\n\\n# 4. The same brand placed under two companies\\n\\nquery5 = f\\\"\\\"\\\"SELECT \\n('The same brand placed under two companies') as DQ,\\nLINE_NUM LINE_NUM, PROD_SKID PROD_SKID, \\nEXTRN_CODE EXTRN_CODE, \\nEXTRN_PROD_NAME 5EXTRN_PROD_NAME, \\nbrand_name brand_name, company_name company_name,\\nEXTRN_PROD_ATTR_VAL_LIST EXTRN_PROD_ATTR_VAL_LIST \\nFROM (\\n             SELECT DISTINCT LINE_NUM, PROD_SKID, EXTRN_CODE, EXTRN_PROD_NAME, brand_name, company_name,EXTRN_PROD_ATTR_VAL_LIST FROM (\\n     WITH TB AS (SELECT LINE_NUM, PROD_SKID, EXTRN_CODE, EXTRN_PROD_NAME, EXTRN_PROD_ATTR_VAL_LIST,\\n     CASE \\n      WHEN attr_phys_name_1 = 'PG_BRAND_TXT' THEN attr_code_1  \\n      WHEN attr_phys_name_2 = 'PG_BRAND_TXT' THEN attr_code_2\\n      WHEN attr_phys_name_3 = 'PG_BRAND_TXT' THEN attr_code_3\\n      WHEN attr_phys_name_4 = 'PG_BRAND_TXT' THEN attr_code_4\\n      WHEN attr_phys_name_5 = 'PG_BRAND_TXT' THEN attr_code_5\\n      WHEN attr_phys_name_6 = 'PG_BRAND_TXT' THEN attr_code_6\\n      WHEN attr_phys_name_7 = 'PG_BRAND_TXT' THEN attr_code_7\\n      WHEN attr_phys_name_8 = 'PG_BRAND_TXT' THEN attr_code_8\\n    --  WHEN attr_phys_name_9 = 'PG_BRAND_TXT' THEN attr_code_9\\n     END brand,\\n     CASE \\n      WHEN attr_phys_name_1 = 'PG_BRAND_TXT' THEN attr_val_1  \\n      WHEN attr_phys_name_2 = 'PG_BRAND_TXT' THEN attr_val_2\\n      WHEN attr_phys_name_3 = 'PG_BRAND_TXT' THEN attr_val_3\\n      WHEN attr_phys_name_4 = 'PG_BRAND_TXT' THEN attr_val_4\\n      WHEN attr_phys_name_5 = 'PG_BRAND_TXT' THEN attr_val_5\\n      WHEN attr_phys_name_6 = 'PG_BRAND_TXT' THEN attr_val_6\\n      WHEN attr_phys_name_7 = 'PG_BRAND_TXT' THEN attr_val_7\\n      WHEN attr_phys_name_8 = 'PG_BRAND_TXT' THEN attr_val_8\\n    --  WHEN attr_phys_name_9 = 'PG_BRAND_TXT' THEN attr_val_9\\n     END brand_name,\\n     CASE \\n      WHEN attr_phys_name_1 = 'PG_MFGR_TXT' THEN attr_code_1  \\n      WHEN attr_phys_name_2 = 'PG_MFGR_TXT' THEN attr_code_2\\n      WHEN attr_phys_name_3 = 'PG_MFGR_TXT' THEN attr_code_3\\n      WHEN attr_phys_name_4 = 'PG_MFGR_TXT' THEN attr_code_4\\n      WHEN attr_phys_name_5 = 'PG_MFGR_TXT' THEN attr_code_5\\n      WHEN attr_phys_name_6 = 'PG_MFGR_TXT' THEN attr_code_6\\n      WHEN attr_phys_name_7 = 'PG_MFGR_TXT' THEN attr_code_7\\n      WHEN attr_phys_name_8 = 'PG_MFGR_TXT' THEN attr_code_8\\n    --  WHEN attr_phys_name_9 = 'PG_MFGR_TXT' THEN attr_code_9\\n     END company,\\n\\t CASE \\n      WHEN attr_phys_name_1 = 'PG_MFGR_TXT' THEN attr_val_1  \\n      WHEN attr_phys_name_2 = 'PG_MFGR_TXT' THEN attr_val_2\\n      WHEN attr_phys_name_3 = 'PG_MFGR_TXT' THEN attr_val_3\\n      WHEN attr_phys_name_4 = 'PG_MFGR_TXT' THEN attr_val_4\\n      WHEN attr_phys_name_5 = 'PG_MFGR_TXT' THEN attr_val_5\\n      WHEN attr_phys_name_6 = 'PG_MFGR_TXT' THEN attr_val_6\\n      WHEN attr_phys_name_7 = 'PG_MFGR_TXT' THEN attr_val_7\\n      WHEN attr_phys_name_8 = 'PG_MFGR_TXT' THEN attr_val_8\\n    --  WHEN attr_phys_name_9 = 'PG_MFGR_TXT' THEN attr_val_9\\n     END company_name\\n      FROM tier1_prod_gav WHERE PROD_LVL_NAME ='BRAND')\\n\\t SELECT LINE_NUM, PROD_SKID, EXTRN_CODE, EXTRN_PROD_NAME, brand, company, brand_name, company_name, EXTRN_PROD_ATTR_VAL_LIST, approx_count_distinct(company) OVER (PARTITION BY brand) NR\\n     FROM TB\\n     ) WHERE NR>1 AND brand IS NOT NULL AND company IS NOT NULL\\n\\t )\\n--WHERE limit 100 \\n--ORDER BY brand_name\\\"\\\"\\\"\\n\\nprod_attr_brand_mlt = spark.sql(query5).orderBy('brand_name')\\n\\n# Skipped measures\\nquery5 = f\\\"\\\"\\\"SELECT (\\\"Skipped measures\\\") as DQ, measr.LINE_NUM, measr.EXTRN_CODE SUPPLIER_TAG, measr.extrn_name SUPPLIER_DESCRIPTION\\n\\n    FROM tier1_measr_mtrlz_tbl measr\\n    JOIN MM_MEASR_ID_LKP ON EXTRN_MEASR_ID = EXTRN_CODE AND MM_MEASR_ID_LKP.VENDR_ID = {tier1_vendr_id}\\n\\tand (measr_id = 1 and fact_type_code = 'TP' or FACT_TYPE_CODE = '[U]') \\\"\\\"\\\"\\n\\nskip_measr = spark.sql(query5)\\n\\n#6 Bad character in the input of the product file\\n# Common for all the Bad character validations\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndef find_bad_ascii(x):\\n    return str(x).isascii()\\n\\t\\nascii_udf = udf(find_bad_ascii)\\n\\nschema = StructType([ \\n    StructField(\\\"DimensionTag\\\",StringType(),True),\\n    StructField(\\\"IMPACTED_COLUMN\\\",StringType(),True),\\n    StructField(\\\"COLMN_VALUE\\\",StringType(),True)\\n  ])\\n  \\n#Product\\ntier1_prod_mtrlz_tbl.createOrReplaceTempView('temp')\\ndf_bad = spark.sql(\\\"\\\"\\\" select *, concat(*) as concat_str from temp\\\"\\\"\\\")\\ndf_bad = df_bad.filter(ascii_udf('concat_str')!= True).drop('concat_str')\\ncl = df_bad.collect()\\n  \\ndata = []\\nfor i in cl:\\n    for key, value in i.asDict().items():\\n        if str(value).isascii()== False:\\n            data.append(('Product',key,value))\\nprint(data)\\nbad_product = spark.createDataFrame(data,schema)\\n\\n#Market\\ntier1_mkt_mtrlz_tbl.createOrReplaceTempView('temp')\\ndf_bad = spark.sql(\\\"\\\"\\\" select *, concat(*) as concat_str from temp\\\"\\\"\\\")\\ndf_bad = df_bad.filter(ascii_udf('concat_str')!= True).drop('concat_str')\\ncl = df_bad.collect()\\n  \\ndata = []\\nfor i in cl:\\n    for key, value in i.asDict().items():\\n        if str(value).isascii()== False:\\n            data.append(('Market',key,value))\\nprint(data)\\nbad_market = spark.createDataFrame(data,schema)\\n\\n#Measure\\ntier1_measr_mtrlz_tbl.createOrReplaceTempView('temp')\\ndf_bad = spark.sql(\\\"\\\"\\\" select *, concat(*) as concat_str from temp\\\"\\\"\\\")\\ndf_bad = df_bad.filter(ascii_udf('concat_str')!= True).drop('concat_str')\\ncl = df_bad.collect()\\n  \\ndata = []\\nfor i in cl:\\n    for key, value in i.asDict().items():\\n        if str(value).isascii()== False:\\n            data.append(('Measure',key,value))\\nprint(data)\\nbad_measure = spark.createDataFrame(data,schema)\\n\\n#Time\\ntier1_time_mtrlz_tbl.createOrReplaceTempView('temp')\\ndf_bad = spark.sql(\\\"\\\"\\\" select *, concat(*) as concat_str from temp\\\"\\\"\\\")\\ndf_bad = df_bad.filter(ascii_udf('concat_str')!= True).drop('concat_str')\\ncl = df_bad.collect()\\n  \\ndata = []\\nfor i in cl:\\n    for key, value in i.asDict().items():\\n        if str(value).isascii()== False:\\n            data.append(('Time',key,value))\\nprint(data)\\nbad_time = spark.createDataFrame(data,schema)\\n\\n# Invalid combination for attribute for product\\n\\n# levels\\ndata = [[1],[2],[3],[4],[5],[6],[7],[8]] \\ncolumns = ['unpivot_row'] \\ndf_level=spark.createDataFrame(data, columns) \\ndf_level.createOrReplaceTempView(\\\"temp_level\\\")\\n\\ndf_inval_comb = spark.sql(\\\"\\\"\\\" \\nwith temp1 as (SELECT * from tier1_prod_gav ) ,\\ntemp2 as  (SELECT unpivot_row as level FROM temp_level),\\ntemp3(select t1.*,t2.* from temp1 t1 cross join temp2 t2 ),\\nsrc(SELECT categ_id, prod_skid,line_num,\\nDECODE(level, 1, attr_code_1, 2, attr_code_2,3, attr_code_3,4, attr_code_4,5, attr_code_5,6, attr_code_6,7, attr_code_7,8, attr_code_8,null) AS attr_code,\\nDECODE(level, 1, attr_phys_name_1,2, attr_phys_name_2,3, attr_phys_name_3,4, attr_phys_name_4,5, attr_phys_name_5,6, attr_phys_name_6,7, attr_phys_name_7,8, attr_phys_name_8,null) AS attr_name from temp3),\\nsrcd as (\\nselect s1.line_num, s1.categ_id, s1.attr_code attr_code_1, s1.attr_name attr_name_1, s2.attr_code attr_code_2, s2.attr_name attr_name_2 from src s1 join src s2 on s1.prod_skid = s2.prod_skid\\nand s1.attr_name!= s2.attr_name\\nand s1.attr_Code is not null\\nand s2.attr_Code is not null\\n),\\nerr AS (\\nSELECT srcd.LINE_NUM, srcd.CATEG_ID, ATTR_CODE_1, ATTR_NAME_1, ATTR_CODE_2, ATTR_NAME_2, ATTR_PHYS_NAME_1, OPRTR_NAME_1, PROD_ATTR_VAL_CODE_LIST_1, ATTR_PHYS_NAME_2, OPRTR_NAME_2,\\nPROD_ATTR_VAL_CODE_LIST_2 FROM srcd JOIN MM_COMB_RULE_PRC_VLDN_VW cmb\\nON srcd.categ_id = cmb.categ_id\\nAND ATTR_NAME_1 = ATTR_PHYS_NAME_1\\nAND ATTR_NAME_2 = ATTR_PHYS_NAME_2),\\nret as (SELECT (\\\"Invalid combination for attribute for product\\\") as DQ, LINE_NUM, CATEG_ID, ATTR_CODE_1, ATTR_NAME_1, ATTR_CODE_2, ATTR_NAME_2 FROM err\\nWHERE\\nREGEXP_LIKE(attr_code_1, prod_attr_val_code_list_1) AND OPRTR_NAME_1 = 'IN'\\nAND NOT REGEXP_LIKE(attr_code_2, prod_attr_val_code_list_2) AND OPRTR_NAME_2 = 'IN'\\nOR\\nNOT REGEXP_LIKE(attr_code_1, prod_attr_val_code_list_1) AND OPRTR_NAME_1 = 'NOT IN'\\nAND NOT REGEXP_LIKE(attr_code_2, prod_attr_val_code_list_2) AND OPRTR_NAME_2 = 'IN'\\nOR\\nREGEXP_LIKE(attr_code_1, prod_attr_val_code_list_1) AND OPRTR_NAME_1 = 'IN'\\nAND NOT attr_code_2 LIKE prod_attr_val_code_list_2 AND OPRTR_NAME_2 = 'LIKE'\\nOR\\nNOT REGEXP_LIKE(attr_code_1, prod_attr_val_code_list_1) AND OPRTR_NAME_1 = 'NOT IN'\\nAND NOT attr_code_2 LIKE prod_attr_val_code_list_2 AND OPRTR_NAME_2 = 'LIKE'\\nOR\\nREGEXP_LIKE(attr_code_1, prod_attr_val_code_list_1) AND OPRTR_NAME_1 = 'IN'\\nAND REGEXP_LIKE(attr_code_2, prod_attr_val_code_list_2) AND OPRTR_NAME_2 = 'NOT IN'\\nOR\\nREGEXP_LIKE(attr_code_1, prod_attr_val_code_list_1) AND OPRTR_NAME_1 = 'IN'\\nAND attr_code_2 LIKE prod_attr_val_code_list_2 AND OPRTR_NAME_2 = 'NOT LIKE')\\nselect * from ret\\n\\\"\\\"\\\")\\n\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\ndq1 = df_inp_diff_exp\\ndq2 = df_mod_mkt_desc\\ndq3 = df_miss_prod\\ndq4 = prod_attr_brand_mlt\\ndq5 = skip_measr\\ndq6 = bad_product\\ndq7 = bad_market\\ndq8 = bad_measure\\ndq9 = bad_time\\ndq10 = df_inval_comb\\n\\ndf_combine = dq1.unionByName(dq2, True).unionByName(dq3, True).unionByName(dq4, True).unionByName(dq5, True).unionByName(dq6, True).unionByName(dq7, True).unionByName(dq8, True).unionByName(dq9, True).unionByName(dq10, True)\\n\\n#Report Generation\\n\\nimport subprocess\\nimport os\\nimport shutil\\nimport subprocess\\nimport sys\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\nimport pandas as pd\\nsubprocess.check_call([sys.executable,\\\"-m\\\",\\\"pip\\\",\\\"install\\\",\\\"xlsxwriter\\\"])\\nsubprocess.check_call([sys.executable,\\\"-m\\\",\\\"pip\\\",\\\"install\\\",\\\"openpyxl\\\"])\\nimport xlsxwriter\\nimport openpyxl\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\nrpt_path = '<@@PATH1@@>'\\n\\nwriter = pd.ExcelWriter(f'tp_dvm_rprt_{run_id}.xlsx', engine='xlsxwriter')\\n\\n# Prepare KPI\\ndata = []\\n\\ndq_fyi_val = ('For Your Information Validations','','')\\ndata.append(dq_fyi_val)\\n\\nif dq1.count()==0:\\n  dq_val=('Measure description from the input file is different from expected', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Measure description from the input file is different from expected', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL1!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq2.count()==0:\\n  dq_val=('Modified market description', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Modified market description', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL2!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq3.count()==0:\\n  dq_val=('Missing product', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Missing product', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL3!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq4.count()==0:\\n  dq_val=('The same brand placed under two companies', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('The same brand placed under two companies', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL4!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq5.count()==0:\\n  dq_val=('Skipped measures', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Skipped measures', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL5!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq6.count()==0:\\n  dq_val=('Bad character in the input of the product file', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Bad character in the input of the product file', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL6!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq7.count()==0:\\n  dq_val=('Bad character in the input of the market file', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Bad character in the input of the market file', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL7!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq8.count()==0:\\n  dq_val=('Bad character in the input of the measure file', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Bad character in the input of the measure file', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL8!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq9.count()==0:\\n  dq_val=('Bad character in the input of the time file', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Bad character in the input of the time file', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL9!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq10.count()==0:\\n  dq_val=('Invalid combination for attribute for product', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Invalid combination for attribute for product', 'FAILED', '=HYPERLINK(\\\"#FYI_VAL10!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\n#Prepare Summary Report dataframe\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"Validation\\\",StringType(),True),\\n    StructField(\\\"Result\\\",StringType(),True),\\n    StructField(\\\"Details\\\",StringType(),True)\\n  ])\\n  \\n# Creation of Summary Tab\\n\\ndf_fyi_summary = spark.createDataFrame(data, schema_for_kpi)\\ndf_fyi_summary = df_fyi_summary.orderBy('Result')\\nsummary = df_file_struct_summary.unionByName(df_ref_vendors_summary, True).unionByName(df_fyi_summary, True)\\nsummary_vldtn = df_file_struct_summary.unionByName(df_ref_vendors_summary, True)\\n\\n# Creation of Validation Details Tab\\ndf_vld_dtls = df_vld_dtls.filter(\\\" Validation <> 'Overall Validation Result'\\\")\\n\\nif summary_vldtn.filter(\\\" Result = 'FAILED' \\\").count()>0:\\n    validation_result= 'FAILED'\\nelse:\\n    validation_result= 'PASSED'\\noverall_validation_result = ('Overall Validation Result', validation_result)\\ndata_dvm_dtls =[]\\ndata_dvm_dtls.append(overall_validation_result)\\n\\nschema_for_vld_dtls = StructType([ \\n    StructField(\\\"Validation\\\",StringType(),True),\\n    StructField(\\\"Details\\\",StringType(),True)\\n  ])\\n\\n# Creation of Validation Details Tab\\ndf_overall_result = spark.createDataFrame(data_dvm_dtls, schema_for_vld_dtls)\\ndf_vld_dtls = df_overall_result.unionByName(df_vld_dtls, True)\\n\\nvldtn_dtls = df_vld_dtls.toPandas()\\nvldtn_dtls.to_excel(writer,sheet_name=\\\"VALIDATION_DETAILS\\\",index=False)\\n\\n\\n#Prepare Summary Tab\\nSUMMARY = summary.toPandas()\\nSUMMARY.to_excel(writer,sheet_name=\\\"SUMMARY\\\",index=False)\\n\\n# Creation of other tabs\\n\\nlst_dfs = [dq1, dq2, dq3, dq4, dq5, dq6, dq7, dq8, dq9, dq10]\\nc = 1\\nfor i in lst_dfs:\\n  if i.count()>0:\\n    i = i.toPandas()\\n    i.to_excel(writer,sheet_name=f\\\"FYI_VAL{c}\\\",index=False)\\n  c= c+1\\n\\n# Close Excel Report and Save\\nwriter.close()\\n\\nfiles = [f for f in os.listdir('.') if os.path.isfile(f)]\\nfor f in files:\\n  if f==f'tp_dvm_rprt_{run_id}.xlsx':\\n    shutil.copyfile(f, f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}.xlsx')\\n\\n# Read and combine the report\\nwriter = pd.ExcelWriter(f'tp_dvm_rprt_{run_id}_summary.xlsx', engine='xlsxwriter')\\n\\n# Creation of Summary Tab Once again\\n\\ndf_file_struct_summary = df_file_struct_summary.orderBy('Result')\\ndf_ref_vendors_summary = df_ref_vendors_summary.orderBy('Result')\\ndf_fyi_summary = df_fyi_summary.orderBy('Result')\\nsummary = df_file_struct_summary.unionByName(df_ref_vendors_summary, True).unionByName(df_fyi_summary, True)\\n\\nSUMMARY = summary.toPandas()\\nSUMMARY.to_excel(writer,sheet_name=\\\"SUMMARY\\\",index=False)\\n\\n#Read Summary Report\\ndf_rpt_summary = pd.read_excel(f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}_summary.xlsx', sheet_name=None, dtype={'prod_skid': str , 'cntrt_id': str, 'mkt_skid': str})\\n\\nfor key, value in df_rpt_summary.items():\\n  if (key!='SUMMARY'):\\n    df_i = df_rpt_summary[key]\\n    df_i.to_excel(writer,sheet_name=f\\\"{key}\\\",index=False)\\n\\n#Read Current report\\ndf_rpt_curr = pd.read_excel(f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}.xlsx', sheet_name=None, dtype={'prod_skid': str , 'cntrt_id': str, 'mkt_skid': str})\\n\\nfor key, value in df_rpt_curr.items():\\n  if (key!='SUMMARY'):\\n    df_i = df_rpt_curr[key]\\n    df_i.to_excel(writer,sheet_name=f\\\"{key}\\\",index=False)\\n\\nwriter.close()\\n\\nfiles = [f for f in os.listdir('.') if os.path.isfile(f)]\\nfor f in files:\\n  if f==f'tp_dvm_rprt_{run_id}_summary.xlsx':\\n    shutil.move(f, f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}_summary.xlsx')\\n\\n\\n\\ndict_all_dfs['df_fyi_summary'] = {\\\"df_object\\\" :df_fyi_summary}\\ndf_output_dict['df_fyi_summary'] = df_fyi_summary\\n\\ndict_all_dfs['df_combine_fyi'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_fyi'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mm_prod_xref\"\n    },\n    {\n      \"name\": \"tier1_measr_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_time_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"mm_run_measr_plc\"\n    },\n    {\n      \"name\": \"tier1_mkt_dsdim\"\n    },\n    {\n      \"name\": \"tier1_prod_dsdim\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc_type\"\n    },\n    {\n      \"name\": \"mm_time_perd_assoc\"\n    },\n    {\n      \"name\": \"mm_run_mkt_plc\"\n    },\n    {\n      \"name\": \"df_file_struct_summary\"\n    },\n    {\n      \"name\": \"df_ref_vendors_summary\"\n    },\n    {\n      \"name\": \"df_fact\"\n    },\n    {\n      \"name\": \"mm_run_prttn_plc\"\n    },\n    {\n      \"name\": \"mm_time_perd_fdim\"\n    },\n    {\n      \"name\": \"tier1_prod_gav\"\n    },\n    {\n      \"name\": \"df_vld_dtls\"\n    },\n    {\n      \"name\": \"tier1_mkt_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_prod_mtrlz_tbl\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fyi_summary\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_fyi\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Formatting Report",
      "predecessorName": "FYI Validations and Report Generation",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nimport subprocess\\nimport os\\nimport shutil\\nimport subprocess\\nimport sys\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\nimport pandas as pd\\nsubprocess.check_call([sys.executable,\\\"-m\\\",\\\"pip\\\",\\\"install\\\",\\\"xlsxwriter\\\"])\\nsubprocess.check_call([sys.executable,\\\"-m\\\",\\\"pip\\\",\\\"install\\\",\\\"openpyxl\\\"])\\nimport xlsxwriter\\nimport openpyxl\\n\\nfrom openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder\\nfrom openpyxl.utils import get_column_letter\\nfrom openpyxl.workbook.workbook import Workbook\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\nrpt_path = '<@@PATH1@@>'\\n\\n\\n#Formatting of the report\\n# Opening The report\\nwb = openpyxl.load_workbook(f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}_summary.xlsx')\\nwb.move_sheet('VALIDATION_DETAILS',-1)\\n\\nws_summary = wb['SUMMARY']\\n\\n# Summary Header Changes\\n\\nfrom openpyxl.styles import colors\\nfrom openpyxl.styles import Font, PatternFill\\nws_summary['A1'].font = Font(color = '000000',bold=True, size=12) ## black\\nws_summary['B1'].font = Font(color = '000000',bold=True, size=12) ## black\\nws_summary['C1'].font = Font(color = '000000',bold=True, size=12) ## black\\n\\nws_summary['A1'].fill = PatternFill('solid', start_color=\\\"8DB4E2\\\") ## blue\\nws_summary['B1'].fill = PatternFill('solid', start_color=\\\"8DB4E2\\\") ## blue\\nws_summary['C1'].fill = PatternFill('solid', start_color=\\\"8DB4E2\\\") ## blue\\n\\n# Conditional Formatting\\n\\nfrom openpyxl.formatting.rule import CellIsRule\\n\\nfill = PatternFill(\\n    start_color='C6EFCE',\\n    end_color='C6EFCE',fill_type='solid') # specify background color\\n\\npassed = \\\"PASSED\\\"\\nfailed = \\\"FAILED\\\"\\nws_summary.conditional_formatting.add(\\n    'B2:B16594', CellIsRule(operator='equal', formula=['\\\"PASSED\\\"'], fill=fill)) # include formatting rule\\n\\nfill_failed = PatternFill(\\n    start_color='FFC7CE',\\n    end_color='FFC7CE',fill_type='solid') # specify background color\\n\\nws_summary.conditional_formatting.add(\\n    'B2:B16594', CellIsRule(operator='equal', formula=['\\\"FAILED\\\"'], fill=fill_failed)) # include formatting rule\\n\\nfill_val_highlight = Font(color = '000000',bold=True, size=12) ## black\\n\\nws_summary.conditional_formatting.add(\\n    'A2:A16594', CellIsRule(operator='equal', formula=['\\\"File Structure Validation\\\"'], font=fill_val_highlight)) # include formatting rule\\nws_summary.conditional_formatting.add(\\n    'A2:A16594', CellIsRule(operator='equal', formula=['\\\"Reference Data Vendors Validations\\\"'], font=fill_val_highlight)) # include formatting rule\\nws_summary.conditional_formatting.add(\\n    'A2:A16594', CellIsRule(operator='equal', formula=['\\\"For Your Information Validations\\\"'], font=fill_val_highlight)) # include formatting rule\\nws_summary.conditional_formatting.add(\\n    'A2:A16594', CellIsRule(operator='equal', formula=['\\\"Reference Data Validations\\\"'], font=fill_val_highlight)) # include formatting rule\\nws_summary.conditional_formatting.add(\\n    'A2:A16594', CellIsRule(operator='equal', formula=['\\\"Business Validations\\\"'], font=fill_val_highlight)) # include formatting rule\\n\\n# Auto Size columns from all the workbooks\\n\\nfor sheet_name in wb.sheetnames:\\n    dims = {}\\n    for row in wb[sheet_name].rows:\\n        for cell in row:\\n            if cell.value:\\n                dims[cell.column] = max((dims.get(cell.column, 0), len(str(cell.value)))) \\n    for col, value in dims.items():\\n        wb[sheet_name].column_dimensions[get_column_letter(col)].width = value+2\\n\\n\\n# Updating Validation Details\\nws_vd = wb['VALIDATION_DETAILS']\\nws_vd['A1'].font = Font(color = '000000',bold=True, size=12) ## black\\nws_vd['B1'].font = Font(color = '000000',bold=True, size=12) ## black\\n\\nws_vd['A1'].fill = PatternFill('solid', start_color=\\\"8DB4E2\\\") ## blue\\nws_vd['B1'].fill = PatternFill('solid', start_color=\\\"8DB4E2\\\") ## blue\\n\\nwb.save(f'tp_dvm_rprt_{run_id}_summary.xlsx')\\nshutil.move(f'tp_dvm_rprt_{run_id}_summary.xlsx', f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}_summary.xlsx')\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_fyi_summary\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fyi_summary\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Mail Sender",
      "predecessorName": "Formatting Report",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nfrom pyspark.sql.functions import *\\n\\n# Run information from Postgres\\nrun_id = <<PROCESS_RUN_KEY>>\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\n# mm_process_run_lkp_vw\\nmm_process_run_lkp_vw = spark.read.format(\\\"jdbc\\\").option(\\\"driver\\\", \\\"org.postgresql.Driver\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"query\\\", f\\\"select * from adwgp_mm.mm_process_run_lkp_vw where run_id = {run_id}\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").option(\\\"ssl\\\", True).option(\\\"sslmode\\\", \\\"require\\\").option(\\\"sslfactory\\\",\\\"org.postgresql.ssl.NonValidatingFactory\\\").load()\\nmm_process_run_lkp_vw.createOrReplaceTempView('mm_process_run_lkp_vw')\\n\\nmm_process_run_lkp_vw = mm_process_run_lkp_vw.select('file_name', 'start_date_time')\\nrun_info = mm_process_run_lkp_vw.collect()\\n\\n\\n#Mail Sender\\n\\nimport smtplib\\nfrom email.mime.text import MIMEText\\nfrom email.mime.multipart import MIMEMultipart\\nfrom email.mime.base import MIMEBase\\nfrom email import encoders\\n# Setup port number and server name\\n\\nsmtp_port = 587                 # Standard secure SMTP port\\nsmtp_server = \\\"smtp.office365.com\\\"  # Google SMTP Server\\n\\ncontacts = '<<CONTACTS>>'\\nlst_of_contacts = [i.strip() for i in contacts.split(\\\";\\\")]\\n#lst_of_contacts = []\\n\\n# Set up the email lists\\nemail_from = \\\"cpnotification.im@pg.com\\\"\\n\\nemail_list = []\\n\\n\\nfor c in lst_of_contacts:\\n  email_list.append(c)\\n\\npswd = dbutils.secrets.get('tp_dpf2cdl', 'cpnotification-password')\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\nfile_name = run_info[0]['file_name']\\nfile_timestamp= run_info[0]['start_date_time']\\nreport_name = f\\\"tp_dvm_rprt_{run_id}_summary.xlsx\\\"\\nrpt_path = '<@@PATH1@@>/tp_dvm_rpt/'\\n\\n# name the email subject\\nsubject = f\\\"TP For Your Information DVM Validation Report for run {run_id}\\\"\\n\\n# Define the email function (dont call it email!)\\ndef send_emails(email_list):\\n\\n    for person in email_list:\\n\\n        # Make the body of the email\\n        body = f\\\"\\\"\\\"\\n\\nValidation summary:\\n\\nSource File Name: {file_name} \\n\\\\nRun id: {run_id} \\n\\nPlease find the attachments for detailed validation report\\n\\nRegards,\\nTradepanel Team    \\n        \\\"\\\"\\\"\\n        print(body)\\n        # make a MIME object to define parts of the email\\n        msg = MIMEMultipart()\\n        msg['From'] = email_from\\n        msg['To'] = person\\n        msg['Subject'] = subject\\n\\n        # Attach the body of the message\\n        msg.attach(MIMEText(body, 'plain'))\\n\\n        # Define the file to attach\\n        filename_path = f\\\"/dbfs/mnt/{rpt_path}/tp_dvm_rprt_{run_id}_summary.xlsx\\\"\\n\\n        # Open the file in python as a binary\\n        attachment= open(filename_path, 'rb')  # r for read and b for binary\\n\\n        # Encode as base 64\\n        attachment_package = MIMEBase('application', 'octet-stream')\\n        attachment_package.set_payload((attachment).read())\\n        encoders.encode_base64(attachment_package)\\n        attachment_package.add_header('Content-Disposition', \\\"attachment; filename= \\\" + report_name)\\n        msg.attach(attachment_package)\\n\\n        # Cast as string\\n        text = msg.as_string()\\n\\n        # Connect with the server\\n        print(\\\"Connecting to server...\\\")\\n        TIE_server = smtplib.SMTP(smtp_server, smtp_port)\\n        TIE_server.starttls()\\n        TIE_server.login(email_from, pswd)\\n        print(\\\"Succesfully connected to server\\\")\\n        print()\\n\\n\\n        # Send emails to \\\"person\\\" as list is iterated\\n        print(f\\\"Sending email to: {person}...\\\")\\n        TIE_server.sendmail(email_from, person, text)\\n        print(f\\\"Email sent to: {person}\\\")\\n        print()\\n\\n    # Close the port\\n    TIE_server.quit()\\n\\n\\n# Run the function\\nsend_emails(email_list)\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_fyi_summary\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_fyi_summary\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI Eligible KPIs",
      "predecessorName": "Mail Sender",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_fyi.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_fyi\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI - DQ Checks",
      "predecessorName": "FYI Eligible KPIs",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_fyi.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_fyi\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_fyi_chk\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "Report Generation",
      "predecessorName": "FYI - DQ Checks",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"saveToCSV\": \"true\",\n  \"generateHTMLReport\": \"true\",\n  \"generatePDFReport\": \"false\",\n  \"includeDetailedValidationResults\": \"failed rows only\",\n  \"numberOfRowsToDisplay\": 100,\n  \"reportTemplate\": \"default\"\n}",
      "operationVersionName": "DataQualityReport",
      "overridableIndicator": false
    }
  ],
  "graphName": "NNIT_TP_t1_dq_fyi_validation"
}