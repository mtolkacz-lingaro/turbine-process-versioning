{
  "applicationName": "TRADEPANEL",
  "jsonSpecification": "{\r\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\r\n    \"title\": \"DQ Test\",\r\n    \"description\": \"DQ test\",\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \r\n   },\r\n    \"required\": [],\r\n    \"configurable\": []\r\n}",
  "nodes": [
    {
      "operationName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"manualSchema\": \"true\",\n  \"transformations\": [\n    {\n      \"columnType\": \"string\",\n      \"columnName\": \"test\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "CreateSchema",
      "overridableIndicator": false
    },
    {
      "operationName": "inputs",
      "predecessorName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col\\nfrom pyspark.sql.window import Window\\n\\n\\n#Input Dataframes\\n\\ncntrt_id = <<CNTRT_ID>>\\n\\nmmc_mkt_source = dict_all_dfs['mmc_mkt_source'][\\\"df_object\\\"]\\nmmc_prod_source = dict_all_dfs['mmc_prod_source'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_tmp_extrnl_tbl = dict_all_dfs['tier2_tmp_extrnl_tbl'][\\\"df_object\\\"]\\ndf_db_cols_map = dict_all_dfs['df_dpf_col_asign_vw'][\\\"df_object\\\"]\\n\\n\\n#Reading Optional Indicator\\nfrom pyspark.sql.functions import col\\n\\ndf_db_cols_map = df_db_cols_map.withColumnRenamed('database_column_name', 'DPF_COL_NAME').withColumnRenamed('file_column_name', 'FILE_COL_NAME').withColumnRenamed('table_type', 'TBL_TYPE_CODE').withColumnRenamed(\\\"optional\\\", \\\"optional_ind\\\")\\n\\ndf_db_cols_map = df_db_cols_map.filter(col('CNTRT_ID')==cntrt_id)\\n\\n#df_db_cols_map = spark.read.format('csv').option('header', True).load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/DQ_DPF_COL_ASIGN_VW_opt_ind.csv').filter(col('CNTRT_ID')==cntrt_id)\\n\\n# Copy dataframes\\n#mmc_prod_source = df_prod_extrn\\n#mmc_mkt_source = df_mkt_extrn\\n#tier2_fact_mtrlz_tbl = df_fact_extrn\\n#tier2_tmp_extrnl_tbl = df_fact_raw\\ndf_db_cols_map\\n\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\nmmc_mkt_source = mmc_mkt_source.withColumn(\\\"mkt_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\nmmc_prod_source = mmc_prod_source.withColumn(\\\"prod_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_fact_mtrlz_tbl =  tier2_fact_mtrlz_tbl.withColumn(\\\"fact_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_tmp_extrnl_tbl = tier2_tmp_extrnl_tbl.withColumn(\\\"raw_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n\\n\\nmmc_prod_source.createOrReplaceTempView('MMC_PROD_SOURCE')\\nmmc_mkt_source.createOrReplaceTempView('MMC_MKT_SOURCE')\\ntier2_fact_mtrlz_tbl.createOrReplaceTempView('TIER2_FACT_MTRLZ_TBL')\\n\\nchk_dq4 = ('<<CHK_DQ4>>'== 'true')\\nchk_dq5 = ('<<CHK_DQ5>>'== 'true')\\nchk_dq6 = ('<<CHK_DQ6>>'== 'true')\\nchk_dq7 = ('<<CHK_DQ7>>'== 'true')\\nchk_dq8 = ('<<CHK_DQ8>>'== 'true')\\nchk_dq9 = ('<<CHK_DQ9>>'== 'true')\\n\\n\\n#Bad Fact data\\n\\nfact_cols1 = tier2_fact_mtrlz_tbl.columns\\n\\nstr1 = ''\\ni = 1\\nfor c in fact_cols1:\\n  str1 = str1+ c \\n  if i < len(fact_cols1):\\n    str1 = str1+ ' IS NULL AND '\\n  else:\\n    str1 = str1+ ' IS NULL '\\n  i=i+1\\n\\nfrom pyspark.sql.functions import expr\\nfact_bad_df = tier2_fact_mtrlz_tbl.withColumn('BadFactData', when(expr(str1), lit('Y')).otherwise(lit('N'))).filter(\\\"BadFactData = 'Y' \\\")\\n\\n\\n#Missing Mandatory columns\\nfrom pyspark.sql.functions import lower, lit, col, when\\n\\ndf_opt_ind_only_N = df_db_cols_map.filter((col('cntrt_id')==cntrt_id) &(col('optional_ind')=='N') )\\ndf_opt_ind_only_N = df_opt_ind_only_N.withColumn('dpf_col_name', lower('dpf_col_name'))\\n\\nmkt_cols_list = ['MKT', 'mkt', 'market']\\nprod_cols_list = ['prod', 'PROD', 'product']\\nfact_cols_list = ['FACT', 'fact']\\n\\ndf_opt_ind_fact = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in fact_cols_list])))\\ndf_opt_ind_mkt = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in mkt_cols_list])))\\ndf_opt_ind_prod = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in prod_cols_list])))\\n\\nfact_cols = tier2_fact_mtrlz_tbl.columns\\nmkt_cols = mmc_mkt_source.columns\\nprod_cols = mmc_prod_source.columns\\ndf_opt_fact = df_opt_ind_fact.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_mkt = df_opt_ind_mkt.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_prod = df_opt_ind_prod.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\n\\ndpf_opt_union = df_opt_fact.unionByName(df_opt_ind_mkt, False).unionByName(df_opt_ind_prod, False)\\n\\n\\n# Fact uses supplier tag not defined in the reference part\\n\\nquery1 = \\\"\\\"\\\"WITH\\nf AS (\\n  SELECT f.fact_row_id AS line_num, f.extrn_prod_id, f.extrn_mkt_id FROM TIER2_FACT_MTRLZ_TBL f\\n),\\np AS (\\n  SELECT p.extrn_prod_id FROM MMC_PROD_SOURCE  p\\n),\\nm AS (\\n  SELECT m.extrn_mkt_id FROM MMC_MKT_SOURCE  m\\n), \\nt1 AS (\\n  SELECT\\n    'PROD' AS comp,\\n    min(f.line_num) line_num, \\n    f.extrn_prod_id extrn_code\\n  FROM f \\n  LEFT JOIN p ON f.extrn_prod_id = p.extrn_prod_id\\n  WHERE 1=1\\n    AND p.extrn_prod_id IS NULL\\n  GROUP BY f.extrn_prod_id\\n  UNION ALL\\n    SELECT\\n    'MKT' AS comp,\\n    min(f.line_num) line_num, \\n    f.extrn_mkt_id extrn_code\\n  FROM f \\n  LEFT JOIN m ON f.extrn_mkt_id = m.extrn_mkt_id\\n  WHERE 1=1\\n    AND m.extrn_mkt_id IS NULL\\n  GROUP BY f.extrn_mkt_id\\n) \\nSELECT comp, line_num, extrn_code FROM t1\\\"\\\"\\\"\\n\\ndf_dq6 = spark.sql(query1)\\n\\n\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\n# Market and Product Uniqueness\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\n\\n\\nif chk_dq8:\\n  df_mkt_non_dup = mmc_mkt_source.groupBy('mkt_row_id','extrn_mkt_id', 'mkt_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_mkt_id', 'Duplicate_extrn_mkt_id').withColumnRenamed('mkt_name', 'Duplicate_mkt_name').withColumn('FileType_1', lit('MKT')).withColumn('DQ8', lit('Market Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_mkt_non_dup = df_mkt_non_dup.select('row_id',df_mkt_non_dup.mkt_row_id.alias('LINE_NUMBER_MKT'),'DQ8',df_mkt_non_dup.Duplicate_extrn_mkt_id.alias('SUPPLIER_MKT_TAG'),df_mkt_non_dup.Duplicate_mkt_name.alias('SUPPLIER_MARKET_DESCRIPTION'))\\nelse:\\n  df_mkt_non_dup = df_empty\\n\\nif chk_dq9:\\n  df_prod_non_dup = mmc_prod_source.groupBy('prod_row_id','extrn_prod_id', 'short_prod_desc_txt', 'prod_lvl_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_prod_id', 'Duplicate_extrn_prod_id').withColumnRenamed('short_prod_desc_txt', 'Duplicate_product_name').withColumn('FileType_2', lit('PROD')).withColumn('DQ9', lit('Product Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_prod_non_dup = df_prod_non_dup.select('row_id', df_prod_non_dup.prod_row_id.alias('LINE_NUMBER_PROD'),'DQ9',df_prod_non_dup.Duplicate_extrn_prod_id.alias('SUPPLIER_PROD_TAG'),df_prod_non_dup.Duplicate_product_name.alias('SUPPLIER_PRODUCT_DESCRIPTION'), df_prod_non_dup.prod_lvl_name.alias('PRODUCT_LEVEL_NAME'))\\nelse:\\n  df_prod_non_dup = df_empty\\n\\nif chk_dq7:\\n  df_fact_non_dup = tier2_fact_mtrlz_tbl.groupBy('fact_row_id','extrn_prod_id','extrn_mkt_id','extrn_time_perd_id').count().filter(\\\"count=1\\\").withColumn('FileType_3', lit('FACT')).withColumn('DQ7', lit('Fact Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_fact_non_dup = df_fact_non_dup.select('row_id',df_fact_non_dup.fact_row_id.alias('LINE_NUMBER') ,'DQ7',df_fact_non_dup.extrn_prod_id.alias('SUPPLIER_PRODUCT_TAG') ,df_fact_non_dup.extrn_mkt_id.alias('SUPPLIER_MARKET_TAG') ,df_fact_non_dup.extrn_time_perd_id.alias('SUPPLIER_TIME_PERIOD_CODE') )\\nelse:\\n  df_fact_non_dup = df_empty\\n\\nif chk_dq5:\\n  dpf_opt_union = dpf_opt_union.filter(\\\"dlvrd_ind = 'N' \\\").withColumn('DQ5', lit('Missing Mandatory Columns')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  dpf_opt_union = dpf_opt_union.select('row_id','DQ5',dpf_opt_union.TBL_TYPE_CODE.alias('COMPONENT'),dpf_opt_union.FILE_COL_NAME.alias('FILE_COLUMN_NAME'), dpf_opt_union.dpf_col_name.alias('DB_COLUMN_NAME'))\\nelse:\\n  dpf_opt_union = df_empty\\n\\nif chk_dq6:\\n  df_dq6 = df_dq6.withColumn('DQ6', lit('Fact uses supplier tag not defined in the reference part')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq6 = df_dq6.select('row_id','DQ6',df_dq6.comp.alias('COMPONENT_DQ6'), df_dq6.line_num.alias('LINE_NUMBER_IN_FACT'), df_dq6.extrn_code.alias('UNDEFINED_TAG_FROM_FACT_PART') )\\nelse:\\n  df_dq6 = df_empty\\n\\nif chk_dq4:\\n  fact_bad_df = fact_bad_df.withColumn('DQ4', lit('Bad Fact Data')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  fact_bad_df = fact_bad_df.select('row_id','DQ4','BadFactData' )\\nelse:\\n  fact_bad_df = df_empty\\n\\nrow_id_lst = ['row_id']\\ndf_combine1 = df_mkt_non_dup.join(df_prod_non_dup, ['row_id'] ,'full').join(df_fact_non_dup,  ['row_id']  , 'full').join(dpf_opt_union,['row_id']  ,'full').join(df_dq6,['row_id'] ,'full').join(fact_bad_df,['row_id'] ,'full').drop( *row_id_lst)\\n\\ndf_combine = df_combine1.withColumn(\\\"row_num\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_num'))\\n\\ndf_output_dict['df_combine'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mmc_mkt_source\"\n    },\n    {\n      \"name\": \"mmc_prod_source\"\n    },\n    {\n      \"name\": \"tier2_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_tmp_extrnl_tbl\"\n    },\n    {\n      \"name\": \"df_dpf_col_asign_vw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "inputs - File struct",
      "predecessorName": "inputs",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col\\nfrom pyspark.sql.window import Window\\n\\n\\n#Input Dataframes\\n\\ncntrt_id = <<CNTRT_ID>>\\n\\nmmc_mkt_source = dict_all_dfs['mmc_mkt_source'][\\\"df_object\\\"]\\nmmc_prod_source = dict_all_dfs['mmc_prod_source'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_tmp_extrnl_tbl = dict_all_dfs['tier2_tmp_extrnl_tbl'][\\\"df_object\\\"]\\n\\n#Reading Optional Indicator\\nfrom pyspark.sql.functions import col\\ndf_db_cols_map = spark.read.format('csv').option('header', True).load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/DQ_DPF_COL_ASIGN_VW_opt_ind.csv').filter(col('CNTRT_ID')==cntrt_id)\\n\\n# Copy dataframes\\n#mmc_prod_source = df_prod_extrn\\n#mmc_mkt_source = df_mkt_extrn\\n#tier2_fact_mtrlz_tbl = df_fact_extrn\\n#tier2_tmp_extrnl_tbl = df_fact_raw\\ndf_db_cols_map\\n\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\nmmc_mkt_source = mmc_mkt_source.withColumn(\\\"mkt_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\nmmc_prod_source = mmc_prod_source.withColumn(\\\"prod_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_fact_mtrlz_tbl =  tier2_fact_mtrlz_tbl.withColumn(\\\"fact_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_tmp_extrnl_tbl = tier2_tmp_extrnl_tbl.withColumn(\\\"raw_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n\\n\\nmmc_prod_source.createOrReplaceTempView('MMC_PROD_SOURCE')\\nmmc_mkt_source.createOrReplaceTempView('MMC_MKT_SOURCE')\\ntier2_fact_mtrlz_tbl.createOrReplaceTempView('TIER2_FACT_MTRLZ_TBL')\\n\\nchk_dq4 = True\\nchk_dq5 = True\\nchk_dq6 = True\\nchk_dq7 = True\\nchk_dq8 = True\\nchk_dq9 = True\\n\\n\\n#Bad Fact data\\n\\nfact_cols1 = tier2_fact_mtrlz_tbl.columns\\n\\nstr1 = ''\\ni = 1\\nfor c in fact_cols1:\\n  str1 = str1+ c \\n  if i < len(fact_cols1):\\n    str1 = str1+ ' IS NULL AND '\\n  else:\\n    str1 = str1+ ' IS NULL '\\n  i=i+1\\n\\nfrom pyspark.sql.functions import expr\\nfact_bad_df = tier2_fact_mtrlz_tbl.withColumn('BadFactData', when(expr(str1), lit('Y')).otherwise(lit('N'))).filter(\\\"BadFactData = 'Y' \\\")\\n\\n\\n#Missing Mandatory columns\\nfrom pyspark.sql.functions import lower, lit, col, when\\n\\ndf_opt_ind_only_N =df_db_cols_map.filter((col('cntrt_id')==cntrt_id) &(col('optional_ind')=='N') )\\ndf_opt_ind_only_N = df_opt_ind_only_N.withColumn('dpf_col_name', lower('dpf_col_name'))\\n\\ndf_opt_ind_fact = df_opt_ind_only_N.filter(\\\" tbl_type_code = 'FACT' \\\")\\ndf_opt_ind_mkt = df_opt_ind_only_N.filter(\\\" tbl_type_code = 'MKT' \\\")\\ndf_opt_ind_prod = df_opt_ind_only_N.filter(\\\" tbl_type_code = 'PROD' \\\")\\n\\nfact_cols = tier2_fact_mtrlz_tbl.columns\\nmkt_cols = mmc_mkt_source.columns\\nprod_cols = mmc_prod_source.columns\\ndf_opt_fact = df_opt_ind_fact.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_mkt = df_opt_ind_mkt.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_prod = df_opt_ind_prod.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\n\\ndpf_opt_union = df_opt_fact.unionByName(df_opt_ind_mkt, False).unionByName(df_opt_ind_prod, False)\\n\\n\\n# Fact uses supplier tag not defined in the reference part\\n\\nquery1 = \\\"\\\"\\\"  SELECT f.fact_row_id as line_number, f.extrn_prod_id as Undefined_extrn_prod_id, f.extrn_mkt_id as Undefined_extrn_mkt_id FROM\\n    TIER2_FACT_MTRLZ_TBL f\\n  LEFT JOIN MMC_PROD_SOURCE p\\n  ON\\n    f.extrn_prod_id = p.extrn_prod_id\\n  LEFT JOIN MMC_MKT_SOURCE m\\n  ON\\n    f.extrn_mkt_id = m.extrn_mkt_id\\n  WHERE 1=1\\n  AND p.extrn_prod_id IS NULL OR m.extrn_mkt_id IS NULL \\\"\\\"\\\"\\n\\ndf_dq6 = spark.sql(query1)\\n\\n\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\n# Market and Product Uniqueness\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\n\\nif chk_dq8:\\n  df_mkt_non_dup = mmc_mkt_source.groupBy('mkt_row_id','extrn_mkt_id', 'mkt_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_mkt_id', 'Duplicate_extrn_mkt_id').withColumnRenamed('mkt_name', 'Duplicate_mkt_name').withColumn('FileType_1', lit('MKT')).withColumn('DQ8', lit('Market Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_mkt_non_dup = df_mkt_non_dup.select('row_id',\\\"mkt_row_id\\\",'DQ8','FileType_1', 'Duplicate_extrn_mkt_id','Duplicate_mkt_name')\\nelse:\\n  df_mkt_non_dup = df_empty\\n\\nif chk_dq9:\\n  df_prod_non_dup = mmc_prod_source.groupBy('prod_row_id','extrn_prod_id', 'short_prod_desc_txt').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_prod_id', 'Duplicate_extrn_prod_id').withColumnRenamed('short_prod_desc_txt', 'Duplicate_product_name').withColumn('FileType_2', lit('PROD')).withColumn('DQ9', lit('Product Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_prod_non_dup = df_prod_non_dup.select('row_id',\\\"prod_row_id\\\",'DQ9','FileType_2', 'Duplicate_extrn_prod_id','Duplicate_product_name')\\nelse:\\n  df_prod_non_dup = df_empty\\n\\nif chk_dq7:\\n  df_fact_non_dup = tier2_fact_mtrlz_tbl.groupBy('fact_row_id','extrn_prod_id','extrn_mkt_id','extrn_time_perd_id').count().filter(\\\"count=1\\\").withColumn('FileType_3', lit('FACT')).withColumn('DQ7', lit('Fact Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_fact_non_dup = df_fact_non_dup.select('row_id','fact_row_id','DQ7','FileType_3','extrn_prod_id','extrn_mkt_id','extrn_time_perd_id')\\nelse:\\n  df_fact_non_dup = df_empty\\n\\nif chk_dq5:\\n  dpf_opt_union = dpf_opt_union.filter(\\\"dlvrd_ind = 'N' \\\").withColumn('DQ5', lit('Missing Mandatory Columns')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).select('row_id','DQ5','TBL_TYPE_CODE','FILE_COL_NAME', 'dpf_col_name','OPTIONAL_IND' )\\nelse:\\n  dpf_opt_union = df_empty\\n\\nif chk_dq6:\\n  df_dq6 = df_dq6.withColumn('DQ6', lit('Fact uses supplier tag not defined in the reference part')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).select('row_id','DQ6','line_number','Undefined_extrn_prod_id', 'Undefined_extrn_mkt_id' )\\nelse:\\n  df_dq6 = df_empty\\n\\nif chk_dq4:\\n  fact_bad_df = fact_bad_df.withColumn('DQ4', lit('Bad Fact Data')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).select('row_id','DQ4','BadFactData' )\\nelse:\\n  fact_bad_df = df_empty\\n\\ndf_combine = df_mkt_non_dup.join(df_prod_non_dup, df_mkt_non_dup.row_id == df_prod_non_dup.row_id ,'full').join(df_fact_non_dup,  (df_fact_non_dup.row_id == df_prod_non_dup.row_id) , 'full').join(dpf_opt_union,df_fact_non_dup.row_id ==dpf_opt_union.row_id ,'full').join(df_dq6,df_fact_non_dup.row_id ==df_dq6.row_id ,'full').join(fact_bad_df,df_fact_non_dup.row_id ==fact_bad_df.row_id ,'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\n# KPI information\\ndq8_columns = ['mkt_row_id','DQ8', 'FileType_1', 'Duplicate_extrn_mkt_id', 'Duplicate_mkt_name']\\ndq9_columns = ['prod_row_id','DQ9', 'FileType_2', 'Duplicate_extrn_prod_id', 'Duplicate_product_name']\\ndq7_columns = ['fact_row_id','DQ7', 'FileType_3', 'extrn_prod_id', 'extrn_mkt_id', 'extrn_time_perd_id']\\ndq5_columns = ['DQ5', 'TBL_TYPE_CODE', 'FILE_COL_NAME', 'dpf_col_name', 'OPTIONAL_IND']\\ndq6_columns = ['line_number', 'DQ6', 'Undefined_extrn_prod_id', 'Undefined_extrn_mkt_id']\\ndq4_columns = ['DQ4', 'BadFactData']\\n\\ncombined_cols = ['row_id']\\ndata = []\\nif chk_dq4:\\n  [combined_cols.append(i) for i in dq4_columns]\\n  dq4_val = ('BadFactData', 'SQL Validation KPI', \\\"BadFactData IN ('N') OR BadFactData IS NULL\\\", '', 'false', 'Input MKT data uniqueness validation', 100 )\\n  data.append(dq4_val)\\nif chk_dq5:\\n  [combined_cols.append(i) for i in dq5_columns]\\n  dq5_val = ('dpf_col_name', 'SQL Validation KPI', \\\"dpf_col_name IS NULL\\\", '', 'false', 'Missing Mandatory Columns', 100 )\\n  data.append(dq5_val)\\nif chk_dq6:\\n  [combined_cols.append(i) for i in dq6_columns]\\n  dq6_val = ('line_number', 'SQL Validation KPI', \\\"line_number IS NULL\\\", '', 'false', 'Fact uses supplier tag not defined in the reference part', 100 )\\n  data.append(dq6_val)\\nif chk_dq7:\\n  [combined_cols.append(i) for i in dq7_columns]\\n  dq7_val = ('fact_row_id', 'SQL Validation KPI', \\\"fact_row_id IS NULL\\\", '', 'false', 'Fact Data uniqueness', 100 )\\n  data.append(dq7_val)\\nif chk_dq8:\\n  [combined_cols.append(i) for i in dq8_columns]\\n  dq8_val = ('Duplicate_extrn_mkt_id', 'SQL Validation KPI', \\\"Duplicate_extrn_mkt_id IS NULL\\\", '', 'false', 'Market Data Uniqueness', 100 )\\n  data.append(dq8_val)\\nif chk_dq9:\\n  [combined_cols.append(i) for i in dq9_columns]\\n  dq9_val = ('Duplicate_extrn_prod_id', 'SQL Validation KPI', \\\"Duplicate_extrn_prod_id IS NULL\\\", '', 'false', 'Product Data Uniqueness', 100 )\\n  data.append(dq9_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nschema = ['column', 'kpi_type', 'param_1', 'param_2','fail_on_error', 'check_description','target'  ]\\ndf_file_struct = spark.createDataFrame(data, schema)\\n\\n\\n#File Structure Check Eligibility\\nfile_strct_elig = chk_dq4 | chk_dq5 | chk_dq6 | chk_dq7 | chk_dq8 | chk_dq9\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(file_strct_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"File_struct\\\",BooleanType(),True),\\n    StructField(\\\"File_struct_elig\\\",StringType(),True)\\n  ])\\n \\ndf_file_struct_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_file_struct_eligibility = df_file_struct_eligibility.withColumn('File_struct_elig', when(col('File_struct'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_file_struct_eligibility'] = {\\\"df_object\\\" :df_file_struct_eligibility}\\n\\ndict_all_dfs['df_file_struct'] = {\\\"df_object\\\" :df_file_struct}\\n\\n\\n\\n\\n\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mmc_mkt_source\"\n    },\n    {\n      \"name\": \"mmc_prod_source\"\n    },\n    {\n      \"name\": \"tier2_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_tmp_extrnl_tbl\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_file_struct_eligibility\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_file_struct\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "inputs - File Struct v1",
      "predecessorName": "inputs - File struct",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col\\nfrom pyspark.sql.window import Window\\n\\n\\n#Input Dataframes\\n\\ncntrt_id = <<CNTRT_ID>>\\n\\nmmc_mkt_source = dict_all_dfs['mmc_mkt_source'][\\\"df_object\\\"]\\nmmc_prod_source = dict_all_dfs['mmc_prod_source'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_tmp_extrnl_tbl = dict_all_dfs['tier2_tmp_extrnl_tbl'][\\\"df_object\\\"]\\ndf_db_cols_map = dict_all_dfs['df_dpf_col_asign_vw'][\\\"df_object\\\"]\\n\\n\\n#Reading Optional Indicator\\nfrom pyspark.sql.functions import col\\n\\ndf_db_cols_map = df_db_cols_map.withColumnRenamed('database_column_name', 'DPF_COL_NAME').withColumnRenamed('file_column_name', 'FILE_COL_NAME').withColumnRenamed('table_type', 'TBL_TYPE_CODE').withColumnRenamed(\\\"optional\\\", \\\"optional_ind\\\")\\n\\ndf_db_cols_map = df_db_cols_map.filter(col('CNTRT_ID')==cntrt_id)\\n\\n#df_db_cols_map = spark.read.format('csv').option('header', True).load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/DQ_DPF_COL_ASIGN_VW_opt_ind.csv').filter(col('CNTRT_ID')==cntrt_id)\\n\\n# Copy dataframes\\n#mmc_prod_source = df_prod_extrn\\n#mmc_mkt_source = df_mkt_extrn\\n#tier2_fact_mtrlz_tbl = df_fact_extrn\\n#tier2_tmp_extrnl_tbl = df_fact_raw\\ndf_db_cols_map\\n\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\nmmc_mkt_source = mmc_mkt_source.withColumn(\\\"mkt_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\nmmc_prod_source = mmc_prod_source.withColumn(\\\"prod_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_fact_mtrlz_tbl =  tier2_fact_mtrlz_tbl.withColumn(\\\"fact_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_tmp_extrnl_tbl = tier2_tmp_extrnl_tbl.withColumn(\\\"raw_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n\\n\\nmmc_prod_source.createOrReplaceTempView('MMC_PROD_SOURCE')\\nmmc_mkt_source.createOrReplaceTempView('MMC_MKT_SOURCE')\\ntier2_fact_mtrlz_tbl.createOrReplaceTempView('TIER2_FACT_MTRLZ_TBL')\\n\\nchk_dq4 = ('<<CHK_DQ4>>'== 'true')\\nchk_dq5 = ('<<CHK_DQ5>>'== 'true')\\nchk_dq6 = ('<<CHK_DQ6>>'== 'true')\\nchk_dq7 = ('<<CHK_DQ7>>'== 'true')\\nchk_dq8 = ('<<CHK_DQ8>>'== 'true')\\nchk_dq9 = ('<<CHK_DQ9>>'== 'true')\\n\\n\\n#Bad Fact data\\n\\nfact_cols1 = tier2_fact_mtrlz_tbl.columns\\n\\nstr1 = ''\\ni = 1\\nfor c in fact_cols1:\\n  str1 = str1+ c \\n  if i < len(fact_cols1):\\n    str1 = str1+ ' IS NULL AND '\\n  else:\\n    str1 = str1+ ' IS NULL '\\n  i=i+1\\n\\nfrom pyspark.sql.functions import expr\\nfact_bad_df = tier2_fact_mtrlz_tbl.withColumn('BadFactData', when(expr(str1), lit('Y')).otherwise(lit('N'))).filter(\\\"BadFactData = 'Y' \\\")\\n\\n\\n#Missing Mandatory columns\\nfrom pyspark.sql.functions import lower, lit, col, when\\n\\ndf_opt_ind_only_N = df_db_cols_map.filter((col('cntrt_id')==cntrt_id) &(col('optional_ind')=='N') )\\ndf_opt_ind_only_N = df_opt_ind_only_N.withColumn('dpf_col_name', lower('dpf_col_name'))\\n\\nmkt_cols_list = ['MKT', 'mkt', 'market']\\nprod_cols_list = ['prod', 'PROD', 'product']\\nfact_cols_list = ['FACT', 'fact']\\n\\ndf_opt_ind_fact = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in fact_cols_list])))\\ndf_opt_ind_mkt = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in mkt_cols_list])))\\ndf_opt_ind_prod = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in prod_cols_list])))\\n\\nfact_cols = tier2_fact_mtrlz_tbl.columns\\nmkt_cols = mmc_mkt_source.columns\\nprod_cols = mmc_prod_source.columns\\ndf_opt_fact = df_opt_ind_fact.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_mkt = df_opt_ind_mkt.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_prod = df_opt_ind_prod.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\n\\ndpf_opt_union = df_opt_fact.unionByName(df_opt_ind_mkt, False).unionByName(df_opt_ind_prod, False)\\n\\n\\n# Fact uses supplier tag not defined in the reference part\\n\\nquery1 = \\\"\\\"\\\"WITH\\nf AS (\\n  SELECT f.fact_row_id AS line_num, f.extrn_prod_id, f.extrn_mkt_id FROM TIER2_FACT_MTRLZ_TBL f\\n),\\np AS (\\n  SELECT p.extrn_prod_id FROM MMC_PROD_SOURCE  p\\n),\\nm AS (\\n  SELECT m.extrn_mkt_id FROM MMC_MKT_SOURCE  m\\n), \\nt1 AS (\\n  SELECT\\n    'PROD' AS comp,\\n    min(f.line_num) line_num, \\n    f.extrn_prod_id extrn_code\\n  FROM f \\n  LEFT JOIN p ON f.extrn_prod_id = p.extrn_prod_id\\n  WHERE 1=1\\n    AND p.extrn_prod_id IS NULL\\n  GROUP BY f.extrn_prod_id\\n  UNION ALL\\n    SELECT\\n    'MKT' AS comp,\\n    min(f.line_num) line_num, \\n    f.extrn_mkt_id extrn_code\\n  FROM f \\n  LEFT JOIN m ON f.extrn_mkt_id = m.extrn_mkt_id\\n  WHERE 1=1\\n    AND m.extrn_mkt_id IS NULL\\n  GROUP BY f.extrn_mkt_id\\n) \\nSELECT comp, line_num, extrn_code FROM t1\\\"\\\"\\\"\\n\\ndf_dq6 = spark.sql(query1)\\n\\n\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\n# Market and Product Uniqueness\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\n\\nif chk_dq8:\\n  df_mkt_non_dup = mmc_mkt_source.groupBy('mkt_row_id','extrn_mkt_id', 'mkt_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_mkt_id', 'Duplicate_extrn_mkt_id').withColumnRenamed('mkt_name', 'Duplicate_mkt_name').withColumn('FileType_1', lit('MKT')).withColumn('DQ8', lit('Market Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_mkt_non_dup = df_mkt_non_dup.select('row_id',\\\"mkt_row_id\\\",'DQ8','FileType_1', 'Duplicate_extrn_mkt_id','Duplicate_mkt_name')\\nelse:\\n  df_mkt_non_dup = df_empty\\n\\nif chk_dq9:\\n  df_prod_non_dup = mmc_prod_source.groupBy('prod_row_id','extrn_prod_id', 'short_prod_desc_txt', 'prod_lvl_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_prod_id', 'Duplicate_extrn_prod_id').withColumnRenamed('short_prod_desc_txt', 'Duplicate_product_name').withColumn('FileType_2', lit('PROD')).withColumn('DQ9', lit('Product Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_prod_non_dup = df_prod_non_dup.select('row_id',\\\"prod_row_id\\\",'DQ9','FileType_2', 'Duplicate_extrn_prod_id','Duplicate_product_name', 'prod_lvl_name')\\nelse:\\n  df_prod_non_dup = df_empty\\n\\nif chk_dq7:\\n  df_fact_non_dup = tier2_fact_mtrlz_tbl.groupBy('fact_row_id','extrn_prod_id','extrn_mkt_id','extrn_time_perd_id').count().filter(\\\"count=1\\\").withColumn('FileType_3', lit('FACT')).withColumn('DQ7', lit('Fact Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_fact_non_dup = df_fact_non_dup.select('row_id','fact_row_id','DQ7','FileType_3','extrn_prod_id','extrn_mkt_id','extrn_time_perd_id')\\nelse:\\n  df_fact_non_dup = df_empty\\n\\nif chk_dq5:\\n  dpf_opt_union = dpf_opt_union.filter(\\\"dlvrd_ind = 'N' \\\").withColumn('DQ5', lit('Missing Mandatory Columns')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).select('row_id','DQ5','TBL_TYPE_CODE','FILE_COL_NAME', 'dpf_col_name','OPTIONAL_IND' )\\nelse:\\n  dpf_opt_union = df_empty\\n\\nif chk_dq6:\\n  df_dq6 = df_dq6.withColumn('DQ6', lit('Fact uses supplier tag not defined in the reference part')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).select('row_id','DQ6','comp','line_num','extrn_code' )\\nelse:\\n  df_dq6 = df_empty\\n\\nif chk_dq4:\\n  fact_bad_df = fact_bad_df.withColumn('DQ4', lit('Bad Fact Data')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).select('row_id','DQ4','BadFactData' )\\nelse:\\n  fact_bad_df = df_empty\\n\\ndf_combine = df_mkt_non_dup.join(df_prod_non_dup, df_mkt_non_dup.row_id == df_prod_non_dup.row_id ,'full').join(df_fact_non_dup,  (df_fact_non_dup.row_id == df_prod_non_dup.row_id) , 'full').join(dpf_opt_union,df_fact_non_dup.row_id ==dpf_opt_union.row_id ,'full').join(df_dq6,df_fact_non_dup.row_id ==df_dq6.row_id ,'full').join(fact_bad_df,df_fact_non_dup.row_id ==fact_bad_df.row_id ,'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\ndf_combine.createOrReplaceTempView('FILE_STRUCT')\\n\\nquery =\\\"\\\"\\\"SELECT\\nrow_id,\\nDQ4,\\nBadFactData,\\nDQ5,\\nTBL_TYPE_CODE AS COMPONENT,\\nFILE_COL_NAME  AS FILE_COLUMN_NAME,\\ndpf_col_name AS DB_COLUMN_NAME,\\nDQ6,\\ncomp AS COMPONENT_DQ6,\\nline_num AS LINE_NUMBER_IN_FACT,\\nextrn_code AS UNDEFINED_TAG_FROM_FACT_PART,\\nDQ7,\\nfact_row_id AS LINE_NUMBER,\\nextrn_prod_id AS SUPPLIER_PRODUCT_TAG,\\nextrn_mkt_id AS SUPPLIER_MARKET_TAG,\\nextrn_time_perd_id AS SUPPLIER_TIME_PERIOD_CODE,\\nDQ8,\\nmkt_row_id AS LINE_NUMBER_MKT,\\nDuplicate_extrn_mkt_id AS SUPPLIER_MKT_TAG,\\nDuplicate_mkt_name AS SUPPLIER_MARKET_DESCRIPTION,\\nDQ9,\\nprod_row_id AS LINE_NUMBER_PROD,\\nDuplicate_extrn_prod_id AS SUPPLIER_PROD_TAG,\\nDuplicate_product_name AS SUPPLIER_PRODUCT_DESCRIPTION,\\nprod_lvl_name AS PRODUCT_LEVEL_NAME\\n\\nfrom \\nFILE_STRUCT\\\"\\\"\\\"\\n\\ndf_combine = spark.sql(query)\\n\\n\\n# KPI information\\ndq4_columns = ['DQ4', 'BadFactData']\\ndq5_columns = ['DQ5', 'COMPONENT', 'FILE_COLUMN_NAME', 'DB_COLUMN_NAME']\\ndq6_columns = ['DQ6','COMPONENT_DQ6','LINE_NUMBER_IN_FACT', 'UNDEFINED_TAG_FROM_FACT_PART']\\ndq8_columns = ['DQ8','LINE_NUMBER_MKT','SUPPLIER_MKT_TAG', 'SUPPLIER_MARKET_DESCRIPTION']\\ndq9_columns = ['DQ9', 'LINE_NUMBER_PROD', 'SUPPLIER_PROD_TAG', 'SUPPLIER_PRODUCT_DESCRIPTION','PRODUCT_LEVEL_NAME']\\ndq7_columns = ['DQ7','LINE_NUMBER', 'SUPPLIER_PRODUCT_TAG', 'SUPPLIER_MARKET_TAG', 'SUPPLIER_TIME_PERIOD_CODE']\\n\\n\\n\\ncombined_cols = ['row_id']\\ndata = []\\nif chk_dq4:\\n  [combined_cols.append(i) for i in dq4_columns]\\n  dq4_val = ('BadFactData', 'SQL Validation KPI', \\\"BadFactData IN ('N') OR BadFactData IS NULL\\\", '', 'false', 'Input MKT data uniqueness validation', 100 )\\n  data.append(dq4_val)\\nif chk_dq5:\\n  [combined_cols.append(i) for i in dq5_columns]\\n  dq5_val = ('DB_COLUMN_NAME', 'SQL Validation KPI', \\\"DB_COLUMN_NAME IS NULL\\\", '', 'false', 'Missing Mandatory Columns', 100 )\\n  data.append(dq5_val)\\nif chk_dq6:\\n  [combined_cols.append(i) for i in dq6_columns]\\n  dq6_val = ('UNDEFINED_TAG_FROM_FACT_PART', 'SQL Validation KPI', \\\"UNDEFINED_TAG_FROM_FACT_PART IS NULL\\\", '', 'false', 'Fact uses supplier tag not defined in the reference part', 100 )\\n  data.append(dq6_val)\\nif chk_dq7:\\n  [combined_cols.append(i) for i in dq7_columns]\\n  dq7_val = ('LINE_NUMBER', 'SQL Validation KPI', \\\"LINE_NUMBER IS NULL\\\", '', 'false', 'Fact Data uniqueness', 100 )\\n  data.append(dq7_val)\\nif chk_dq8:\\n  [combined_cols.append(i) for i in dq8_columns]\\n  dq8_val = ('SUPPLIER_MKT_TAG', 'SQL Validation KPI', \\\"SUPPLIER_MKT_TAG IS NULL\\\", '', 'false', 'Market Data Uniqueness', 100 )\\n  data.append(dq8_val)\\nif chk_dq9:\\n  [combined_cols.append(i) for i in dq9_columns]\\n  dq9_val = ('SUPPLIER_PROD_TAG', 'SQL Validation KPI', \\\"SUPPLIER_PROD_TAG IS NULL\\\", '', 'false', 'Product Data Uniqueness', 100 )\\n  data.append(dq9_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n\\n\\n\\n#Prepare KPI\\n\\nschema = ['column', 'kpi_type', 'param_1', 'param_2','fail_on_error', 'check_description','target'  ]\\ndf_file_struct = spark.createDataFrame(data, schema)\\n\\n\\n#File Structure Check Eligibility\\nfile_strct_elig = chk_dq4 | chk_dq5 | chk_dq6 | chk_dq7 | chk_dq8 | chk_dq9\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(file_strct_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"File_struct\\\",BooleanType(),True),\\n    StructField(\\\"File_struct_elig\\\",StringType(),True)\\n  ])\\n \\ndf_file_struct_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_file_struct_eligibility = df_file_struct_eligibility.withColumn('File_struct_elig', when(col('File_struct'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine_file'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_file_struct_eligibility'] = {\\\"df_object\\\" :df_file_struct_eligibility}\\n\\ndict_all_dfs['df_file_struct'] = {\\\"df_object\\\" :df_file_struct}\\ndf_output_dict['df_file_struct'] = df_file_struct\\ndf_output_dict['df_combine_file'] = df_combine\\ndf_output_dict['df_file_struct_eligibility'] = df_file_struct_eligibility\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mmc_mkt_source\"\n    },\n    {\n      \"name\": \"mmc_prod_source\"\n    },\n    {\n      \"name\": \"tier2_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_tmp_extrnl_tbl\"\n    },\n    {\n      \"name\": \"df_dpf_col_asign_vw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_file\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_file_struct_eligibility\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_file_struct\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "inputs - File Struct v2",
      "predecessorName": "inputs - File Struct v1",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col\\nfrom pyspark.sql.window import Window\\n\\n\\n#Input Dataframes\\n\\ncntrt_id = <<CNTRT_ID>>\\n\\nmmc_mkt_source = dict_all_dfs['mmc_mkt_source'][\\\"df_object\\\"]\\nmmc_prod_source = dict_all_dfs['mmc_prod_source'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_tmp_extrnl_tbl = dict_all_dfs['tier2_tmp_extrnl_tbl'][\\\"df_object\\\"]\\ndf_db_cols_map = dict_all_dfs['df_dpf_col_asign_vw'][\\\"df_object\\\"]\\n\\n\\n#Reading Optional Indicator\\nfrom pyspark.sql.functions import col\\n\\ndf_db_cols_map = df_db_cols_map.withColumnRenamed('database_column_name', 'DPF_COL_NAME').withColumnRenamed('file_column_name', 'FILE_COL_NAME').withColumnRenamed('table_type', 'TBL_TYPE_CODE').withColumnRenamed(\\\"optional\\\", \\\"optional_ind\\\")\\n\\ndf_db_cols_map = df_db_cols_map.filter(col('CNTRT_ID')==cntrt_id)\\n\\n#df_db_cols_map = spark.read.format('csv').option('header', True).load('/mnt/unrefined/cloudpanel-test-unref/test/dvm/DQ_DPF_COL_ASIGN_VW_opt_ind.csv').filter(col('CNTRT_ID')==cntrt_id)\\n\\n# Copy dataframes\\n#mmc_prod_source = df_prod_extrn\\n#mmc_mkt_source = df_mkt_extrn\\n#tier2_fact_mtrlz_tbl = df_fact_extrn\\n#tier2_tmp_extrnl_tbl = df_fact_raw\\ndf_db_cols_map\\n\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\nmmc_mkt_source = mmc_mkt_source.withColumn(\\\"mkt_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\nmmc_prod_source = mmc_prod_source.withColumn(\\\"prod_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_fact_mtrlz_tbl =  tier2_fact_mtrlz_tbl.withColumn(\\\"fact_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ntier2_tmp_extrnl_tbl = tier2_tmp_extrnl_tbl.withColumn(\\\"raw_row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n\\n\\nmmc_prod_source.createOrReplaceTempView('MMC_PROD_SOURCE')\\nmmc_mkt_source.createOrReplaceTempView('MMC_MKT_SOURCE')\\ntier2_fact_mtrlz_tbl.createOrReplaceTempView('TIER2_FACT_MTRLZ_TBL')\\n\\nchk_dq4 = ('<<CHK_DQ4>>'== 'true')\\nchk_dq5 = ('<<CHK_DQ5>>'== 'true')\\nchk_dq6 = ('<<CHK_DQ6>>'== 'true')\\nchk_dq7 = ('<<CHK_DQ7>>'== 'true')\\nchk_dq8 = ('<<CHK_DQ8>>'== 'true')\\nchk_dq9 = ('<<CHK_DQ9>>'== 'true')\\n\\n\\n#Bad Fact data\\n\\nfact_cols1 = tier2_fact_mtrlz_tbl.columns\\n\\nstr1 = ''\\ni = 1\\nfor c in fact_cols1:\\n  str1 = str1+ c \\n  if i < len(fact_cols1):\\n    str1 = str1+ ' IS NULL AND '\\n  else:\\n    str1 = str1+ ' IS NULL '\\n  i=i+1\\n\\nfrom pyspark.sql.functions import expr\\nfact_bad_df = tier2_fact_mtrlz_tbl.withColumn('BadFactData', when(expr(str1), lit('Y')).otherwise(lit('N'))).filter(\\\"BadFactData = 'Y' \\\")\\n\\n\\n#Missing Mandatory columns\\nfrom pyspark.sql.functions import lower, lit, col, when\\n\\ndf_opt_ind_only_N = df_db_cols_map.filter((col('cntrt_id')==cntrt_id) &(col('optional_ind')=='N') )\\ndf_opt_ind_only_N = df_opt_ind_only_N.withColumn('dpf_col_name', lower('dpf_col_name'))\\n\\nmkt_cols_list = ['MKT', 'mkt', 'market']\\nprod_cols_list = ['prod', 'PROD', 'product']\\nfact_cols_list = ['FACT', 'fact']\\n\\ndf_opt_ind_fact = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in fact_cols_list])))\\ndf_opt_ind_mkt = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in mkt_cols_list])))\\ndf_opt_ind_prod = df_opt_ind_only_N.where(df_opt_ind_only_N['TBL_TYPE_CODE'].rlike(\\\"|\\\".join([\\\"(\\\" + column + \\\")\\\" for column in prod_cols_list])))\\n\\nfact_cols = tier2_fact_mtrlz_tbl.columns\\nmkt_cols = mmc_mkt_source.columns\\nprod_cols = mmc_prod_source.columns\\ndf_opt_fact = df_opt_ind_fact.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_mkt = df_opt_ind_mkt.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_prod = df_opt_ind_prod.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\n\\ndpf_opt_union = df_opt_fact.unionByName(df_opt_ind_mkt, False).unionByName(df_opt_ind_prod, False)\\n\\n\\n# Fact uses supplier tag not defined in the reference part\\n\\nquery1 = \\\"\\\"\\\"WITH\\nf AS (\\n  SELECT f.fact_row_id AS line_num, f.extrn_prod_id, f.extrn_mkt_id FROM TIER2_FACT_MTRLZ_TBL f\\n),\\np AS (\\n  SELECT p.extrn_prod_id FROM MMC_PROD_SOURCE  p\\n),\\nm AS (\\n  SELECT m.extrn_mkt_id FROM MMC_MKT_SOURCE  m\\n), \\nt1 AS (\\n  SELECT\\n    'PROD' AS comp,\\n    min(f.line_num) line_num, \\n    f.extrn_prod_id extrn_code\\n  FROM f \\n  LEFT JOIN p ON f.extrn_prod_id = p.extrn_prod_id\\n  WHERE 1=1\\n    AND p.extrn_prod_id IS NULL\\n  GROUP BY f.extrn_prod_id\\n  UNION ALL\\n    SELECT\\n    'MKT' AS comp,\\n    min(f.line_num) line_num, \\n    f.extrn_mkt_id extrn_code\\n  FROM f \\n  LEFT JOIN m ON f.extrn_mkt_id = m.extrn_mkt_id\\n  WHERE 1=1\\n    AND m.extrn_mkt_id IS NULL\\n  GROUP BY f.extrn_mkt_id\\n) \\nSELECT comp, line_num, extrn_code FROM t1\\\"\\\"\\\"\\n\\ndf_dq6 = spark.sql(query1)\\n\\n\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\n# Market and Product Uniqueness\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\n\\n\\n\\nif chk_dq8:\\n  df_mkt_non_dup = mmc_mkt_source.groupBy('mkt_row_id','extrn_mkt_id', 'mkt_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_mkt_id', 'Duplicate_extrn_mkt_id').withColumnRenamed('mkt_name', 'Duplicate_mkt_name').withColumn('FileType_1', lit('MKT')).withColumn('DQ8', lit('Market Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_mkt_non_dup = df_mkt_non_dup.select('row_id',df_mkt_non_dup.mkt_row_id.alias('LINE_NUMBER_MKT'),'DQ8',df_mkt_non_dup.Duplicate_extrn_mkt_id.alias('SUPPLIER_MKT_TAG'),df_mkt_non_dup.Duplicate_mkt_name.alias('SUPPLIER_MARKET_DESCRIPTION'))\\nelse:\\n  df_mkt_non_dup = df_empty\\n\\nif chk_dq9:\\n  df_prod_non_dup = mmc_prod_source.groupBy('prod_row_id','extrn_prod_id', 'short_prod_desc_txt', 'prod_lvl_name').count().filter(\\\"count = 1\\\").withColumnRenamed('extrn_prod_id', 'Duplicate_extrn_prod_id').withColumnRenamed('short_prod_desc_txt', 'Duplicate_product_name').withColumn('FileType_2', lit('PROD')).withColumn('DQ9', lit('Product Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_prod_non_dup = df_prod_non_dup.select('row_id', df_prod_non_dup.prod_row_id.alias('LINE_NUMBER_PROD'),'DQ9',df_prod_non_dup.Duplicate_extrn_prod_id.alias('SUPPLIER_PROD_TAG'),df_prod_non_dup.Duplicate_product_name.alias('SUPPLIER_PRODUCT_DESCRIPTION'), df_prod_non_dup.prod_lvl_name.alias('PRODUCT_LEVEL_NAME'))\\nelse:\\n  df_prod_non_dup = df_empty\\n\\nif chk_dq7:\\n  df_fact_non_dup = tier2_fact_mtrlz_tbl.groupBy('fact_row_id','extrn_prod_id','extrn_mkt_id','extrn_time_perd_id').count().filter(\\\"count=1\\\").withColumn('FileType_3', lit('FACT')).withColumn('DQ7', lit('Fact Data Unquiness')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_fact_non_dup = df_fact_non_dup.select('row_id',df_fact_non_dup.fact_row_id.alias('LINE_NUMBER') ,'DQ7',df_fact_non_dup.extrn_prod_id.alias('SUPPLIER_PRODUCT_TAG') ,df_fact_non_dup.extrn_mkt_id.alias('SUPPLIER_MARKET_TAG') ,df_fact_non_dup.extrn_time_perd_id.alias('SUPPLIER_TIME_PERIOD_CODE') )\\nelse:\\n  df_fact_non_dup = df_empty\\n\\nif chk_dq5:\\n  dpf_opt_union = dpf_opt_union.filter(\\\"dlvrd_ind = 'N' \\\").withColumn('DQ5', lit('Missing Mandatory Columns')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  dpf_opt_union = dpf_opt_union.select('row_id','DQ5',dpf_opt_union.TBL_TYPE_CODE.alias('COMPONENT'),dpf_opt_union.FILE_COL_NAME.alias('FILE_COLUMN_NAME'), dpf_opt_union.dpf_col_name.alias('DB_COLUMN_NAME'))\\nelse:\\n  dpf_opt_union = df_empty\\n\\nif chk_dq6:\\n  df_dq6 = df_dq6.withColumn('DQ6', lit('Fact uses supplier tag not defined in the reference part')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq6 = df_dq6.select('row_id','DQ6',df_dq6.comp.alias('COMPONENT_DQ6'), df_dq6.line_num.alias('LINE_NUMBER_IN_FACT'), df_dq6.extrn_code.alias('UNDEFINED_TAG_FROM_FACT_PART') )\\nelse:\\n  df_dq6 = df_empty\\n\\nif chk_dq4:\\n  fact_bad_df = fact_bad_df.withColumn('DQ4', lit('Bad Fact Data')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  fact_bad_df = fact_bad_df.select('row_id','DQ4','BadFactData' )\\nelse:\\n  fact_bad_df = df_empty\\n\\nrow_id_lst = ['row_id']\\n\\nrow_id_lst = ['row_id']\\ndf_combine1 = df_mkt_non_dup.join(df_prod_non_dup, ['row_id'] ,'full').join(df_fact_non_dup,  ['row_id']  , 'full').join(dpf_opt_union,['row_id']  ,'full').join(df_dq6,['row_id'] ,'full').join(fact_bad_df,['row_id'] ,'full').drop( *row_id_lst)\\n\\ndf_combine = df_combine1.withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\n\\n# KPI information\\ndq4_columns = ['DQ4', 'BadFactData']\\ndq5_columns = ['DQ5', 'COMPONENT', 'FILE_COLUMN_NAME', 'DB_COLUMN_NAME']\\ndq6_columns = ['DQ6','COMPONENT_DQ6','LINE_NUMBER_IN_FACT', 'UNDEFINED_TAG_FROM_FACT_PART']\\ndq8_columns = ['DQ8','LINE_NUMBER_MKT','SUPPLIER_MKT_TAG', 'SUPPLIER_MARKET_DESCRIPTION']\\ndq9_columns = ['DQ9', 'LINE_NUMBER_PROD', 'SUPPLIER_PROD_TAG', 'SUPPLIER_PRODUCT_DESCRIPTION','PRODUCT_LEVEL_NAME']\\ndq7_columns = ['DQ7','LINE_NUMBER', 'SUPPLIER_PRODUCT_TAG', 'SUPPLIER_MARKET_TAG', 'SUPPLIER_TIME_PERIOD_CODE']\\n\\n\\n\\ncombined_cols = ['row_id']\\ndata = []\\nif chk_dq4:\\n  [combined_cols.append(i) for i in dq4_columns]\\n  dq4_val = ('BadFactData', 'SQL Validation KPI', \\\"BadFactData IN ('N') OR BadFactData IS NULL\\\", '', 'false', 'Input MKT data uniqueness validation', 100 )\\n  data.append(dq4_val)\\nif chk_dq5:\\n  [combined_cols.append(i) for i in dq5_columns]\\n  dq5_val = ('DB_COLUMN_NAME', 'SQL Validation KPI', \\\"DB_COLUMN_NAME IS NULL\\\", '', 'false', 'Missing Mandatory Columns', 100 )\\n  data.append(dq5_val)\\nif chk_dq6:\\n  [combined_cols.append(i) for i in dq6_columns]\\n  dq6_val = ('UNDEFINED_TAG_FROM_FACT_PART', 'SQL Validation KPI', \\\"UNDEFINED_TAG_FROM_FACT_PART IS NULL\\\", '', 'false', 'Fact uses supplier tag not defined in the reference part', 100 )\\n  data.append(dq6_val)\\nif chk_dq7:\\n  [combined_cols.append(i) for i in dq7_columns]\\n  dq7_val = ('LINE_NUMBER', 'SQL Validation KPI', \\\"LINE_NUMBER IS NULL\\\", '', 'false', 'Fact Data uniqueness', 100 )\\n  data.append(dq7_val)\\nif chk_dq8:\\n  [combined_cols.append(i) for i in dq8_columns]\\n  dq8_val = ('SUPPLIER_MKT_TAG', 'SQL Validation KPI', \\\"SUPPLIER_MKT_TAG IS NULL\\\", '', 'false', 'Market Data Uniqueness', 100 )\\n  data.append(dq8_val)\\nif chk_dq9:\\n  [combined_cols.append(i) for i in dq9_columns]\\n  dq9_val = ('SUPPLIER_PROD_TAG', 'SQL Validation KPI', \\\"SUPPLIER_PROD_TAG IS NULL\\\", '', 'false', 'Product Data Uniqueness', 100 )\\n  data.append(dq9_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n\\n\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\n# schema = ['column', 'kpi_type', 'param_1', 'param_2','fail_on_error', 'check_description','target'  ]\\ndf_file_struct = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\n#File Structure Check Eligibility\\nfile_strct_elig = chk_dq4 | chk_dq5 | chk_dq6 | chk_dq7 | chk_dq8 | chk_dq9\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(file_strct_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"File_struct\\\",BooleanType(),True),\\n    StructField(\\\"File_struct_elig\\\",StringType(),True)\\n  ])\\n \\ndf_file_struct_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_file_struct_eligibility = df_file_struct_eligibility.withColumn('File_struct_elig', when(col('File_struct'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine_file'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_file_struct_eligibility'] = {\\\"df_object\\\" :df_file_struct_eligibility}\\n\\ndict_all_dfs['df_file_struct'] = {\\\"df_object\\\" :df_file_struct}\\ndf_output_dict['df_file_struct'] = df_file_struct\\ndf_output_dict['df_combine_file'] = df_combine\\ndf_output_dict['df_file_struct_eligibility'] = df_file_struct_eligibility\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mmc_mkt_source\"\n    },\n    {\n      \"name\": \"mmc_prod_source\"\n    },\n    {\n      \"name\": \"tier2_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_tmp_extrnl_tbl\"\n    },\n    {\n      \"name\": \"df_dpf_col_asign_vw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_file\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_file_struct_eligibility\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_file_struct\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "File Structure Checks Eligible KPIs",
      "predecessorName": "inputs - File Struct v2",
      "jsonSpecification": "{\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_file.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_file_struct\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": false
    },
    {
      "operationName": "[GEN] - To update delivery lookup",
      "predecessorName": "File Structure Checks Eligible KPIs",
      "jsonSpecification": "{\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\nrows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 3, 1, 2, <<PROCESS_RUN_KEY>>]]\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"jdbc\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"dbtable\\\", \\\"adwgp_mm.MM_DLVRY_RUN_LKP\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").mode(\\\"append\\\").save()\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dummy\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ - Flie Stucture checks",
      "predecessorName": "[GEN] - To update delivery lookup",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_file.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_file\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_chk\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ-Report File Structure",
      "predecessorName": "DQ - Flie Stucture checks",
      "jsonSpecification": "{\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"saveToCSV\": \"true\",\n  \"generateHTMLReport\": \"true\",\n  \"generatePDFReport\": \"false\",\n  \"includeDetailedValidationResults\": \"failed rows only\",\n  \"numberOfRowsToDisplay\": 100,\n  \"reportTemplate\": \"default\"\n}",
      "operationVersionName": "DataQualityReport",
      "overridableIndicator": false
    },
    {
      "operationName": "Stop Calc",
      "predecessorName": "DQ-Report File Structure",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf = dict_all_dfs['df_combine_chk'][\\\"df_object\\\"]\\n\\nresult = df.columns[4]\\ndf = df.withColumnRenamed(result, 'result')\\n\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\nref_db_url = dbutils.secrets.get('tp_dpf2cdl', 'refDBjdbcURL')\\nrefDBname = dbutils.secrets.get('tp_dpf2cdl', 'refDBname')\\nrefDBuser = dbutils.secrets.get('tp_dpf2cdl', 'refDBuser')\\nrefDBpwd = dbutils.secrets.get('tp_dpf2cdl', 'refDBpwd')\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\n\\ncnt = df.filter(\\\"result = 'Fail' \\\" ).count()\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [('Pass',cnt)\\n  ]\\nschema = StructType([ \\n    StructField(\\\"result\\\",StringType(),True),\\n\\tStructField(\\\"count\\\",IntegerType(),True)\\n  ])\\n \\n\\nif (cnt>0):\\n  df\\nelse:\\n  df = spark.createDataFrame(data=data2,schema=schema)\\n\\nif (cnt>0):\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 9, 1, 4, <<PROCESS_RUN_KEY>>]]\\nelse:\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 3, 1, 3, <<PROCESS_RUN_KEY>>]]\\n\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"jdbc\\\").option(\\\"url\\\", f\\\"{ref_db_url}/{refDBname}\\\").option(\\\"dbtable\\\", \\\"adwgp_mm.MM_DLVRY_RUN_LKP\\\").option(\\\"user\\\", f\\\"{refDBuser}\\\").option(\\\"password\\\", f\\\"{refDBpwd}\\\").mode(\\\"append\\\").save()\\n\\ndict_all_dfs['df_combine_chk'] = {\\\"df_object\\\" :df}\\ndf_output_dict['df_combine_chk'] = df\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_chk\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_chk\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Conditional Stop",
      "predecessorName": "Stop Calc",
      "jsonSpecification": "{\n  \"expression\": \"result = 'Fail'\",\n  \"processStatus\": \"DQ_ISSUE\",\n  \"conditionValue\": \"true\",\n  \"milestone\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_chk\"\n    }\n  ]\n}",
      "operationVersionName": "ConditionalStop",
      "overridableIndicator": false
    }
  ],
  "graphName": "cdl_t2_dq_file_struct_v2"
}