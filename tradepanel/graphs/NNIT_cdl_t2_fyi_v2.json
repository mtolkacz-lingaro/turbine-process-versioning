{
  "applicationName": "TURBINE_INTERNAL",
  "jsonSpecification": "{\r\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\r\n    \"title\": \"DQ Test\",\r\n    \"description\": \"DQ test\",\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \r\n   },\r\n    \"required\": [],\r\n    \"configurable\": []\r\n}",
  "nodes": [
    {
      "operationName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"manualSchema\": \"true\",\n  \"transformations\": [\n    {\n      \"columnType\": \"string\",\n      \"columnName\": \"test\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "CreateSchema",
      "overridableIndicator": false
    },
    {
      "operationName": "inputs - FYI v1",
      "predecessorName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n# Inputs\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col\\nfrom pyspark.sql.window import Window\\n\\n# Copy dataframes\\nmmc_prod_source = dict_all_dfs['mmc_prod_source'][\\\"df_object\\\"]\\nmmc_mkt_source = dict_all_dfs['mmc_mkt_source'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['tier2_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ndf_db_cols_map = dict_all_dfs['df_dpf_col_asign_vw'][\\\"df_object\\\"]\\n\\ncntrt_id = <<CNTRT_ID>>\\nsrce_sys_id = <<SRCE_SYS_ID>>\\nTIER2_TIME_PERD_TYPE_CODE = '<<PERIOD_TYPE>>'\\nTIER2_TIME_PERD_CLASS_CODE = '<<PERIOD_TYPE_CODE>>'\\nrun_id = <<PROCESS_RUN_KEY>>\\n\\nchk_dq10 = ('<<CHK_DQ10>>' == 'true')\\nchk_dq11 = ('<<CHK_DQ11>>' == 'true')\\nchk_dq12 = ('<<CHK_DQ12>>' == 'true')\\nchk_dq13 = ('<<CHK_DQ13>>' == 'true')\\nchk_dq14 = ('<<CHK_DQ14>>' == 'true')\\nchk_dq15 = ('<<CHK_DQ15>>' == 'true')\\n\\n#DQ10\\nfrom pyspark.sql.functions import lower, lit, col, when\\n#df_opt_ind = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/DQ_DPF_COL_ASIGN_VW_opt_ind.csv').filter(f\\\"cntrt_id = {cntrt_id}\\\")\\n\\ndf_db_cols_map = df_db_cols_map.withColumnRenamed('database_column_name', 'DPF_COL_NAME').withColumnRenamed('file_column_name', 'FILE_COL_NAME').withColumnRenamed('table_type', 'TBL_TYPE_CODE').withColumnRenamed(\\\"optional\\\". \\\"optional_ind\\\")\\ndf_opt_ind  = df_db_cols_map.filter(f\\\"cntrt_id = {cntrt_id}\\\")\\n\\nfrom pyspark.sql.functions import col\\n\\ndf_opt_ind_only_N =df_opt_ind.filter((col('cntrt_id')==cntrt_id) &(col('optional_ind')=='N') )\\ndf_opt_ind_only_N = df_opt_ind.withColumn('dpf_col_name', lower('dpf_col_name'))\\n\\ndf_opt_ind_fact = df_opt_ind.filter(\\\" tbl_type_code = 'FACT' \\\")\\ndf_opt_ind_mkt = df_opt_ind.filter(\\\" tbl_type_code = 'MKT' \\\")\\ndf_opt_ind_prod = df_opt_ind.filter(\\\" tbl_type_code = 'PROD' \\\")\\n\\nfact_cols = tier2_fact_mtrlz_tbl.columns\\nmkt_cols = mmc_mkt_source.columns\\nprod_cols = mmc_prod_source.columns\\ndf_opt_fact = df_opt_ind_fact.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_mkt = df_opt_ind_mkt.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_prod = df_opt_ind_prod.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\n\\ndpf_opt_union = df_opt_fact.unionByName(df_opt_ind_mkt, False).unionByName(df_opt_ind_prod, False)\\ndpf_opt_union.filter(\\\"((dpf_col_name IS NULL) OR (optional_ind = 'Y' AND dlvrd_ind = 'N'))\\\")\\ndpf_opt_union = dpf_opt_union.select('TBL_TYPE_CODE','FILE_COL_NAME', 'DPF_COL_NAME','OPTIONAL_IND','dlvrd_ind')\\ndpf_opt_union = dpf_opt_union.withColumnRenamed('TBL_TYPE_CODE', 'TBL_TYPE_CODE1').withColumnRenamed('FILE_COL_NAME', 'miss_FILE_COL_NAME').withColumnRenamed('DPF_COL_NAME', 'miss_DPF_COL_NAME').withColumnRenamed('OPTIONAL_IND', 'DQ10_OPTIONAL_IND').withColumnRenamed('dlvrd_ind', 'DQ10_dlvrd_ind')\\n\\n# DQ11\\nfrom pyspark.sql.functions import lit, col, abs, concat_ws, collect_list\\n#dpf_run_vw = spark.read.format('parquet').load('/mnt/unrefined/NNIT/tradepanel/adw/DPF_ALL_RUN_VW')\\ndpf_run_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/ f\\\"select * from adwgp_mm.mm_process_run_lkp_vw where cntrt_id = {cntrt_id}\\\")\\n\\n#mm_run_prttn_plc = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/tables/mm_run_prttn_plc.csv')\\nmm_run_prttn_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PRTTN_PLC/')\\nmm_prod_dim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_DIM_VW')\\nmm_prod_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_XREF')\\n\\ndpf_run_vw.createOrReplaceTempView('dpf_run_vw')\\nmm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\nmm_prod_dim.createOrReplaceTempView('mm_prod_dim')\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\nmmc_prod_source.createOrReplaceTempView('MMC_PROD_SOURCE')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('tier2_prod_mtrlz_tbl')\\n\\ndq11_query1 = f\\\"\\\"\\\"SELECT  cntrt_id, MAX(run_id) run_id\\n  FROM \\n    dpf_run_vw WHERE cntrt_id = {cntrt_id}\\n  AND process_status = 'FINISHED' \\n  GROUP BY cntrt_id\\\"\\\"\\\"\\nlast_run = spark.sql(dq11_query1)\\nlast_run.createOrReplaceTempView('last_run')\\n\\ndq11_query2 = f\\\"\\\"\\\"SELECT  rp.* \\n  FROM\\n    mm_run_prttn_plc rp, last_run lr\\n  WHERE rp.srce_sys_id = {srce_sys_id} \\n    AND rp.cntrt_id = lr.cntrt_id\\n    AND rp.run_id = lr.run_id\\\"\\\"\\\"\\nlast_prttn = spark.sql(dq11_query2)\\nlast_prttn.createOrReplaceTempView('last_prttn')\\n\\ndq11_query3 = f\\\"\\\"\\\"SELECT \\n    DISTINCT x.extrn_prod_id,  d.prod_skid, d.prod_lvl_name, nvl(short_prod_desc_txt,long_prod_desc_txt) prod_desc\\n  FROM \\n    mm_prod_dim d, \\n    last_prttn lp, \\n    mm_prod_xref x\\n  WHERE d.srce_sys_id = lp.srce_sys_id\\n    AND d.prod_prttn_code = lp.prod_prttn_code\\n    AND d.run_id = lp.run_id\\n    AND d.prod_skid = x.prod_skid\\n    AND d.srce_sys_id = x.srce_sys_id\\n    AND d.prod_prttn_code = x.prod_prttn_code\\n\\tand d.srce_sys_id = {srce_sys_id} \\\"\\\"\\\"\\nlast_prod = spark.sql(dq11_query3)\\nlast_prod.createOrReplaceTempView('last_prod')\\n\\ndq11_query4 = \\\"\\\"\\\"SELECT * FROM last_prod WHERE extrn_prod_id NOT IN (SELECT extrn_prod_id FROM MMC_PROD_SOURCE p)\\\"\\\"\\\"\\n\\nprod_diff = spark.sql(dq11_query4)\\nprod_diff.createOrReplaceTempView('prod_diff')\\n\\ndq11_query5 = \\\"\\\"\\\"SELECT extrn_prod_id, prod_desc extrn_prod_name, prod_skid, prod_desc, prod_lvl_name FROM prod_diff\\\"\\\"\\\"\\ndf_dq11 = spark.sql(dq11_query5)\\n\\n\\n#DQ12\\nfrom pyspark.sql.functions import col\\n# mm_cntrt_lkp = spark.read.format('parquet').load('/mnt/unrefined/NNIT/tradepanel/adw/MM_CNTRT_LKP_VW')\\nmm_cntrt_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/ f\\\"select * from adwgp_mm.MM_CNTRT_LKP where cntrt_id = {cntrt_id}\\\")\\nmm_cntrt_lkp.createOrReplaceTempView('MM_CNTRT_LKP')\\nmm_time_perd_fdim = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_FDIM_VW')\\nmm_time_perd_fdim.createOrReplaceTempView('MM_TIME_PERD_FDIM')\\nmm_time_perd_assoc_type = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_ASSOC_TYPE_VW')\\nmm_time_perd_assoc_type.createOrReplaceTempView('mm_time_perd_assoc_type')\\nmm_time_perd_assoc = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_ASSOC_VW')\\nmm_time_perd_assoc.createOrReplaceTempView('mm_time_perd_assoc')\\nmm_time_perd = spark.read.format('parquet').load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/tables/TIME_PERD_SDADS/')\\nmm_time_perd.createOrReplaceTempView('mm_time_perd')\\n                                                 \\nsub_que1 = \\\"\\\"\\\"SELECT   2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_b,\\n          tb.time_perd_start_date time_perd_start_date_b,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_a,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          ta.time_perd_end_date time_perd_end_date_a,\\n          ta.time_perd_start_date time_perd_start_date_a\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n     WHERE assoc.CAL_TYPE_ID=2\\\"\\\"\\\"\\nCAL_TYPE_2 = spark.sql(sub_que1)\\nCAL_TYPE_2.createOrReplaceTempView('CAL_TYPE_2')\\n\\nsub_que2 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_B TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_A TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          tb.time_perd_end_date time_perd_end_date_A,\\n          tb.time_perd_start_date time_perd_start_date_A\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\\"\\\"\\\"\\n\\nBIMTH_2_MTH = spark.sql(sub_que2)\\nBIMTH_2_MTH.createOrReplaceTempView('BIMTH_2_MTH')\\n\\nsub_que3 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A ,\\n          assoc.TIME_PERD_ID_B ,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_A,\\n          ta.time_perd_end_date time_perd_end_date_A,\\n          ta.time_perd_start_date time_perd_start_date_A,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_B,\\n          tb.time_perd_start_date time_perd_start_date_B\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1 \\\"\\\"\\\"\\nMTH_2_BIMTH = spark.sql(sub_que3)\\nMTH_2_BIMTH.createOrReplaceTempView('MTH_2_BIMTH')\\n\\nsub_que4 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          ta.EVEN_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.EVEN_BIMTH_TIME_PERD_CLASS_COD TIME_PERD_CLASS_CODE_A,\\n          ta.EVEN_BIMTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.EVEN_BIMTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.EVEN_BIMTH_START_DATE TIME_PERD_START_DATE_A\\n     FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='EB' \\\"\\\"\\\"\\nBW_2_EB = spark.sql(sub_que4)\\nBW_2_EB.createOrReplaceTempView('BW_2_EB')\\n\\nsub_que5 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          ta.ODD_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.MTH_TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.MTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.MTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.MTH_START_DATE TIME_PERD_START_DATE_A\\n               FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='MTH' \\\"\\\"\\\"\\nBW_2_MTH = spark.sql(sub_que5)\\nBW_2_MTH.createOrReplaceTempView('BW_2_MTH')\\n\\nsub_que6 = \\\"\\\"\\\"select cal.CAL_TYPE_ID, \\n     cal.TIME_PERD_ID_A,\\n     assoc.TIME_PERD_ID_B,\\n     cal.TIME_PERD_ASSOC_TYPE_ID,\\n     ta.TIME_PERD_CLASS_CODE as TIME_PERD_CLASS_CODE_B,\\n     ta.TIME_PERD_TYPE_CODE as TIME_PERD_TYPE_CODE_B,\\n     cal.TIME_PERD_END_DATE_B  as TIME_PERD_END_DATE_B,\\n     ta.TIME_PERD_START_DATE as TIME_PERD_START_DATE_B,\\n     cal.TIME_PERD_CLASS_CODE_A,\\n     cal.TIME_PERD_TYPE_CODE_A,\\n     cal.TIME_PERD_END_DATE_A,\\n     cal.TIME_PERD_START_DATE_A\\n     from CAL_TYPE_2 cal\\n     join mm_time_perd_assoc assoc on cal.TIME_PERD_ID_B=assoc.TIME_PERD_ID_A\\n     JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.TIME_PERD_ID_B\\n     where cal.TIME_PERD_type_CODE_B ='WKMS' and cal.TIME_PERD_CLASS_CODE_B='WK'\\n     and cal.TIME_PERD_TYPE_CODE_A in ('EB','OB','MH')  and cal.TIME_PERD_CLASS_CODE_A in ('BIMTH','MTH')\\n     and ta.TIME_PERD_type_CODE = 'WKSS' and ta.TIME_PERD_CLASS_CODE='WK' \\\"\\\"\\\"\\nWKSS_2_MTH = spark.sql(sub_que6)\\nWKSS_2_MTH.createOrReplaceTempView('WKSS_2_MTH')\\n\\nsub_que7 = \\\"\\\"\\\"SELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM CAL_TYPE_2\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BIMTH_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_EB\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID, CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM MTH_2_BIMTH\\nunion\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM WKSS_2_MTH\\\"\\\"\\\"\\n\\nMM_TIME_PERD_ASSOC_IGRTD_VW = spark.sql(sub_que7)\\nMM_TIME_PERD_ASSOC_IGRTD_VW.createOrReplaceTempView('MM_TIME_PERD_ASSOC_IGRTD_VW')\\n\\n\\ndq12_query1 = f\\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE\\n    FROM   MM_RUN_PRTTN_PLC plc\\n    WHERE  RUN_ID = {run_id}\\n       AND TIME_PERD_CLASS_CODE = '{TIER2_TIME_PERD_CLASS_CODE}' \\\"\\\"\\\"\\ntime_loaded = spark.sql(dq12_query1)\\ntime_loaded.createOrReplaceTempView('time_loaded')\\n\\ndq12_query2 = f\\\"\\\"\\\"SELECT DISTINCT MM_TIME_PERD_END_DATE\\n    FROM   MM_RUN_PRTTN_PLC plc\\n           JOIN dpf_run_vw run\\n             ON run.RUN_ID = plc.RUN_ID\\n           JOIN MM_CNTRT_LKP cntrt\\n             ON cntrt.cntrt_id = run.cntrt_id\\n    WHERE  cntrt.CNTRT_ID = '{cntrt_id}'\\n      AND  run.process_status = 'FINISHED'\\\"\\\"\\\"\\n\\ntime_prev_all = spark.sql(dq12_query2)\\ntime_prev_all.createOrReplaceTempView('time_prev_all')\\n\\ndq12_query3 = \\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE,\\n           'new' AS STATUS\\n    FROM time_loaded\\n    MINUS\\n    SELECT MM_TIME_PERD_END_DATE,\\n           'new' AS STATUS\\n    FROM time_prev_all\\\"\\\"\\\"\\n\\ntime_new = spark.sql(dq12_query3)\\ntime_new.createOrReplaceTempView('time_new')\\n\\ndq12_query4 = \\\"\\\"\\\"SELECT  MM_TIME_PERD_END_DATE,\\n            'redelivered' AS STATUS\\n    FROM time_loaded\\n    INTERSECT\\n    SELECT MM_TIME_PERD_END_DATE,\\n           'redelivered' AS STATUS\\n    FROM time_prev_all\\\"\\\"\\\"\\n\\ntime_redelivered = spark.sql(dq12_query4)\\ntime_redelivered.createOrReplaceTempView('time_redelivered')\\n\\ndq12_query5 = \\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE,\\n         'existing' AS STATUS\\n    FROM time_prev_all\\n    MINUS\\n    SELECT MM_TIME_PERD_END_DATE,\\n           'existing' AS STATUS\\n    FROM time_loaded\\\"\\\"\\\"\\n\\ntime_existing = spark.sql(dq12_query5)\\ntime_existing.createOrReplaceTempView('time_existing')\\n\\ndq12_query6 = \\\"\\\"\\\"SELECT *\\n     FROM time_new\\n     UNION ALL\\n     SELECT *\\n     FROM time_redelivered\\n     UNION ALL\\n     SELECT *\\n     FROM time_existing\\\"\\\"\\\"\\ntime_all = spark.sql(dq12_query6)\\ntime_all.createOrReplaceTempView('time_all')\\n\\ndq12_query7 = f\\\"\\\"\\\"SELECT TIME_PERD_NAME,\\n            TIME_PERD_END_DATE MM_TIME_PERD_END_DATE\\n     FROM   MM_TIME_PERD_FDIM\\n     WHERE  TIME_PERD_TYPE_CODE = '{TIER2_TIME_PERD_TYPE_CODE}'\\n       AND  TIME_PERD_END_DATE BETWEEN (SELECT MIN(MM_TIME_PERD_END_DATE)\\n                                        FROM   time_all)\\n                                   AND (SELECT MAX(MM_TIME_PERD_END_DATE)\\n                                        FROM   time_all)\\\"\\\"\\\"\\n\\ntime_proper_all = spark.sql(dq12_query7)\\ntime_proper_all.createOrReplaceTempView('time_proper_all')\\n\\ndq12_query8 = f\\\"\\\"\\\"SELECT *\\n    FROM   MM_TIME_PERD_ASSOC_IGRTD_VW\\n    WHERE  TIME_PERD_TYPE_CODE_B = '{TIER2_TIME_PERD_TYPE_CODE}'\\n                AND TIME_PERD_type_CODE_A = 'MH' \\\"\\\"\\\"\\n\\nwk_mth_assoc = spark.sql(dq12_query8)\\nwk_mth_assoc.createOrReplaceTempView('wk_mth_assoc')\\n\\ndq12_query9= f\\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE,\\n\\t\\\"new\\\" as STATUS\\n    FROM   MM_RUN_PRTTN_PLC plc\\n    WHERE  RUN_ID = {run_id}\\n       AND TIME_PERD_CLASS_CODE = '{TIER2_TIME_PERD_CLASS_CODE}' \\\"\\\"\\\"\\nwk_loaded = spark.sql(dq12_query9)\\nwk_loaded.createOrReplaceTempView('wk_loaded')\\n\\ndq12_query10 = f\\\"\\\"\\\"SELECT DISTINCT MM_TIME_PERD_END_DATE, \\\"existing\\\" as STATUS\\n    FROM   MM_RUN_PRTTN_PLC plc\\n           JOIN dpf_run_vw run\\n             ON run.RUN_ID = plc.RUN_ID\\n           JOIN MM_CNTRT_LKP cntrt\\n             ON cntrt.cntrt_id = run.cntrt_id\\n    WHERE  cntrt.CNTRT_ID = {cntrt_id}\\n      AND  run.process_status = 'FINISHED'\\n    MINUS\\n    SELECT MM_TIME_PERD_END_DATE, \\\"existing\\\" as STATUS\\n    FROM   wk_loaded\\\"\\\"\\\"\\nwk_prev_loaded = spark.sql(dq12_query10)\\nwk_prev_loaded.createOrReplaceTempView('wk_prev_loaded')\\n\\ndq12_query11 = \\\"\\\"\\\"SELECT *\\n    FROM   wk_loaded\\n    UNION ALL\\n    SELECT *\\n    FROM   wk_prev_loaded\\\"\\\"\\\"\\nwk_all = spark.sql(dq12_query11)\\nwk_all.createOrReplaceTempView('wk_all')\\n\\ndq12_query12 = \\\"\\\"\\\"SELECT DISTINCT wk_mth_assoc.TIME_PERD_END_DATE_A,\\n                    wk_mth_assoc.TIME_PERD_ID_A\\n    FROM wk_loaded\\n         JOIN wk_mth_assoc\\n           ON wk_mth_assoc.TIME_PERD_END_DATE_B = wk_loaded.MM_TIME_PERD_END_DATE\\\"\\\"\\\"\\nmth_loaded = spark.sql(dq12_query12)\\nmth_loaded.createOrReplaceTempView('mth_loaded')\\n\\ndq12_wk_query1 = \\\"\\\"\\\"SELECT time_perd_id AS TIME_PERD_NAME, MM_TIME_PERD_END_DATE as TIME_PERD_END_DATE, STATUS\\n                       FROM mth_loaded\\n                            JOIN wk_mth_assoc\\n                              ON wk_mth_assoc.TIME_PERD_ID_A = mth_loaded.TIME_PERD_ID_A\\n                            LEFT OUTER JOIN wk_all\\n                                         ON wk_all.MM_TIME_PERD_END_DATE = wk_mth_assoc.TIME_PERD_END_DATE_B\\n                            JOIN mm_time_perd time_perd\\n                              ON wk_mth_assoc.TIME_PERD_ID_B = time_perd.TIME_PERD_ID\\n                 WHERE wk_all.MM_TIME_PERD_END_DATE IS NULL\\\"\\\"\\\"\\n\\t\\t\\t\\t \\ndf_dq12_wk = spark.sql(dq12_wk_query1)\\n\\ndq12_time_query1 = \\\"\\\"\\\"SELECT TIME_PERD_NAME, time_proper_all.mm_time_perd_end_date as TIME_PERD_END_DATE , time_all.STATUS\\n                       FROM   time_proper_all\\n                              LEFT OUTER JOIN time_all\\n                                           ON time_proper_all.mm_time_perd_end_date = time_all.mm_time_perd_end_date\\n                       WHERE  time_all.mm_time_perd_end_date IS NULL\\\"\\\"\\\"\\n\\t\\t\\t\\t\\t   \\n\\nif TIER2_TIME_PERD_CLASS_CODE == 'WK':\\n  df_dq12 = spark.sql(dq12_wk_query1)\\nelse:\\n  df_dq12 = spark.sql(dq12_time_query1)\\n\\n#DQ13\\nfrom pyspark.sql.functions import col, lower, expr\\ndf_mkt_dim_sel = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_DIM_VW/').filter(col('srce_sys_id')==srce_sys_id)\\ndf_mkt_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_XREF/') \\ndf_mkt_dim_sel = df_mkt_dim_sel.join(df_mkt_xref, ( (df_mkt_dim_sel.srce_sys_id == df_mkt_xref.SRCE_SYS_ID) & (df_mkt_dim_sel.mkt_skid == df_mkt_xref.MKT_SKID) ), \\\"inner\\\").select('extrn_mkt_id', 'mkt_name')\\ndf_mkt_dim_sel1 = df_mkt_dim_sel.withColumnRenamed('extrn_mkt_id', 'old_extrn_mkt_id').withColumnRenamed('mkt_name', 'old_mkt_name')\\ndf_mmc_mkt_source1 = mmc_mkt_source.filter( expr(\\\"extrn_mkt_id NOT LIKE '%MARKET%TAG%' \\\") ).withColumnRenamed('extrn_mkt_id', 'new_extrn_mkt_id').withColumnRenamed('mkt_name', 'new_mkt_name').select('new_extrn_mkt_id', 'new_mkt_name')\\ndf_mmc_mkt_source1 = df_mmc_mkt_source1.join(df_mkt_dim_sel1, (df_mmc_mkt_source1.new_extrn_mkt_id == df_mkt_dim_sel1.old_extrn_mkt_id) & (df_mmc_mkt_source1.new_mkt_name != df_mkt_dim_sel1.old_mkt_name), 'inner')\\n\\n#DQ14\\ndf_mm_run_prod_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/mm_run_prod_plc')\\nfrom pyspark.sql.functions import col\\ndf_mm_run_prod_plc = df_mm_run_prod_plc.select('SRCE_SYS_ID','PROD_PRTTN_CODE', 'EXTRN_PROD_ID', 'RUN_ID' )\\ndf_mm_run_prod_plc1=df_mm_run_prod_plc.alias('a').join(df_mm_run_prod_plc.alias('b'),~( (col('a.SRCE_SYS_ID') == col('b.SRCE_SYS_ID')) & (col('a.PROD_PRTTN_CODE') == col('b.PROD_PRTTN_CODE')) & (col('a.EXTRN_PROD_ID') == col('b.EXTRN_PROD_ID')) & (col('b.RUN_ID') < run_id) ), \\\"left_anti\\\").selectExpr(\\\"a.PROD_PRTTN_CODE\\\")\\n\\n\\ndf_mm_run_prod_plc.createOrReplaceTempView('MM_RUN_PROD_PLC')\\n\\ndq14_query = f\\\"\\\"\\\"SELECT EXTRN_CODE, PROD_DESCRIPTION FROM (\\nWITH \\nret AS (\\nselect SRCE_SYS_ID, PROD_PRTTN_CODE, EXTRN_PROD_ID from MM_RUN_PROD_PLC a where a.run_id = {run_id}\\nand not exists (\\nselect * from MM_RUN_PROD_PLC b where\\na.SRCE_SYS_ID = b.SRCE_SYS_ID\\nand a.PROD_PRTTN_CODE = b.PROD_PRTTN_CODE\\nand a.EXTRN_PROD_ID = b.EXTRN_PROD_ID\\nand b.run_id < {run_id}\\n)\\n),\\ndata as (\\nselect a.* from tier2_prod_mtrlz_tbl a\\n)\\nselect a.EXTRN_PROD_ID as EXTRN_CODE, nvl(a.SHORT_PROD_DESC_TXT,a.LONG_PROD_DESC_TXT) as PROD_DESCRIPTION\\nfrom data a, ret b\\nwhere a.EXTRN_PROD_ID = b.EXTRN_PROD_ID\\nand a.SRCE_SYS_ID = b.SRCE_SYS_ID\\nand a.PROD_PRTTN_CODE = b.PROD_PRTTN_CODE\\n)\\\"\\\"\\\"\\ndf_dq14 = spark.sql(dq14_query)\\n\\n\\n#DQ15\\ndf_dq15 = df_opt_fact.filter(\\\" (dpf_col_name NOT IN ('EXTRN_PROD_ID', 'EXTRN_MKT_ID','EXTRN_TIME_PERD_ID')) AND  (dlvrd_ind = 'Y') \\\").select('TBL_TYPE_CODE','FILE_COL_NAME','DPF_COL_NAME','OPTIONAL_IND','dlvrd_ind')\\n\\n#Create empty dataframe\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.window import Window\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\nif chk_dq10:\\n  dpf_opt_union = dpf_opt_union.withColumn('DQ10', lit('Unmapped and missing optional columns')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  dpf_opt_union = dpf_opt_union.select('row_id','DQ10','TBL_TYPE_CODE1', 'miss_FILE_COL_NAME','miss_DPF_COL_NAME','DQ10_OPTIONAL_IND', 'DQ10_dlvrd_ind')\\nelse:\\n  df_empty\\n  \\nif chk_dq11:\\n  df_dq11 = df_dq11.withColumn('DQ11', lit('Missing Products')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq11 = df_dq11.select('row_id','DQ11','extrn_prod_id', 'extrn_prod_name', 'prod_skid', 'prod_desc','prod_lvl_name')\\nelse:\\n  df_empty\\n\\n\\nif chk_dq12:\\n  df_dq12 = df_dq12.withColumn('DQ12', lit('Missing and delivered and not properly generated time period')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq12 = df_dq12.select('row_id','DQ12','TIME_PERD_NAME','TIME_PERD_END_DATE','STATUS')\\nelse:\\n  df_empty\\n  \\nif chk_dq13:\\n  df_mmc_mkt_source1 = df_mmc_mkt_source1.withColumn('DQ13', lit('Missing Area Description')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_mmc_mkt_source1 = df_mmc_mkt_source1.select('row_id','DQ13','new_extrn_mkt_id','new_mkt_name','old_extrn_mkt_id','old_mkt_name')\\nelse:\\n  df_empty\\n  \\nif chk_dq14:\\n  df_dq14 = df_dq14.withColumn('DQ14', lit('Missing Products List')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq14 = df_dq14.select('row_id', 'DQ14','EXTRN_CODE', 'PROD_DESCRIPTION' )\\nelse:\\n  df_empty\\n  \\nif chk_dq15:\\n  df_dq15 = df_dq15.withColumn('DQ15', lit('Delivered Measures List')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq15 = df_dq15.select('row_id', 'DQ15','TBL_TYPE_CODE','FILE_COL_NAME' ,'DPF_COL_NAME' ,'OPTIONAL_IND' ,'dlvrd_ind')\\nelse:\\n  df_empty\\n\\nfyi_elig = chk_dq10 | chk_dq11 | chk_dq12 | chk_dq13 | chk_dq14 | chk_dq15\\n\\nif fyi_elig:\\n  df_combine = dpf_opt_union.join(df_dq11, dpf_opt_union.row_id == df_dq11.row_id ,'full').join(df_dq12, df_dq11.row_id == df_dq12.row_id, 'full').join(df_mmc_mkt_source1, df_dq12.row_id == df_mmc_mkt_source1.row_id,'full').join(df_dq14, df_mmc_mkt_source1.row_id == df_dq14.row_id , 'full').join(df_dq15, df_dq14.row_id ==df_dq15.row_id, 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\nelse:\\n  df_combine = df_empty\\n\\ndf_combine.createOrReplaceTempView(\\\"FYI_DATA\\\")\\n\\ndq_query = \\\"\\\"\\\"SELECT \\nrow_id,\\nDQ10,\\nTBL_TYPE_CODE1 AS COMPONENT,\\nmiss_FILE_COL_NAME AS FILE_COLUMN_NAME,\\nmiss_DPF_COL_NAME AS DB_COLUMN_NAME,\\nDQ10_OPTIONAL_IND AS MISSING_OPTIONAL,\\nDQ10_dlvrd_ind AS DELIVERED,\\nDQ11,\\nextrn_prod_id AS SUPPLIER_PRODUCT_TAG,\\nextrn_prod_name AS SUPPLIER_PRODUCT_DESCRIPTION,\\nprod_skid AS PG_PRODUCT_TAG,\\nprod_desc AS PG_PRODUCT_DESCRIPTION,\\nprod_lvl_name AS PRODUCT_LVL_NAME,\\nDQ12,\\nTIME_PERD_NAME,\\nTIME_PERD_END_DATE,\\nSTATUS,\\nDQ13,\\nnew_extrn_mkt_id AS MARKET_SUPPLIER_TAG,\\nnew_mkt_name AS NEW_SUPPLIER_DESCRIPTION,\\nold_mkt_name AS OLD_SUPPLIER_DESCRIPTION,\\nDQ14,\\nEXTRN_CODE AS SUPPLIER_TAG,\\nPROD_DESCRIPTION AS SUPPLIER_DESCRIPTION,\\nDQ15,\\nTBL_TYPE_CODE,\\nFILE_COL_NAME,\\nDPF_COL_NAME,\\nOPTIONAL_IND,\\ndlvrd_ind\\n\\nfrom FYI_DATA\\\"\\\"\\\"\\n\\ndf_combine = spark.sql(dq_query)\\n\\n# KPI information\\ndq10_columns = ['DQ10','COMPONENT', 'FILE_COLUMN_NAME','DB_COLUMN_NAME','MISSING_OPTIONAL', 'DELIVERED']\\ndq11_columns = ['DQ11','SUPPLIER_PRODUCT_TAG','SUPPLIER_PRODUCT_DESCRIPTION', 'PG_PRODUCT_TAG', 'PG_PRODUCT_DESCRIPTION', 'PRODUCT_LVL_NAME' ]\\ndq12_columns = ['DQ12','TIME_PERD_NAME','TIME_PERD_END_DATE','STATUS']\\ndq13_columns = ['DQ13','MARKET_SUPPLIER_TAG','NEW_SUPPLIER_DESCRIPTION','OLD_SUPPLIER_DESCRIPTION']\\ndq14_columns = ['DQ14', 'SUPPLIER_TAG', 'SUPPLIER_DESCRIPTION']\\ndq15_columns = ['DQ15','TBL_TYPE_CODE','FILE_COL_NAME' ,'DPF_COL_NAME' ,'OPTIONAL_IND' ,'dlvrd_ind']\\n\\ncombined_cols = ['row_id']\\ndata = []\\n\\nif chk_dq10:\\n  [combined_cols.append(i) for i in dq10_columns]\\n  dq10_val = ('DELIVERED', 'SQL Validation KPI', \\\"DELIVERED IS NULL\\\", '', 'false', 'Unmapped and missing optional columns', 100 )\\n  data.append(dq10_val)\\nif chk_dq11:\\n  [combined_cols.append(i) for i in dq11_columns]\\n  dq11_val = ('SUPPLIER_PRODUCT_TAG', 'SQL Validation KPI', \\\"SUPPLIER_PRODUCT_TAG IS NULL\\\", '', 'false', 'Missing Products', 100 )\\n  data.append(dq11_val)\\nif chk_dq12:\\n  [combined_cols.append(i) for i in dq12_columns]\\n  dq12_val = ('TIME_PERD_NAME', 'SQL Validation KPI', \\\"TIME_PERD_NAME IS NULL\\\", '', 'false', 'Missing and delivered and not properly generated time period', 100 )\\n  data.append(dq12_val)\\nif chk_dq13:\\n  [combined_cols.append(i) for i in dq13_columns]\\n  dq13_val = ('MARKET_SUPPLIER_TAG', 'SQL Validation KPI', \\\"MARKET_SUPPLIER_TAG IS NULL\\\", '', 'false', 'Modified Area Description', 100 )\\n  data.append(dq13_val)\\nif chk_dq14:\\n  [combined_cols.append(i) for i in dq14_columns]\\n  dq14_val = ('SUPPLIER_TAG', 'SQL Validation KPI', \\\"SUPPLIER_TAG IS NULL\\\", '', 'false', 'New Products list', 100 )\\n  data.append(dq14_val)\\nif chk_dq15:\\n  [combined_cols.append(i) for i in dq15_columns]\\n  dq15_val = ('dlvrd_ind', 'SQL Validation KPI', \\\"dlvrd_ind IS NULL\\\", '', 'false', 'Delivered Measures', 100 )\\n  data.append(dq15_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nschema = ['column', 'kpi_type', 'param_1', 'param_2','fail_on_error', 'check_description','target'  ]\\nif fyi_elig:\\n  df_fyi = spark.createDataFrame(data, schema)\\nelse:\\n  df_fyi = df_empty\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(fyi_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"fyi\\\",BooleanType(),True),\\n    StructField(\\\"fyi_elig\\\",StringType(),True)\\n  ])\\n \\ndf_fyi_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_fyi_eligibility = df_fyi_eligibility.withColumn('fyi_elig', when(col('fyi'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine_fyi'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_fyi'] = {\\\"df_object\\\" :df_fyi}\\ndict_all_dfs['df_fyi_eligibility'] = {\\\"df_object\\\" :df_fyi_eligibility}\\n\\ndf_output_dict['df_combine_fyi'] = df_combine\\ndf_output_dict['df_fyi'] = df_fyi\\ndf_output_dict['df_fyi_eligibility'] = df_fyi_eligibility\\n\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mmc_prod_source\"\n    },\n    {\n      \"name\": \"mmc_mkt_source\"\n    },\n    {\n      \"name\": \"tier2_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"df_dpf_col_asign_vw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_fyi\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_fyi\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_fyi_eligibility\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "inputs - FYI v2",
      "predecessorName": "inputs - FYI v1",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n# Inputs\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col\\nfrom pyspark.sql.window import Window\\n\\n# Copy dataframes\\nmmc_prod_source = dict_all_dfs['mmc_prod_source'][\\\"df_object\\\"]\\nmmc_mkt_source = dict_all_dfs['mmc_mkt_source'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['tier2_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ndf_db_cols_map = dict_all_dfs['df_dpf_col_asign_vw'][\\\"df_object\\\"]\\n\\ncntrt_id = <<CNTRT_ID>>\\nsrce_sys_id = <<SRCE_SYS_ID>>\\nTIER2_TIME_PERD_TYPE_CODE = '<<PERIOD_TYPE>>'\\nTIER2_TIME_PERD_CLASS_CODE = '<<PERIOD_TYPE_CODE>>'\\nrun_id = <<PROCESS_RUN_KEY>>\\n\\nchk_dq10 = ('<<CHK_DQ10>>' == 'true')\\nchk_dq11 = ('<<CHK_DQ11>>' == 'true')\\nchk_dq12 = ('<<CHK_DQ12>>' == 'true')\\nchk_dq13 = ('<<CHK_DQ13>>' == 'true')\\nchk_dq14 = ('<<CHK_DQ14>>' == 'true')\\nchk_dq15 = ('<<CHK_DQ15>>' == 'true')\\n\\n\\n\\n#DQ10\\nfrom pyspark.sql.functions import lower, lit, col, when\\n#df_opt_ind = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/DQ_DPF_COL_ASIGN_VW_opt_ind.csv').filter(f\\\"cntrt_id = {cntrt_id}\\\")\\n\\ndf_db_cols_map = df_db_cols_map.withColumnRenamed('database_column_name', 'DPF_COL_NAME').withColumnRenamed('file_column_name', 'FILE_COL_NAME').withColumnRenamed('table_type', 'TBL_TYPE_CODE').withColumnRenamed(\\\"optional\\\", \\\"optional_ind\\\")\\n\\ndf_opt_ind  = df_db_cols_map.filter(f\\\"cntrt_id = {cntrt_id}\\\")\\n\\nfrom pyspark.sql.functions import col\\n\\ndf_opt_ind_only_N =df_opt_ind.filter((col('cntrt_id')==cntrt_id) &(col('optional_ind')=='N') )\\ndf_opt_ind_only_N = df_opt_ind.withColumn('dpf_col_name', lower('dpf_col_name'))\\n\\ndf_opt_ind_fact = df_opt_ind.filter(\\\" tbl_type_code = 'FACT' \\\")\\ndf_opt_ind_mkt = df_opt_ind.filter(\\\" tbl_type_code = 'MKT' \\\")\\ndf_opt_ind_prod = df_opt_ind.filter(\\\" tbl_type_code = 'PROD' \\\")\\n\\nfact_cols = tier2_fact_mtrlz_tbl.columns\\nmkt_cols = mmc_mkt_source.columns\\nprod_cols = mmc_prod_source.columns\\ndf_opt_fact = df_opt_ind_fact.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_mkt = df_opt_ind_mkt.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\ndf_opt_ind_prod = df_opt_ind_prod.withColumn('dlvrd_ind', when(col('dpf_col_name').isin(fact_cols), lit('Y')).otherwise(lit('N')) )\\n\\ndpf_opt_union = df_opt_fact.unionByName(df_opt_ind_mkt, False).unionByName(df_opt_ind_prod, False)\\ndpf_opt_union.filter(\\\"((dpf_col_name IS NULL) OR (optional_ind = 'Y' AND dlvrd_ind = 'N'))\\\")\\ndpf_opt_union = dpf_opt_union.select('TBL_TYPE_CODE','FILE_COL_NAME', 'DPF_COL_NAME','OPTIONAL_IND','dlvrd_ind')\\ndpf_opt_union = dpf_opt_union.withColumnRenamed('TBL_TYPE_CODE', 'TBL_TYPE_CODE1').withColumnRenamed('FILE_COL_NAME', 'miss_FILE_COL_NAME').withColumnRenamed('DPF_COL_NAME', 'miss_DPF_COL_NAME').withColumnRenamed('OPTIONAL_IND', 'DQ10_OPTIONAL_IND').withColumnRenamed('dlvrd_ind', 'DQ10_dlvrd_ind')\\n\\n# DQ11\\nfrom pyspark.sql.functions import lit, col, abs, concat_ws, collect_list\\n#dpf_run_vw = spark.read.format('parquet').load('/mnt/unrefined/NNIT/tradepanel/adw/DPF_ALL_RUN_VW')\\ndpf_run_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROCESS_RUN_LKP_VW\\\")\\ndpf_run_vw = dpf_run_vw.filter(f'cntrt_id = {cntrt_id}')\\n\\n#mm_run_prttn_plc = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/tables/mm_run_prttn_plc.csv')\\nmm_run_prttn_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PRTTN_PLC/')\\nmm_prod_dim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_DIM_VW')\\nmm_prod_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_XREF')\\n\\ndpf_run_vw.createOrReplaceTempView('dpf_run_vw')\\nmm_run_prttn_plc.createOrReplaceTempView('mm_run_prttn_plc')\\nmm_prod_dim.createOrReplaceTempView('mm_prod_dim')\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\nmmc_prod_source.createOrReplaceTempView('MMC_PROD_SOURCE')\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('tier2_prod_mtrlz_tbl')\\n\\ndq11_query1 = f\\\"\\\"\\\"SELECT  cntrt_id, MAX(run_id) run_id\\n  FROM \\n    dpf_run_vw WHERE cntrt_id = {cntrt_id}\\n  AND process_status = 'FINISHED' \\n  GROUP BY cntrt_id\\\"\\\"\\\"\\nlast_run = spark.sql(dq11_query1)\\nlast_run.createOrReplaceTempView('last_run')\\n\\ndq11_query2 = f\\\"\\\"\\\"SELECT  rp.* \\n  FROM\\n    mm_run_prttn_plc rp, last_run lr\\n  WHERE rp.srce_sys_id = {srce_sys_id} \\n    AND rp.cntrt_id = lr.cntrt_id\\n    AND rp.run_id = lr.run_id\\\"\\\"\\\"\\nlast_prttn = spark.sql(dq11_query2)\\nlast_prttn.createOrReplaceTempView('last_prttn')\\n\\ndq11_query3 = f\\\"\\\"\\\"SELECT \\n    DISTINCT x.extrn_prod_id,  d.prod_skid, d.prod_lvl_name, nvl(short_prod_desc_txt,long_prod_desc_txt) prod_desc\\n  FROM \\n    mm_prod_dim d, \\n    last_prttn lp, \\n    mm_prod_xref x\\n  WHERE d.srce_sys_id = lp.srce_sys_id\\n    AND d.prod_prttn_code = lp.prod_prttn_code\\n    AND d.run_id = lp.run_id\\n    AND d.prod_skid = x.prod_skid\\n    AND d.srce_sys_id = x.srce_sys_id\\n    AND d.prod_prttn_code = x.prod_prttn_code\\n\\tand d.srce_sys_id = {srce_sys_id} \\\"\\\"\\\"\\nlast_prod = spark.sql(dq11_query3)\\nlast_prod.createOrReplaceTempView('last_prod')\\n\\ndq11_query4 = \\\"\\\"\\\"SELECT * FROM last_prod WHERE extrn_prod_id NOT IN (SELECT extrn_prod_id FROM MMC_PROD_SOURCE p)\\\"\\\"\\\"\\n\\nprod_diff = spark.sql(dq11_query4)\\nprod_diff.createOrReplaceTempView('prod_diff')\\n\\ndq11_query5 = \\\"\\\"\\\"SELECT extrn_prod_id, prod_desc extrn_prod_name, prod_skid, prod_desc, prod_lvl_name FROM prod_diff\\\"\\\"\\\"\\ndf_dq11 = spark.sql(dq11_query5)\\n\\n\\n#DQ12\\nfrom pyspark.sql.functions import col\\n# mm_cntrt_lkp = spark.read.format('parquet').load('/mnt/unrefined/NNIT/tradepanel/adw/MM_CNTRT_LKP_VW')\\nmm_cntrt_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CNTRT_LKP\\\")\\nmm_cntrt_lkp = mm_cntrt_lkp.filter(f'cntrt_id = {cntrt_id}')\\n\\nmm_cntrt_lkp.createOrReplaceTempView('MM_CNTRT_LKP')\\nmm_time_perd_fdim = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_FDIM_VW')\\nmm_time_perd_fdim.createOrReplaceTempView('MM_TIME_PERD_FDIM')\\nmm_time_perd_assoc_type = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_ASSOC_TYPE_VW')\\nmm_time_perd_assoc_type.createOrReplaceTempView('mm_time_perd_assoc_type')\\nmm_time_perd_assoc = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_TIME_PERD_ASSOC_VW')\\nmm_time_perd_assoc.createOrReplaceTempView('mm_time_perd_assoc')\\nmm_time_perd = spark.read.format('parquet').load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/tables/TIME_PERD_SDADS/')\\nmm_time_perd.createOrReplaceTempView('mm_time_perd')\\n                                                 \\nsub_que1 = \\\"\\\"\\\"SELECT   2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_b,\\n          tb.time_perd_start_date time_perd_start_date_b,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_a,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          ta.time_perd_end_date time_perd_end_date_a,\\n          ta.time_perd_start_date time_perd_start_date_a\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n     WHERE assoc.CAL_TYPE_ID=2\\\"\\\"\\\"\\nCAL_TYPE_2 = spark.sql(sub_que1)\\nCAL_TYPE_2.createOrReplaceTempView('CAL_TYPE_2')\\n\\nsub_que2 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_B TIME_PERD_ID_A,\\n          assoc.TIME_PERD_ID_A TIME_PERD_ID_B,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_a,\\n          tb.time_perd_end_date time_perd_end_date_A,\\n          tb.time_perd_start_date time_perd_start_date_A\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1\\\"\\\"\\\"\\n\\nBIMTH_2_MTH = spark.sql(sub_que2)\\nBIMTH_2_MTH.createOrReplaceTempView('BIMTH_2_MTH')\\n\\nsub_que3 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          assoc.TIME_PERD_ID_A ,\\n          assoc.TIME_PERD_ID_B ,\\n          assoc.TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_A,\\n          ta.time_perd_end_date time_perd_end_date_A,\\n          ta.time_perd_start_date time_perd_start_date_A,\\n          TB.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          TB.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          tb.time_perd_end_date time_perd_end_date_B,\\n          tb.time_perd_start_date time_perd_start_date_B\\n     FROM mm_time_perd_assoc assoc\\n          JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.time_perd_id_a\\n          JOIN mm_time_perd_fdim tb ON tb.time_perd_id = assoc.time_perd_id_b\\n          JOIN mm_time_perd_assoc_type asty\\n             ON asty.time_perd_assoc_type_id = assoc.time_perd_assoc_type_id\\n    WHERE\\n    (TA.TIME_PERD_TYPE_CODE in ('EB','OB')   AND TB.TIME_PERD_CLASS_CODE = 'MTH' AND TB.TIME_PERD_TYPE_CODE = 'MH')\\n AND assoc.CAL_TYPE_ID =2 AND assoc.TIME_PERD_ASSOC_TYPE_ID=1 \\\"\\\"\\\"\\nMTH_2_BIMTH = spark.sql(sub_que3)\\nMTH_2_BIMTH.createOrReplaceTempView('MTH_2_BIMTH')\\n\\nsub_que4 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          ta.EVEN_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.EVEN_BIMTH_TIME_PERD_CLASS_COD TIME_PERD_CLASS_CODE_A,\\n          ta.EVEN_BIMTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.EVEN_BIMTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.EVEN_BIMTH_START_DATE TIME_PERD_START_DATE_A\\n     FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='EB' \\\"\\\"\\\"\\nBW_2_EB = spark.sql(sub_que4)\\nBW_2_EB.createOrReplaceTempView('BW_2_EB')\\n\\nsub_que5 = \\\"\\\"\\\"SELECT  2 CAL_TYPE_ID,\\n          ta.ODD_BIMTH_TIME_PERD_ID TIME_PERD_ID_A,\\n          ta.TIME_PERD_ID TIME_PERD_ID_B,\\n          1 TIME_PERD_ASSOC_TYPE_ID ,\\n          ta.TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_B,\\n          ta.TIME_PERD_TYPE_CODE TIME_PERD_type_CODE_B,\\n          ta.time_perd_end_date time_perd_end_date_B,\\n          ta.time_perd_start_date time_perd_start_date_B,\\n          ta.MTH_TIME_PERD_CLASS_CODE TIME_PERD_CLASS_CODE_A,\\n          ta.MTH_TIME_PERD_TYPE_CODE TIME_PERD_TYPE_CODE_A,\\n          ta.MTH_END_DATE TIME_PERD_END_DATE_A,\\n          ta.MTH_START_DATE TIME_PERD_START_DATE_A\\n               FROM mm_time_perd_fdim ta\\n    WHERE\\n    TA.TIME_PERD_TYPE_CODE in ('BW') AND  TA.EVEN_BIMTH_TIME_PERD_TYPE_CODE ='MTH' \\\"\\\"\\\"\\nBW_2_MTH = spark.sql(sub_que5)\\nBW_2_MTH.createOrReplaceTempView('BW_2_MTH')\\n\\nsub_que6 = \\\"\\\"\\\"select cal.CAL_TYPE_ID, \\n     cal.TIME_PERD_ID_A,\\n     assoc.TIME_PERD_ID_B,\\n     cal.TIME_PERD_ASSOC_TYPE_ID,\\n     ta.TIME_PERD_CLASS_CODE as TIME_PERD_CLASS_CODE_B,\\n     ta.TIME_PERD_TYPE_CODE as TIME_PERD_TYPE_CODE_B,\\n     cal.TIME_PERD_END_DATE_B  as TIME_PERD_END_DATE_B,\\n     ta.TIME_PERD_START_DATE as TIME_PERD_START_DATE_B,\\n     cal.TIME_PERD_CLASS_CODE_A,\\n     cal.TIME_PERD_TYPE_CODE_A,\\n     cal.TIME_PERD_END_DATE_A,\\n     cal.TIME_PERD_START_DATE_A\\n     from CAL_TYPE_2 cal\\n     join mm_time_perd_assoc assoc on cal.TIME_PERD_ID_B=assoc.TIME_PERD_ID_A\\n     JOIN mm_time_perd_fdim ta ON ta.time_perd_id = assoc.TIME_PERD_ID_B\\n     where cal.TIME_PERD_type_CODE_B ='WKMS' and cal.TIME_PERD_CLASS_CODE_B='WK'\\n     and cal.TIME_PERD_TYPE_CODE_A in ('EB','OB','MH')  and cal.TIME_PERD_CLASS_CODE_A in ('BIMTH','MTH')\\n     and ta.TIME_PERD_type_CODE = 'WKSS' and ta.TIME_PERD_CLASS_CODE='WK' \\\"\\\"\\\"\\nWKSS_2_MTH = spark.sql(sub_que6)\\nWKSS_2_MTH.createOrReplaceTempView('WKSS_2_MTH')\\n\\nsub_que7 = \\\"\\\"\\\"SELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM CAL_TYPE_2\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BIMTH_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_EB\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID, CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM BW_2_MTH\\nUNION\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM MTH_2_BIMTH\\nunion\\nSELECT TIME_PERD_ASSOC_TYPE_ID,CAL_TYPE_ID ,TIME_PERD_ID_A,TIME_PERD_CLASS_CODE_A,TIME_PERD_TYPE_CODE_A,TIME_PERD_START_DATE_A,TIME_PERD_END_DATE_A,\\nTIME_PERD_ID_B,TIME_PERD_CLASS_CODE_B,TIME_PERD_TYPE_CODE_B,TIME_PERD_START_DATE_B,TIME_PERD_END_DATE_B\\nFROM WKSS_2_MTH\\\"\\\"\\\"\\n\\nMM_TIME_PERD_ASSOC_IGRTD_VW = spark.sql(sub_que7)\\nMM_TIME_PERD_ASSOC_IGRTD_VW.createOrReplaceTempView('MM_TIME_PERD_ASSOC_IGRTD_VW')\\n\\n\\ndq12_query1 = f\\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE\\n    FROM   MM_RUN_PRTTN_PLC plc\\n    WHERE  RUN_ID = {run_id}\\n       AND TIME_PERD_CLASS_CODE = '{TIER2_TIME_PERD_CLASS_CODE}' \\\"\\\"\\\"\\ntime_loaded = spark.sql(dq12_query1)\\ntime_loaded.createOrReplaceTempView('time_loaded')\\n\\ndq12_query2 = f\\\"\\\"\\\"SELECT DISTINCT MM_TIME_PERD_END_DATE\\n    FROM   MM_RUN_PRTTN_PLC plc\\n           JOIN dpf_run_vw run\\n             ON run.RUN_ID = plc.RUN_ID\\n           JOIN MM_CNTRT_LKP cntrt\\n             ON cntrt.cntrt_id = run.cntrt_id\\n    WHERE  cntrt.CNTRT_ID = '{cntrt_id}'\\n      AND  run.process_status = 'FINISHED'\\\"\\\"\\\"\\n\\ntime_prev_all = spark.sql(dq12_query2)\\ntime_prev_all.createOrReplaceTempView('time_prev_all')\\n\\ndq12_query3 = \\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE,\\n           'new' AS STATUS\\n    FROM time_loaded\\n    MINUS\\n    SELECT MM_TIME_PERD_END_DATE,\\n           'new' AS STATUS\\n    FROM time_prev_all\\\"\\\"\\\"\\n\\ntime_new = spark.sql(dq12_query3)\\ntime_new.createOrReplaceTempView('time_new')\\n\\ndq12_query4 = \\\"\\\"\\\"SELECT  MM_TIME_PERD_END_DATE,\\n            'redelivered' AS STATUS\\n    FROM time_loaded\\n    INTERSECT\\n    SELECT MM_TIME_PERD_END_DATE,\\n           'redelivered' AS STATUS\\n    FROM time_prev_all\\\"\\\"\\\"\\n\\ntime_redelivered = spark.sql(dq12_query4)\\ntime_redelivered.createOrReplaceTempView('time_redelivered')\\n\\ndq12_query5 = \\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE,\\n         'existing' AS STATUS\\n    FROM time_prev_all\\n    MINUS\\n    SELECT MM_TIME_PERD_END_DATE,\\n           'existing' AS STATUS\\n    FROM time_loaded\\\"\\\"\\\"\\n\\ntime_existing = spark.sql(dq12_query5)\\ntime_existing.createOrReplaceTempView('time_existing')\\n\\ndq12_query6 = \\\"\\\"\\\"SELECT *\\n     FROM time_new\\n     UNION ALL\\n     SELECT *\\n     FROM time_redelivered\\n     UNION ALL\\n     SELECT *\\n     FROM time_existing\\\"\\\"\\\"\\ntime_all = spark.sql(dq12_query6)\\ntime_all.createOrReplaceTempView('time_all')\\n\\ndq12_query7 = f\\\"\\\"\\\"SELECT TIME_PERD_NAME,\\n            TIME_PERD_END_DATE MM_TIME_PERD_END_DATE\\n     FROM   MM_TIME_PERD_FDIM\\n     WHERE  TIME_PERD_TYPE_CODE = '{TIER2_TIME_PERD_TYPE_CODE}'\\n       AND  TIME_PERD_END_DATE BETWEEN (SELECT MIN(MM_TIME_PERD_END_DATE)\\n                                        FROM   time_all)\\n                                   AND (SELECT MAX(MM_TIME_PERD_END_DATE)\\n                                        FROM   time_all)\\\"\\\"\\\"\\n\\ntime_proper_all = spark.sql(dq12_query7)\\ntime_proper_all.createOrReplaceTempView('time_proper_all')\\n\\ndq12_query8 = f\\\"\\\"\\\"SELECT *\\n    FROM   MM_TIME_PERD_ASSOC_IGRTD_VW\\n    WHERE  TIME_PERD_TYPE_CODE_B = '{TIER2_TIME_PERD_TYPE_CODE}'\\n                AND TIME_PERD_type_CODE_A = 'MH' \\\"\\\"\\\"\\n\\nwk_mth_assoc = spark.sql(dq12_query8)\\nwk_mth_assoc.createOrReplaceTempView('wk_mth_assoc')\\n\\ndq12_query9= f\\\"\\\"\\\"SELECT MM_TIME_PERD_END_DATE,\\n\\t\\\"new\\\" as STATUS\\n    FROM   MM_RUN_PRTTN_PLC plc\\n    WHERE  RUN_ID = {run_id}\\n       AND TIME_PERD_CLASS_CODE = '{TIER2_TIME_PERD_CLASS_CODE}' \\\"\\\"\\\"\\nwk_loaded = spark.sql(dq12_query9)\\nwk_loaded.createOrReplaceTempView('wk_loaded')\\n\\ndq12_query10 = f\\\"\\\"\\\"SELECT DISTINCT MM_TIME_PERD_END_DATE, \\\"existing\\\" as STATUS\\n    FROM   MM_RUN_PRTTN_PLC plc\\n           JOIN dpf_run_vw run\\n             ON run.RUN_ID = plc.RUN_ID\\n           JOIN MM_CNTRT_LKP cntrt\\n             ON cntrt.cntrt_id = run.cntrt_id\\n    WHERE  cntrt.CNTRT_ID = {cntrt_id}\\n      AND  run.process_status = 'FINISHED'\\n    MINUS\\n    SELECT MM_TIME_PERD_END_DATE, \\\"existing\\\" as STATUS\\n    FROM   wk_loaded\\\"\\\"\\\"\\nwk_prev_loaded = spark.sql(dq12_query10)\\nwk_prev_loaded.createOrReplaceTempView('wk_prev_loaded')\\n\\ndq12_query11 = \\\"\\\"\\\"SELECT *\\n    FROM   wk_loaded\\n    UNION ALL\\n    SELECT *\\n    FROM   wk_prev_loaded\\\"\\\"\\\"\\nwk_all = spark.sql(dq12_query11)\\nwk_all.createOrReplaceTempView('wk_all')\\n\\ndq12_query12 = \\\"\\\"\\\"SELECT DISTINCT wk_mth_assoc.TIME_PERD_END_DATE_A,\\n                    wk_mth_assoc.TIME_PERD_ID_A\\n    FROM wk_loaded\\n         JOIN wk_mth_assoc\\n           ON wk_mth_assoc.TIME_PERD_END_DATE_B = wk_loaded.MM_TIME_PERD_END_DATE\\\"\\\"\\\"\\nmth_loaded = spark.sql(dq12_query12)\\nmth_loaded.createOrReplaceTempView('mth_loaded')\\n\\ndq12_wk_query1 = \\\"\\\"\\\"SELECT time_perd_id AS TIME_PERD_NAME, MM_TIME_PERD_END_DATE as TIME_PERD_END_DATE, STATUS\\n                       FROM mth_loaded\\n                            JOIN wk_mth_assoc\\n                              ON wk_mth_assoc.TIME_PERD_ID_A = mth_loaded.TIME_PERD_ID_A\\n                            LEFT OUTER JOIN wk_all\\n                                         ON wk_all.MM_TIME_PERD_END_DATE = wk_mth_assoc.TIME_PERD_END_DATE_B\\n                            JOIN mm_time_perd time_perd\\n                              ON wk_mth_assoc.TIME_PERD_ID_B = time_perd.TIME_PERD_ID\\n                 WHERE wk_all.MM_TIME_PERD_END_DATE IS NULL\\\"\\\"\\\"\\n\\t\\t\\t\\t \\ndf_dq12_wk = spark.sql(dq12_wk_query1)\\n\\ndq12_time_query1 = \\\"\\\"\\\"SELECT TIME_PERD_NAME, time_proper_all.mm_time_perd_end_date as TIME_PERD_END_DATE , time_all.STATUS\\n                       FROM   time_proper_all\\n                              LEFT OUTER JOIN time_all\\n                                           ON time_proper_all.mm_time_perd_end_date = time_all.mm_time_perd_end_date\\n                       WHERE  time_all.mm_time_perd_end_date IS NULL\\\"\\\"\\\"\\n\\t\\t\\t\\t\\t   \\n\\nif TIER2_TIME_PERD_CLASS_CODE == 'WK':\\n  df_dq12 = spark.sql(dq12_wk_query1)\\nelse:\\n  df_dq12 = spark.sql(dq12_time_query1)\\n\\n#DQ13\\nfrom pyspark.sql.functions import col, lower, expr\\ndf_mkt_dim_sel = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_DIM_VW/').filter(col('srce_sys_id')==srce_sys_id)\\ndf_mkt_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_XREF/') \\ndf_mkt_dim_sel = df_mkt_dim_sel.join(df_mkt_xref, ( (df_mkt_dim_sel.srce_sys_id == df_mkt_xref.srce_sys_id) & (df_mkt_dim_sel.mkt_skid == df_mkt_xref.mkt_skid) ), \\\"inner\\\").select('extrn_mkt_id', 'mkt_name')\\ndf_mkt_dim_sel1 = df_mkt_dim_sel.withColumnRenamed('extrn_mkt_id', 'old_extrn_mkt_id').withColumnRenamed('mkt_name', 'old_mkt_name')\\ndf_mmc_mkt_source1 = mmc_mkt_source.filter( expr(\\\"extrn_mkt_id NOT LIKE '%MARKET%TAG%' \\\") ).withColumnRenamed('extrn_mkt_id', 'new_extrn_mkt_id').withColumnRenamed('mkt_name', 'new_mkt_name').select('new_extrn_mkt_id', 'new_mkt_name')\\ndf_mmc_mkt_source1 = df_mmc_mkt_source1.join(df_mkt_dim_sel1, (df_mmc_mkt_source1.new_extrn_mkt_id == df_mkt_dim_sel1.old_extrn_mkt_id) & (df_mmc_mkt_source1.new_mkt_name != df_mkt_dim_sel1.old_mkt_name), 'inner')\\n\\n#DQ14\\ndf_mm_run_prod_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PROD_PLC')\\nfrom pyspark.sql.functions import col\\ndf_mm_run_prod_plc = df_mm_run_prod_plc.select('srce_sys_id','prod_prttn_code', 'extrn_prod_id', 'run_id' )\\ndf_mm_run_prod_plc1=df_mm_run_prod_plc.alias('a').join(df_mm_run_prod_plc.alias('b'),~( (col('a.srce_sys_id') == col('b.srce_sys_id')) & (col('a.prod_prttn_code') == col('b.prod_prttn_code')) & (col('a.extrn_prod_id') == col('b.extrn_prod_id')) & (col('b.run_id') < run_id) ), \\\"left_anti\\\").selectExpr(\\\"a.prod_prttn_code\\\")\\n\\n\\ndf_mm_run_prod_plc.createOrReplaceTempView('MM_RUN_PROD_PLC')\\n\\ndq14_query = f\\\"\\\"\\\"SELECT EXTRN_CODE, PROD_DESCRIPTION FROM (\\nWITH \\nret AS (\\nselect SRCE_SYS_ID, PROD_PRTTN_CODE, EXTRN_PROD_ID from MM_RUN_PROD_PLC a where a.run_id = {run_id}\\nand not exists (\\nselect * from MM_RUN_PROD_PLC b where\\na.SRCE_SYS_ID = b.SRCE_SYS_ID\\nand a.PROD_PRTTN_CODE = b.PROD_PRTTN_CODE\\nand a.EXTRN_PROD_ID = b.EXTRN_PROD_ID\\nand b.run_id < {run_id}\\n)\\n),\\ndata as (\\nselect a.* from tier2_prod_mtrlz_tbl a\\n)\\nselect a.EXTRN_PROD_ID as EXTRN_CODE, nvl(a.SHORT_PROD_DESC_TXT,a.LONG_PROD_DESC_TXT) as PROD_DESCRIPTION\\nfrom data a, ret b\\nwhere a.EXTRN_PROD_ID = b.EXTRN_PROD_ID\\nand a.SRCE_SYS_ID = b.SRCE_SYS_ID\\nand a.PROD_PRTTN_CODE = b.PROD_PRTTN_CODE\\n)\\\"\\\"\\\"\\ndf_dq14 = spark.sql(dq14_query)\\n\\n\\n#DQ15\\ndf_dq15 = df_opt_fact.filter(\\\" (dpf_col_name NOT IN ('EXTRN_PROD_ID', 'EXTRN_MKT_ID','EXTRN_TIME_PERD_ID')) AND  (dlvrd_ind = 'Y') \\\").select('TBL_TYPE_CODE','FILE_COL_NAME','DPF_COL_NAME','OPTIONAL_IND','dlvrd_ind')\\n\\n#Create empty dataframe\\nfrom pyspark.sql.types import *\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.window import Window\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\nif chk_dq10:\\n  dpf_opt_union = dpf_opt_union.withColumn('DQ10', lit('Unmapped and missing optional columns')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  dpf_opt_union = dpf_opt_union.select('row_id','DQ10',dpf_opt_union.TBL_TYPE_CODE1.alias('COMPONENT'), dpf_opt_union.miss_FILE_COL_NAME.alias('FILE_COLUMN_NAME'),dpf_opt_union.miss_DPF_COL_NAME.alias('DB_COLUMN_NAME'),dpf_opt_union.DQ10_OPTIONAL_IND.alias('MISSING_OPTIONAL'), dpf_opt_union.DQ10_dlvrd_ind.alias('DELIVERED'))\\nelse:\\n  df_empty\\n\\nif chk_dq11:\\n  df_dq11 = df_dq11.withColumn('DQ11', lit('Missing Products')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq11 = df_dq11.select('row_id','DQ11',df_dq11.extrn_prod_id.alias('SUPPLIER_PRODUCT_TAG'), df_dq11.extrn_prod_name.alias('SUPPLIER_PRODUCT_DESCRIPTION'), df_dq11.prod_skid.alias('PG_PRODUCT_TAG'), df_dq11.prod_desc.alias('PG_PRODUCT_DESCRIPTION'),df_dq11.prod_lvl_name.alias('PRODUCT_LVL_NAME'))\\nelse:\\n  df_empty\\n\\nif chk_dq12:\\n  df_dq12 = df_dq12.withColumn('DQ12', lit('Missing and delivered and not properly generated time period')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq12 = df_dq12.select('row_id','DQ12','TIME_PERD_NAME','TIME_PERD_END_DATE','STATUS')\\nelse:\\n  df_empty\\n\\nif chk_dq13:\\n  df_mmc_mkt_source1 = df_mmc_mkt_source1.withColumn('DQ13', lit('Missing Area Description')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_mmc_mkt_source1 = df_mmc_mkt_source1.select('row_id','DQ13',df_mmc_mkt_source1.new_extrn_mkt_id.alias('MARKET_SUPPLIER_TAG'), df_mmc_mkt_source1.new_mkt_name.alias('NEW_SUPPLIER_DESCRIPTION'),df_mmc_mkt_source1.old_mkt_name.alias('OLD_SUPPLIER_DESCRIPTION'))\\nelse:\\n  df_empty\\n\\nif chk_dq14:\\n  df_dq14 = df_dq14.withColumn('DQ14', lit('Missing Products List')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq14 = df_dq14.select('row_id', 'DQ14',df_dq14.EXTRN_CODE.alias('SUPPLIER_TAG'), df_dq14.PROD_DESCRIPTION.alias('SUPPLIER_DESCRIPTION'))\\nelse:\\n  df_empty\\n\\nif chk_dq15:\\n  df_dq15 = df_dq15.withColumn('DQ15', lit('Delivered Measures List')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq15 = df_dq15.select('row_id', 'DQ15','TBL_TYPE_CODE','FILE_COL_NAME' ,'DPF_COL_NAME' ,'OPTIONAL_IND' ,'dlvrd_ind')\\nelse:\\n  df_empty\\n\\nfyi_elig = chk_dq10 | chk_dq11 | chk_dq12 | chk_dq13 | chk_dq14 | chk_dq15\\n\\nif fyi_elig:\\n  df_combine = dpf_opt_union.join(df_dq11, ['row_id'] ,'full').join(df_dq12, ['row_id'], 'full').join(df_mmc_mkt_source1, ['row_id'],'full').join(df_dq14, ['row_id'] , 'full').join(df_dq15, ['row_id'], 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\nelse:\\n  df_combine = df_empty\\n\\n\\n# KPI information\\ndq10_columns = ['DQ10','COMPONENT', 'FILE_COLUMN_NAME','DB_COLUMN_NAME','MISSING_OPTIONAL', 'DELIVERED']\\ndq11_columns = ['DQ11','SUPPLIER_PRODUCT_TAG','SUPPLIER_PRODUCT_DESCRIPTION', 'PG_PRODUCT_TAG', 'PG_PRODUCT_DESCRIPTION', 'PRODUCT_LVL_NAME' ]\\ndq12_columns = ['DQ12','TIME_PERD_NAME','TIME_PERD_END_DATE','STATUS']\\ndq13_columns = ['DQ13','MARKET_SUPPLIER_TAG','NEW_SUPPLIER_DESCRIPTION','OLD_SUPPLIER_DESCRIPTION']\\ndq14_columns = ['DQ14', 'SUPPLIER_TAG', 'SUPPLIER_DESCRIPTION']\\ndq15_columns = ['DQ15','TBL_TYPE_CODE','FILE_COL_NAME' ,'DPF_COL_NAME' ,'OPTIONAL_IND' ,'dlvrd_ind']\\n\\ncombined_cols = ['row_id']\\ndata = []\\n\\nif chk_dq10:\\n  [combined_cols.append(i) for i in dq10_columns]\\n  dq10_val = ('DELIVERED', 'SQL Validation KPI', \\\"DELIVERED IS NULL\\\", '', 'false', 'Unmapped and missing optional columns', 100 )\\n  data.append(dq10_val)\\nif chk_dq11:\\n  [combined_cols.append(i) for i in dq11_columns]\\n  dq11_val = ('SUPPLIER_PRODUCT_TAG', 'SQL Validation KPI', \\\"SUPPLIER_PRODUCT_TAG IS NULL\\\", '', 'false', 'Missing Products', 100 )\\n  data.append(dq11_val)\\nif chk_dq12:\\n  [combined_cols.append(i) for i in dq12_columns]\\n  dq12_val = ('TIME_PERD_NAME', 'SQL Validation KPI', \\\"TIME_PERD_NAME IS NULL\\\", '', 'false', 'Missing and delivered and not properly generated time period', 100 )\\n  data.append(dq12_val)\\nif chk_dq13:\\n  [combined_cols.append(i) for i in dq13_columns]\\n  dq13_val = ('MARKET_SUPPLIER_TAG', 'SQL Validation KPI', \\\"MARKET_SUPPLIER_TAG IS NULL\\\", '', 'false', 'Modified Area Description', 100 )\\n  data.append(dq13_val)\\nif chk_dq14:\\n  [combined_cols.append(i) for i in dq14_columns]\\n  dq14_val = ('SUPPLIER_TAG', 'SQL Validation KPI', \\\"SUPPLIER_TAG IS NULL\\\", '', 'false', 'New Products list', 100 )\\n  data.append(dq14_val)\\nif chk_dq15:\\n  [combined_cols.append(i) for i in dq15_columns]\\n  dq15_val = ('dlvrd_ind', 'SQL Validation KPI', \\\"dlvrd_ind IS NULL\\\", '', 'false', 'Delivered Measures', 100 )\\n  data.append(dq15_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\nif fyi_elig:\\n  df_fyi = spark.createDataFrame(data, schema_for_kpi)\\nelse:\\n  df_fyi = df_empty\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(fyi_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"fyi\\\",BooleanType(),True),\\n    StructField(\\\"fyi_elig\\\",StringType(),True)\\n  ])\\n \\ndf_fyi_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_fyi_eligibility = df_fyi_eligibility.withColumn('fyi_elig', when(col('fyi'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine_fyi'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_fyi'] = {\\\"df_object\\\" :df_fyi}\\ndict_all_dfs['df_fyi_eligibility'] = {\\\"df_object\\\" :df_fyi_eligibility}\\n\\ndf_output_dict['df_combine_fyi'] = df_combine\\ndf_output_dict['df_fyi'] = df_fyi\\ndf_output_dict['df_fyi_eligibility'] = df_fyi_eligibility\\n\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"mmc_prod_source\"\n    },\n    {\n      \"name\": \"mmc_mkt_source\"\n    },\n    {\n      \"name\": \"tier2_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"df_dpf_col_asign_vw\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_fyi\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_fyi\",\n      \"cache\": \"none\"\n    },\n    {\n      \"name\": \"df_fyi_eligibility\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "FP - FYI checks KPI",
      "predecessorName": "inputs - FYI v2",
      "jsonSpecification": "{\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_fyi.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_fyi\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": false
    },
    {
      "operationName": "GEN - Delivery Lookup update",
      "predecessorName": "FP - FYI checks KPI",
      "jsonSpecification": "{\n  \"separateSparkSession\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\n\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\nrows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 4, 1, 2, <<PROCESS_RUN_KEY>>]]\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"parquet\\\").mode(\\\"append\\\").save(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-refined/MM_DLVRY_RUN_LKP\\\")\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dummy\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "FYI checks",
      "predecessorName": "GEN - Delivery Lookup update",
      "jsonSpecification": "{\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_fyi.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_fyi\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_chk\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ Report - FYI",
      "predecessorName": "FYI checks",
      "jsonSpecification": "{\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"saveToCSV\": \"true\",\n  \"generateHTMLReport\": \"true\",\n  \"generatePDFReport\": \"false\",\n  \"includeDetailedValidationResults\": \"failed rows only\",\n  \"numberOfRowsToDisplay\": 100,\n  \"reportTemplate\": \"default\"\n}",
      "operationVersionName": "DataQualityReport",
      "overridableIndicator": false
    }
  ],
  "graphName": "NNIT_cdl_t2_fyi_v2"
}