{
  "applicationName": "TURBINE_INTERNAL",
  "jsonSpecification": "{\r\n    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\r\n    \"title\": \"DQ Test\",\r\n    \"description\": \"DQ test\",\r\n    \"type\": \"object\",\r\n    \"properties\": {\r\n        \r\n   },\r\n    \"required\": [],\r\n    \"configurable\": []\r\n}",
  "nodes": [
    {
      "operationName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"manualSchema\": \"true\",\n  \"transformations\": [\n    {\n      \"columnType\": \"string\",\n      \"columnName\": \"test\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "CreateSchema",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference data chekcs v1",
      "predecessorName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\ncntrt_id =  <<CNTRT_ID>>\\nsrce_sys_id = <<SRCE_SYS_ID>>\\nTIER2_FACT_TYPE_CODE = '<<FACT_TYPE_CODE>>'\\ntime_perd_type_code = '<<PERIOD_TYPE>>'\\nt2_cat_id = '<<CATEGORY_ID>>'\\n\\n\\nchk_dq16 = ('<<CHK_DQ16>>' =='true')\\nchk_dq17 = ('<<CHK_DQ17>>' =='true')\\nchk_dq18 = ('<<CHK_DQ18>>' =='true')\\nchk_dq19 = ('<<CHK_DQ19>>' =='true')\\n\\n\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['tier2_mkt_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['tier2_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_fact_stgng = dict_all_dfs['tier2_fact_stgng'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_stgng'][\\\"df_object\\\"]\\ndf_fact_raw = dict_all_dfs['df_fact_raw'][\\\"df_object\\\"]\\n#df_mkt_xref = dict_all_dfs['df_mkt_xref'][\\\"df_object\\\"]\\ndf_prod_xref = dict_all_dfs['df_prod_xref'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import col, lit\\n\\ndf_prod_sel = tier2_prod_mtrlz_tbl\\n\\n##################################################\\n\\nfrom pyspark.sql.functions import col\\ndf_mkt_dim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_DIM_VW')\\ndf_mkt_sdim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_SDIM_VW')\\ndf_fact = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_TP_<<TIME_PERD_CLASS_CODE>>_FCT/part_srce_sys_id=<<SRCE_SYS_ID>>/part_cntrt_id=<<CNTRT_ID>>')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_fact_mtrlz_tbl.createOrReplaceTempView('TIER2_FACT_MTRLZ_TBL')\\n\\n#dpf_run_vw = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/unrefined/NNIT/tradepanel/adw/DPF_ALL_RUN_VW')\\n\\ndpf_run_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/ f\\\"select * from adwgp_mm.mm_process_run_lkp_vw where cntrt_id = {cntrt_id}\\\")\\ndpf_run_vw.createOrReplaceTempView('DPF_ALL_RUN_VW')\\n\\n#dpf_cntrt_vw = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/unrefined/NNIT/tradepanel/adw/MM_CNTRT_LKP_VW')\\ndpf_cntrt_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/ f\\\"select * from adwgp_mm.MM_CNTRT_LKP where cntrt_id = {cntrt_id}\\\")\\ndpf_cntrt_vw.createOrReplaceTempView('MM_CNTRT_LKP')\\n\\ndf_mm_run_mkt_plc = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_RUN_MKT_PLC/')\\ndf_mm_run_mkt_plc.createOrReplaceTempView('MM_RUN_MKT_PLC')\\n\\n#mm_run_prttn_plc = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/tables/mm_run_prttn_plc.csv')\\nmm_run_prttn_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PRTTN_PLC/')\\nmm_run_prttn_plc.createOrReplaceTempView('MM_RUN_PRTTN_PLC')\\n\\ndf_fact.createOrReplaceTempView('MM_TP_MTH_FACT')\\n\\ndf_mkt_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_XREF')\\ndf_mkt_xref.createOrReplaceTempView('MM_MKT_XREF')\\n\\nfrom pyspark.sql.functions import col\\ndf_prod_dim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_DIM_VW')\\ndf_prod_sdim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_SDIM_VW')\\n\\ndf_prod_dim.createOrReplaceTempView('MM_PROD_DIM')\\n\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\n#df_mm_run_prod_plc = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/file_struct/MM_RUN_PROD_PLC1.csv')\\ndf_mm_run_prod_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PROD_PLC')\\ndf_mm_run_prod_plc.createOrReplaceTempView('MM_RUN_PROD_PLC')\\n\\ndf_prod_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_XREF')\\ndf_prod_xref.createOrReplaceTempView('MM_PROD_XREF')\\n\\n\\n#Markets delivered in file\\ndq16_query1 = \\\"\\\"\\\" SELECT src.EXTRN_MKT_ID,\\n                src.MKT_NAME AS EXTRN_MKT_NAME,\\n                src.MKT_SKID\\n         FROM   TIER2_MKT_MTRLZ_TBL src\\\"\\\"\\\"\\ndf_mkt_delivery = spark.sql(dq16_query1)\\ndf_mkt_delivery.createOrReplaceTempView('mkt_delivery')\\n\\n#Missing Markets in Fact File\\ndq16_query2 = \\\"\\\"\\\"SELECT mkt.EXTRN_MKT_ID,\\n                         mkt.MKT_SKID\\n         FROM   mkt_delivery mkt\\n                LEFT OUTER JOIN TIER2_FACT_MTRLZ_TBL fct\\n                             ON mkt.EXTRN_MKT_ID = fct.EXTRN_MKT_ID\\n         WHERE  fct.EXTRN_MKT_ID IS NULL\\\"\\\"\\\"\\ndf_miss_mkt_fct_delivery = spark.sql(dq16_query2)\\ndf_miss_mkt_fct_delivery.createOrReplaceTempView('miss_mkt_fct_delivery')\\n\\n#Completed runs of the contract\\ndq16_query3 = f\\\"\\\"\\\"SELECT RUN_ID,\\n                start_date_time as START_TIME_STAMP\\n         FROM   DPF_ALL_RUN_VW\\n         WHERE  CNTRT_ID = (SELECT CNTRT_ID\\n                            FROM   MM_CNTRT_LKP\\n                            WHERE  CNTRT_ID = {cntrt_id})\\n            AND process_status = 'FINISHED'\\n            ORDER BY START_TIME_STAMP DESC\\\"\\\"\\\"\\n\\ndf_cntrt_finished_runs = spark.sql(dq16_query3)\\ndf_cntrt_finished_runs.createOrReplaceTempView('cntrt_finished_runs')\\n\\n# Latest run prior to current run id\\ndq16_query4 = f\\\"\\\"\\\" SELECT RUN_ID\\n         FROM   cntrt_finished_runs\\n         WHERE  START_TIME_STAMP < (SELECT start_date_time\\n                                    FROM   DPF_ALL_RUN_VW\\n                                    WHERE  RUN_ID = {run_id})\\n            \\\"\\\"\\\"\\ndf_last_run = spark.sql(dq16_query4).limit(1)\\ndf_last_run.createOrReplaceTempView('last_run')\\n\\n# Markets delivered in last run\\ndq16_query5=\\\"\\\"\\\"SELECT plc.EXTRN_MKT_ID,\\n                plc.EXTRN_MKT_NAME,\\n                COALESCE(mkt_delivery.MKT_SKID, plc.MKT_SKID) AS MKT_SKID\\n         FROM   (SELECT plc.EXTRN_MKT_ID,\\n                        plc.EXTRN_MKT_NAME,\\n                        MKT_SKID\\n                 FROM   MM_RUN_MKT_PLC plc\\n                 WHERE  RUN_ID = (SELECT RUN_ID\\n                                  FROM   last_run)\\n                 AND  EXTRN_MKT_ID IS NOT NULL) plc\\n                 LEFT OUTER JOIN mkt_delivery\\n                              ON plc.EXTRN_MKT_ID = mkt_delivery.EXTRN_MKT_ID\\\"\\\"\\\"\\ndf_mkt_plc_last_run = spark.sql(dq16_query5)\\ndf_mkt_plc_last_run.createOrReplaceTempView('mkt_plc_last_run')\\n\\n# Time periods and Prod Prttns from last run\\ndq16_query6 = f\\\"\\\"\\\"SELECT RUN_ID,\\n                MM_TIME_PERD_END_DATE AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM   MM_RUN_PRTTN_PLC\\n         WHERE  RUN_ID = (SELECT RUN_ID\\n                          FROM   last_run)\\n            AND TIME_PERD_CLASS_CODE = 'MTH'\\n            AND CNTRT_ID = {cntrt_id}\\\"\\\"\\\"\\ndf_last_run_prttn = spark.sql(dq16_query6)\\ndf_last_run_prttn.createOrReplaceTempView('last_run_prttn')\\n\\n#Facts from last run\\ndq16_query7 = \\\"\\\"\\\"SELECT  DISTINCT fct.SRCE_SYS_ID,\\n                                  fct.MKT_SKID\\n         FROM   MM_TP_MTH_FACT fct\\n                JOIN last_run_prttn tp\\n                  ON fct.RUN_ID = tp.RUN_ID\\n                     AND fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE\\\"\\\"\\\"\\ndf_fct_last_run = spark.sql(dq16_query7)\\ndf_fct_last_run.createOrReplaceTempView('fct_last_run')\\n\\n# I think we can use Market SDIM\\ndq16_query8 = f\\\"\\\"\\\"SELECT mkt_skid,MAX(EXTRN_MKT_ID) EXTRN_MKT_ID, MAX(EXTRN_MKT_NAME) EXTRN_MKT_NAME\\n       FROM mm_mkt_xref\\n       WHERE srce_sys_id = {srce_sys_id}\\n       GROUP BY mkt_skid\\\"\\\"\\\"\\ndf_mkt_skid_slkp = spark.sql(dq16_query8)\\ndf_mkt_skid_slkp.createOrReplaceTempView('mkt_skid_slkp')\\n\\ndq16_query9 = \\\"\\\"\\\"SELECT  m.EXTRN_MKT_ID,\\n                         m.EXTRN_MKT_NAME,\\n                         fct.MKT_SKID\\n         FROM   fct_last_run fct left outer join mkt_skid_slkp m on m.mkt_skid= fct.mkt_skid\\\"\\\"\\\"\\ndf_mkt_fct_last_run = spark.sql(dq16_query9)\\ndf_mkt_fct_last_run.createOrReplaceTempView('mkt_fct_last_run')\\n\\ndq16_query10 = \\\"\\\"\\\"SELECT *\\n         FROM   mkt_plc_last_run\\n         UNION ALL\\n         SELECT *\\n         FROM   mkt_fct_last_run\\n         WHERE  (SELECT COUNT(*)\\n                 FROM   mkt_plc_last_run) = 0\\\"\\\"\\\"\\ndf_mkt_last_run = spark.sql(dq16_query10)\\ndf_mkt_last_run.createOrReplaceTempView('mkt_last_run')\\n\\ndq16_query11 = f\\\"\\\"\\\"SELECT CASE\\n     \\n                    WHEN '{time_perd_type_code}' like 'WK%' THEN\\n                        ADD_MONTHS(TIME_PERD_END_DATE, -1)\\n     \\n                    ELSE TIME_PERD_END_DATE\\n                END AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM (SELECT MAX(MM_TIME_PERD_END_DATE) AS TIME_PERD_END_DATE,\\n                      MAX(SRCE_SYS_ID) AS SRCE_SYS_ID,\\n                      MAX(CNTRT_ID) AS CNTRT_ID,\\n                      MAX(FACT_TYPE_CODE) AS FACT_TYPE_CODE,\\n                      MAX(PROD_PRTTN_CODE) AS PROD_PRTTN_CODE\\n               FROM   MM_RUN_PRTTN_PLC plc\\n              WHERE  RUN_ID IN (SELECT RUN_ID\\n                                FROM   cntrt_finished_runs)\\n                      AND TIME_PERD_CLASS_CODE = 'MTH'\\n                AND CNTRT_ID = '||in_cntrt_id||')\\\"\\\"\\\"\\ndf_last_period_prttn = spark.sql(dq16_query11)\\ndf_last_period_prttn.createOrReplaceTempView('last_period_prttn')\\n\\ndq16_query12 = \\\"\\\"\\\"SELECT DISTINCT fct.SRCE_SYS_ID,\\n                                   fct.MKT_SKID\\n         FROM   MM_TP_MTH_FACT fct\\n                JOIN last_period_prttn tp\\n                  ON fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE\\\"\\\"\\\"\\ndf_fct_last_period = spark.sql(dq16_query12)\\ndf_fct_last_period.createOrReplaceTempView('fct_last_period')\\n\\ndq16_query13 = \\\"\\\"\\\"SELECT  m.EXTRN_MKT_ID,\\n                          m.EXTRN_MKT_NAME,\\n                          fct.MKT_SKID\\n         FROM   fct_last_period fct left outer join mkt_skid_slkp m on m.mkt_skid= fct.mkt_skid\\\"\\\"\\\"\\ndf_mkt_fct_last_period = spark.sql(dq16_query13)\\ndf_mkt_fct_last_period.createOrReplaceTempView('mkt_fct_last_period')\\n\\ndq16_query14 = \\\"\\\"\\\"SELECT COALESCE(mkt_delivery.EXTRN_MKT_ID, mkt_last_run.EXTRN_MKT_ID) AS EXTRN_MKT_ID,\\n                                   COALESCE(mkt_delivery.EXTRN_MKT_NAME, mkt_last_run.EXTRN_MKT_NAME) AS EXTRN_MKT_NAME,\\n                                   COALESCE(mkt_delivery.MKT_SKID, mkt_last_run.MKT_SKID) AS MKT_SKID,\\n                                   NVL2(mkt_delivery.MKT_SKID, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(mkt_last_run.MKT_SKID, 'N', 'Y') AS NEW_IND\\n         FROM   mkt_delivery\\n                FULL OUTER JOIN mkt_last_run\\n                             ON mkt_delivery.MKT_SKID = mkt_last_run.MKT_SKID\\\"\\\"\\\"\\ndf_mkt_delivery_vs_last_run = spark.sql(dq16_query14)\\ndf_mkt_delivery_vs_last_run.createOrReplaceTempView('mkt_delivery_vs_last_run')\\n\\ndq16_query15 = \\\"\\\"\\\"SELECT /*+ materialize */ COALESCE(mkt_delivery.EXTRN_MKT_ID, mkt_fct_last_period.EXTRN_MKT_ID) AS EXTRN_MKT_ID,\\n                                   COALESCE(mkt_delivery.EXTRN_MKT_NAME, mkt_fct_last_period.EXTRN_MKT_NAME) AS EXTRN_MKT_NAME,\\n                                   COALESCE(mkt_delivery.MKT_SKID, mkt_fct_last_period.MKT_SKID) AS MKT_SKID,\\n                                   NVL2(mkt_delivery.MKT_SKID, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(mkt_fct_last_period.MKT_SKID, 'N', 'Y') AS NEW_IND\\n         FROM   mkt_delivery\\n                FULL OUTER JOIN mkt_fct_last_period\\n                             ON mkt_delivery.MKT_SKID = mkt_fct_last_period.MKT_SKID\\\"\\\"\\\"\\ndf_mkt_delivery_vs_last_period = spark.sql(dq16_query15)\\ndf_mkt_delivery_vs_last_period.createOrReplaceTempView('mkt_delivery_vs_last_period')\\n\\ndq16_query16 = \\\"\\\"\\\"SELECT  COALESCE(mkt_delivery_vs_last_run.EXTRN_MKT_ID, mkt_delivery_vs_last_period.EXTRN_MKT_ID) AS EXTRN_MKT_ID,\\n                                   COALESCE(mkt_delivery_vs_last_run.EXTRN_MKT_NAME, mkt_delivery_vs_last_period.EXTRN_MKT_NAME) AS EXTRN_MKT_NAME,\\n                                   COALESCE(mkt_delivery_vs_last_run.MKT_SKID, mkt_delivery_vs_last_period.MKT_SKID) AS MKT_SKID,\\n                                   mkt_delivery_vs_last_run.MISSING_IND AS LAST_RUN_MISSING_IND,\\n                                   mkt_delivery_vs_last_run.NEW_IND AS LAST_RUN_NEW_IND,\\n                                   mkt_delivery_vs_last_period.MISSING_IND AS LAST_PERIOD_MISSING_IND,\\n                                   mkt_delivery_vs_last_period.NEW_IND AS LAST_PERIOD_NEW_IND\\n         FROM   mkt_delivery_vs_last_run\\n                FULL OUTER JOIN mkt_delivery_vs_last_period\\n                             ON mkt_delivery_vs_last_run.MKT_SKID = mkt_delivery_vs_last_period.MKT_SKID\\\"\\\"\\\"\\ndf_mkt_delivery_vs_all_cases = spark.sql(dq16_query16)\\ndf_mkt_delivery_vs_all_cases.createOrReplaceTempView('mkt_delivery_vs_all_cases')\\n\\ndq16_query17 = \\\"\\\"\\\"SELECT  miss_mkt_fct_delivery.MKT_SKID,\\n                                   NVL2(mkt_fct_last_run.MKT_SKID, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_mkt_fct_delivery\\n                LEFT OUTER JOIN mkt_fct_last_run\\n                             ON miss_mkt_fct_delivery.MKT_SKID = mkt_fct_last_run.MKT_SKID\\\"\\\"\\\"\\ndf_miss_mkt_vs_last_run = spark.sql(dq16_query17)\\ndf_miss_mkt_vs_last_run.createOrReplaceTempView('miss_mkt_vs_last_run')\\n\\ndq16_query18 = \\\"\\\"\\\"SELECT  miss_mkt_fct_delivery.MKT_SKID,\\n                                   NVL2(mkt_fct_last_period.MKT_SKID, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_mkt_fct_delivery\\n                LEFT OUTER JOIN mkt_fct_last_period\\n                             ON miss_mkt_fct_delivery.MKT_SKID = mkt_fct_last_period.MKT_SKID\\\"\\\"\\\"\\ndf_miss_mkt_vs_last_period = spark.sql(dq16_query18)\\ndf_miss_mkt_vs_last_period.createOrReplaceTempView('miss_mkt_vs_last_period')\\n\\n#Final Select Query with necessary columns\\n\\ndq16_query19 = \\\"\\\"\\\"SELECT\\n               *,\\n               \\n                       \\n                           (  CASE\\n                                    WHEN LAST_RUN_MISS_VAL = 'Y'\\n                                    AND    LAST_RUN_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_RUN_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END + \\n                             CASE\\n                                    WHEN LAST_PERIOD_MISS_VAL = 'Y'\\n                                    AND    LAST_PERIOD_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_PERIOD_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END\\n                      )\\n                AS VAL\\n         FROM   (SELECT\\n                        CASE\\n                          WHEN mkt_delivery_vs_all_cases.LAST_RUN_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_RUN_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_RUN_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_RUN_STATUS,\\n                        CASE\\n                          WHEN mkt_delivery_vs_all_cases.LAST_PERIOD_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_PERIOD_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_PERIOD_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_PERIOD_STATUS,\\n                        miss_mkt_vs_last_run.MISSING_IND AS LAST_RUN_MISS_VAL,\\n                        miss_mkt_vs_last_period.MISSING_IND AS LAST_PERIOD_MISS_VAL,\\n                        mkt_delivery.MKT_SKID AS MKT_SKID,\\n                        mkt_delivery.EXTRN_MKT_ID AS EXTRN_MKT_ID,\\n                        mkt_delivery.EXTRN_MKT_NAME AS EXTRN_MKT_NAME,\\n                        mkt_delivery_vs_all_cases.EXTRN_MKT_NAME AS MKT_NAME\\n                        \\n                 FROM   mkt_delivery\\n                        FULL OUTER JOIN mkt_delivery_vs_all_cases\\n                                     ON mkt_delivery.MKT_SKID = mkt_delivery_vs_all_cases.MKT_SKID\\n                        LEFT OUTER JOIN miss_mkt_vs_last_run\\n                                     ON mkt_delivery.MKT_SKID = miss_mkt_vs_last_run.MKT_SKID\\n                        LEFT OUTER JOIN miss_mkt_vs_last_period\\n                                     ON mkt_delivery.MKT_SKID = miss_mkt_vs_last_period.MKT_SKID\\n                 WHERE  COALESCE(mkt_delivery.EXTRN_MKT_ID, mkt_delivery_vs_all_cases.EXTRN_MKT_ID) NOT IN ('Market Tag', 'Markets Tag'))\\\"\\\"\\\"\\n\\ndf_dq16_1 = spark.sql(dq16_query19).filter('VAL > 0')\\n\\n#DQ17\\n\\n\\n\\ndq17_query = f\\\"\\\"\\\"WITH prod_dlvr\\n     AS (SELECT src.EXTRN_PROD_ID,\\n                src.PROD_LVL_NAME\\n         FROM   TIER2_PROD_MTRLZ_TBL src),\\n     prod_hier_dlvr\\n     AS (SELECT  DISTINCT PROD_LVL_NAME\\n         FROM   PROD_DLVR\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     miss_prod_hier_fct_dlvr\\n     AS (SELECT  PROD_LVL_NAME\\n         FROM (SELECT PROD_LVL_NAME FROM prod_hier_dlvr\\n               MINUS\\n               SELECT DISTINCT prod.PROD_LVL_NAME\\n               FROM   PROD_DLVR prod\\n                      JOIN TIER2_FACT_MTRLZ_TBL fct\\n                        ON prod.EXTRN_PROD_ID = fct.EXTRN_PROD_ID)),\\n      \\n      cntrt_finished_runs\\n     AS (SELECT RUN_ID,\\n                start_date_time as START_TIME_STAMP\\n         FROM   DPF_ALL_RUN_VW\\n         WHERE  CNTRT_ID = (SELECT CNTRT_ID\\n                            FROM   MM_CNTRT_LKP\\n                            WHERE  CNTRT_ID = {cntrt_id})\\n            AND process_status = 'FINISHED'\\n            ORDER BY START_TIME_STAMP DESC),\\n     last_run\\n     AS (SELECT RUN_ID\\n         FROM   cntrt_finished_runs\\n         WHERE  START_TIME_STAMP < (SELECT start_date_time\\n                                    FROM   DPF_ALL_RUN_VW\\n                                    WHERE  RUN_ID = {run_id})\\n            LIMIT 1),\\n      \\n      prod_last_run\\n     AS (SELECT plc.EXTRN_PROD_ID,\\n                dim.PROD_LVL_NAME\\n         FROM   (SELECT *\\n                 FROM   MM_RUN_PROD_PLC\\n                 WHERE  RUN_ID = (SELECT RUN_ID\\n                                  FROM   LAST_RUN)) plc\\n                JOIN MM_PROD_DIM dim\\n                  ON plc.PROD_SKID = dim.PROD_SKID\\n                     AND plc.SRCE_SYS_ID = dim.SRCE_SYS_ID\\n                     AND plc.PROD_PRTTN_CODE = dim.PROD_PRTTN_CODE),\\n       \\n       prod_hier_plc_last_run\\n     AS (SELECT DISTINCT PROD_LVL_NAME\\n         FROM   prod_last_run\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     last_run_prttn\\n     AS (SELECT RUN_ID,\\n                MM_TIME_PERD_END_DATE AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM   MM_RUN_PRTTN_PLC\\n         WHERE  RUN_ID = (SELECT RUN_ID\\n                          FROM   last_run)\\n            AND TIME_PERD_CLASS_CODE = 'MTH'\\n            AND CNTRT_ID = {cntrt_id}),\\n      \\n      fct_last_run\\n     AS (SELECT  DISTINCT fct.SRCE_SYS_ID,\\n                          fct.CNTRT_ID,\\n                          fct.PROD_SKID,\\n                          fct.PROD_PRTTN_CODE\\n         FROM   MM_TP_MTH_FCT fct\\n                JOIN last_run_prttn tp\\n                  ON fct.RUN_ID = tp.RUN_ID\\n                     AND fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE),\\n       \\n       prod_hier_fct_last_run\\n     AS (SELECT  DISTINCT PROD_LVL_NAME\\n         FROM   fct_last_run fct\\n                JOIN MM_PROD_DIM dim\\n                  ON fct.PROD_SKID = dim.PROD_SKID\\n                     AND fct.SRCE_SYS_ID = dim.SRCE_SYS_ID\\n                     AND fct.PROD_PRTTN_CODE = dim.PROD_PRTTN_CODE\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     prod_hier_last_run\\n     AS (SELECT *\\n         FROM   prod_hier_plc_last_run\\n         UNION ALL\\n         SELECT *\\n         FROM   prod_hier_fct_last_run\\n         WHERE  (SELECT COUNT(*)\\n                 FROM   prod_hier_plc_last_run) = 0),\\n                 \\n     last_period_prttn\\n     AS (SELECT CASE\\n     \\n                    WHEN '{time_perd_type_code}' like 'WK%' THEN\\n                        ADD_MONTHS(TIME_PERD_END_DATE, -1)\\n     \\n                    ELSE TIME_PERD_END_DATE\\n                END AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM (SELECT MAX(MM_TIME_PERD_END_DATE) AS TIME_PERD_END_DATE,\\n                      MAX(SRCE_SYS_ID) AS SRCE_SYS_ID,\\n                      MAX(CNTRT_ID) AS CNTRT_ID,\\n                      MAX(FACT_TYPE_CODE) AS FACT_TYPE_CODE,\\n                      MAX(PROD_PRTTN_CODE) AS PROD_PRTTN_CODE\\n               FROM   MM_RUN_PRTTN_PLC plc\\n              WHERE  RUN_ID IN (SELECT RUN_ID\\n                                FROM   cntrt_finished_runs)\\n                      AND TIME_PERD_CLASS_CODE = 'MTH'\\n                AND CNTRT_ID = {cntrt_id})),\\n                \\n      fct_last_period\\n     AS (SELECT  DISTINCT fct.SRCE_SYS_ID,\\n                          fct.PROD_SKID,\\n                          fct.PROD_PRTTN_CODE\\n         FROM   MM_TP_MTH_FCT fct\\n                JOIN last_period_prttn tp\\n                  ON fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE),\\n                     \\n       prod_hier_fct_last_period\\n     AS (SELECT  DISTINCT PROD_LVL_NAME\\n         FROM   fct_last_period fct\\n                JOIN MM_PROD_DIM dim\\n                  ON fct.PROD_SKID = dim.PROD_SKID\\n                     AND fct.SRCE_SYS_ID = dim.SRCE_SYS_ID\\n                     AND fct.PROD_PRTTN_CODE = dim.PROD_PRTTN_CODE\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     prod_hier_dlvr_vs_last_run\\n     AS (SELECT  COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_last_run.PROD_LVL_NAME) AS PROD_LVL_NAME,\\n                                   NVL2(prod_hier_dlvr.PROD_LVL_NAME, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(prod_hier_last_run.PROD_LVL_NAME, 'N', 'Y') AS NEW_IND\\n         FROM   prod_hier_dlvr\\n                FULL OUTER JOIN prod_hier_last_run\\n                             ON prod_hier_dlvr.PROD_LVL_NAME = prod_hier_last_run.PROD_LVL_NAME),\\n                             \\n     prod_hier_dlvr_vs_last_period\\n     AS (SELECT  COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_fct_last_period.PROD_LVL_NAME) AS PROD_LVL_NAME,\\n                                   NVL2(prod_hier_dlvr.PROD_LVL_NAME, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(prod_hier_fct_last_period.PROD_LVL_NAME, 'N', 'Y') AS NEW_IND\\n         FROM   prod_hier_dlvr\\n                FULL OUTER JOIN prod_hier_fct_last_period\\n                             ON prod_hier_dlvr.PROD_LVL_NAME = prod_hier_fct_last_period.PROD_LVL_NAME),\\n                             \\n      prod_hier_dlvr_vs_all_cases\\n     AS (SELECT COALESCE(prod_hier_dlvr_vs_last_run.PROD_LVL_NAME, prod_hier_dlvr_vs_last_period.PROD_LVL_NAME) AS PROD_LVL_NAME,\\n                                   prod_hier_dlvr_vs_last_run.MISSING_IND AS LAST_RUN_MISSING_IND,\\n                                   prod_hier_dlvr_vs_last_run.NEW_IND AS LAST_RUN_NEW_IND,\\n                                   prod_hier_dlvr_vs_last_period.MISSING_IND AS LAST_PERIOD_MISSING_IND,\\n                                   prod_hier_dlvr_vs_last_period.NEW_IND AS LAST_PERIOD_NEW_IND\\n         FROM   prod_hier_dlvr_vs_last_run\\n                FULL OUTER JOIN prod_hier_dlvr_vs_last_period\\n                             ON prod_hier_dlvr_vs_last_run.PROD_LVL_NAME = prod_hier_dlvr_vs_last_period.PROD_LVL_NAME),\\n                             \\n     miss_prod_hier_vs_last_run\\n     AS (SELECT /*+ materialize */ miss_prod_hier_fct_dlvr.PROD_LVL_NAME,\\n                                   NVL2(prod_hier_fct_last_run.PROD_LVL_NAME, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_prod_hier_fct_dlvr\\n                LEFT OUTER JOIN prod_hier_fct_last_run\\n                             ON miss_prod_hier_fct_dlvr.PROD_LVL_NAME = prod_hier_fct_last_run.PROD_LVL_NAME),\\n     \\n     miss_prod_hier_vs_last_period\\n     AS (SELECT /*+ materialize */ miss_prod_hier_fct_dlvr.PROD_LVL_NAME,\\n                                   NVL2(prod_hier_fct_last_period.PROD_LVL_NAME, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_prod_hier_fct_dlvr\\n                LEFT OUTER JOIN prod_hier_fct_last_period\\n                             ON miss_prod_hier_fct_dlvr.PROD_LVL_NAME = prod_hier_fct_last_period.PROD_LVL_NAME),\\n     \\nret\\n     AS (SELECT\\n                           *,\\n                       \\n                             (CASE\\n                                    WHEN LAST_RUN_MISS_VAL = 'Y'\\n                                    AND    LAST_RUN_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_RUN_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END) + \\n                            ( CASE\\n                                    WHEN LAST_PERIOD_MISS_VAL = 'Y'\\n                                    AND    LAST_PERIOD_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_PERIOD_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END)\\n                      \\n                AS VAL\\n         FROM   (SELECT\\n                        CASE\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_RUN_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_RUN_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_RUN_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_RUN_STATUS,\\n                        CASE\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_PERIOD_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_PERIOD_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_PERIOD_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_PERIOD_STATUS,\\n                        miss_prod_hier_vs_last_run.MISSING_IND AS LAST_RUN_MISS_VAL,\\n                        miss_prod_hier_vs_last_period.MISSING_IND AS LAST_PERIOD_MISS_VAL,\\n                        prod_hier_dlvr.PROD_LVL_NAME AS PROD_LVL_NAME\\n                 FROM   prod_hier_dlvr\\n                        FULL OUTER JOIN prod_hier_dlvr_vs_all_cases\\n                                     ON prod_hier_dlvr.PROD_LVL_NAME = prod_hier_dlvr_vs_all_cases.PROD_LVL_NAME\\n                        LEFT OUTER JOIN miss_prod_hier_vs_last_run\\n                                     ON prod_hier_dlvr.PROD_LVL_NAME = miss_prod_hier_vs_last_run.PROD_LVL_NAME\\n                        LEFT OUTER JOIN miss_prod_hier_vs_last_period\\n                                     ON prod_hier_dlvr.PROD_LVL_NAME = miss_prod_hier_vs_last_period.PROD_LVL_NAME\\n                 WHERE  UPPER(COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_dlvr_vs_all_cases.PROD_LVL_NAME)) NOT IN ('LEVEL', 'PG LEVEL', 'HOLD LEVEL')\\n                        and UPPER(COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_dlvr_vs_all_cases.PROD_LVL_NAME)) not like '%DRILL%'))\\n\\nSELECT PROD_LVL_NAME AS miss_prod_lvl_name, LAST_RUN_STATUS AS miss_last_run_status, LAST_PERIOD_STATUS AS miss_last_period_status ,val AS miss_val FROM ret where val > 0 \\\"\\\"\\\"\\n\\ndf_dq17_1 = spark.sql(dq17_query)\\n\\n#DQ18\\ndf_dq_miss_time_pd = tier2_fact_stgng.filter('extrn_time_perd_id IS NULL').select('extrn_time_perd_id')\\n\\n#DQ19\\n\\nfrom pyspark.sql.functions import col, expr, regexp_replace, concat\\n\\n\\ndf_prod_sel = df_prod_sel.filter(expr(\\\"extrn_prod_id NOT LIKE '%PRODUCT%TAG%' \\\")).select('prod_lvl_name','long_prod_desc_txt','short_prod_desc_txt','extrn_prod_id','srce_sys_id','prod_prttn_code')\\n\\ndf_prod_sel = df_prod_sel.withColumn('combine_prod_desc', concat(col('short_prod_desc_txt'), col('long_prod_desc_txt')) ).withColumn('combine_prod_desc', regexp_replace('combine_prod_desc', ' ', '')).select('extrn_prod_id','combine_prod_desc','prod_lvl_name', 'short_prod_desc_txt', 'long_prod_desc_txt')\\n\\ndf_prod_dim_sel = df_prod_dim.filter(col('srce_sys_id')==srce_sys_id)\\ndf_prod_xref = df_prod_xref.filter(col('prod_prttn_code')== t2_cat_id)\\n\\ndf_prod_xref = df_prod_xref.withColumnRenamed('srce_sys_id', 'srce_sys_id1').withColumnRenamed('prod_skid', 'prod_skid1').withColumnRenamed('prod_prttn_code', 'prod_prttn_code1')\\n\\ndf_join = df_prod_dim_sel.join(df_prod_xref, ((df_prod_dim_sel.SRCE_SYS_ID == df_prod_xref.srce_sys_id1) & (df_prod_dim_sel.prod_skid == df_prod_xref.prod_skid1) & (df_prod_dim_sel.prod_prttn_code == df_prod_xref.prod_prttn_code1)  ) , 'inner').select('extrn_prod_id','prod_prttn_code','prod_skid','srce_sys_id','prod_lvl_name','short_prod_desc_txt', 'long_prod_desc_txt')\\n\\ndf_join = df_join.withColumn('old_combine_prod_desc', concat(col('short_prod_desc_txt'), col('long_prod_desc_txt')) ).withColumn('old_combine_prod_desc', regexp_replace('old_combine_prod_desc', ' ', '')).select('extrn_prod_id','old_combine_prod_desc','prod_lvl_name', 'short_prod_desc_txt','long_prod_desc_txt' )\\n\\ndf_join = df_join.withColumnRenamed('extrn_prod_id', 'old_extrn_prod_id').withColumnRenamed('prod_lvl_name', 'old_prod_lvl_name').withColumnRenamed('short_prod_desc_txt', 'old_short_prod_desc_txt').withColumnRenamed('long_prod_desc_txt', 'old_long_prod_desc_txt')\\n\\ndf_dq19 = df_prod_sel.join(df_join, ((df_prod_sel.extrn_prod_id==df_join.old_extrn_prod_id) & (~(df_prod_sel.combine_prod_desc==df_join.old_combine_prod_desc)) ), 'inner' )\\n\\ndf_dq19 = df_dq19.withColumnRenamed('extrn_prod_id', 'new_extrn_prod_id').withColumnRenamed('prod_lvl_name', 'new_prod_lvl_name').withColumnRenamed('combine_prod_desc', 'new_combine_prod_desc').withColumnRenamed('short_prod_desc_txt', 'new_short_prod_desc_txt').withColumnRenamed('long_prod_desc_txt', 'new_long_prod_desc_txt')\\n\\n##############\\n\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\nif chk_dq16:\\n  df_dq16_1 = df_dq16_1.withColumn('DQ16', lit('Missing/Delivered Areas')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq16_1 = df_dq16_1.select('row_id','DQ16','MKT_SKID','EXTRN_MKT_ID','EXTRN_MKT_NAME', 'MKT_NAME','LAST_RUN_STATUS','LAST_PERIOD_STATUS')\\nelse:\\n  df_dq16_1 = df_empty\\n\\nif chk_dq17:\\n  df_dq17_1 = df_dq17_1.withColumn('DQ17', lit('Missing/Delivered Hierarchies')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq17_1 = df_dq17_1.select('row_id','DQ17','miss_prod_lvl_name', 'miss_last_run_status','miss_last_period_status','miss_val')\\nelse:\\n  df_dq17_1 = df_empty\\n\\nif chk_dq18:\\n  df_dq_miss_time_pd = df_dq_miss_time_pd.withColumn('DQ18', lit('Unknown Time periods')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq_miss_time_pd = df_dq_miss_time_pd.select('row_id', 'DQ18', 'extrn_time_perd_id')\\nelse:\\n  df_dq_miss_time_pd = df_empty\\n\\nif chk_dq19:\\n  df_dq19 = df_dq19.withColumn('DQ19', lit('Modified Product Description')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq19 = df_dq19.select('row_id', 'DQ19', 'new_extrn_prod_id', 'new_prod_lvl_name','new_combine_prod_desc', 'new_short_prod_desc_txt', 'new_long_prod_desc_txt','old_extrn_prod_id','old_prod_lvl_name','old_combine_prod_desc','old_short_prod_desc_txt' ,'old_long_prod_desc_txt' )\\nelse:\\n  df_dq19 = df_empty  \\n\\nref_data_elig = chk_dq16 | chk_dq17 | chk_dq18 | chk_dq19\\n\\nif ref_data_elig:\\n  df_combine = df_dq16_1.join(df_dq17_1,df_dq16_1.row_id == df_dq17_1.row_id , 'full').join(df_dq_miss_time_pd,df_dq_miss_time_pd.row_id == df_dq17_1.row_id , 'full').join(df_dq19,df_dq_miss_time_pd.row_id == df_dq19.row_id , 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\nelse:\\n  df_empty\\n\\n\\n# df_combine = df_dq16_1.join(df_dq17_1,df_dq16_1.row_id == df_dq17_1.row_id , 'full').join(df_dq_miss_time_pd,df_dq_miss_time_pd.row_id == df_dq17_1.row_id , 'full').join(df_dq19,df_dq_miss_time_pd.row_id == df_dq19.row_id , 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\ndf_combine.createOrReplaceTempView('REF_DATA')\\n\\nquery = \\\"\\\"\\\"SELECT\\nrow_id,\\nDQ16,\\nMKT_SKID AS MARKET_ID,\\nEXTRN_MKT_ID AS MARKET_NAME,\\nEXTRN_MKT_NAME AS SUPPLIER_TAG,\\nMKT_NAME AS SUPPLIER_DESCRIPTION,\\nLAST_RUN_STATUS AS STATUS_VS_LAST_LOADED_FILE,\\nLAST_PERIOD_STATUS AS STATUS_VS_LAST_TIME_PERIOD_IN_FACT_TABLE,\\nDQ17,\\nmiss_prod_lvl_name AS PRODUCT_LEVEL_NAME,\\nmiss_last_run_status AS STATUS_VS_LAST_LOAD_FILE,\\nmiss_last_period_status AS STATUS_VS_LAST_TIME_PERD_IN_FACT_TABLE,\\nDQ18,\\nextrn_time_perd_id AS EXTERNAL_TIME_PERIOD_TAG,\\nDQ19,\\nold_extrn_prod_id AS OLD_EXTERNAL_PRODUCT_IDENTIFIER,\\nold_prod_lvl_name AS OLD_PRODUCT_LEVEL_NAME,\\nold_short_prod_desc_txt AS OLD_SHORT_PRODUCT_DESCRIPTION,\\nold_long_prod_desc_txt AS OLD_LONG_PRODUCT_DESCRIPTION,\\nnew_extrn_prod_id AS NEW_EXTERNAL_PRODUCT_IDENTIFIER,\\nnew_prod_lvl_name AS NEW_PRODUCT_LEVEL_NAME,\\nnew_short_prod_desc_txt AS NEW_SHORT_PRODUCT_DESCRIPTION,\\nnew_long_prod_desc_txt AS NEW_LONG_PRODUCT_DESCRIPTION\\nfrom REF_DATA\\\"\\\"\\\"\\n\\ndf_combine = spark.sql(query)\\n\\nfrom pyspark.sql.functions import when\\n\\n# KPI information\\ndq16_columns = ['DQ16','MARKET_ID','MARKET_NAME','SUPPLIER_TAG', 'SUPPLIER_DESCRIPTION','STATUS_VS_LAST_LOADED_FILE','STATUS_VS_LAST_TIME_PERIOD_IN_FACT_TABLE']\\ndq17_columns = ['DQ17','PRODUCT_LEVEL_NAME', 'STATUS_VS_LAST_LOAD_FILE','STATUS_VS_LAST_TIME_PERD_IN_FACT_TABLE']\\ndq18_columns = ['DQ18', 'EXTERNAL_TIME_PERIOD_TAG']\\ndq19_columns = [ 'DQ19','OLD_EXTERNAL_PRODUCT_IDENTIFIER','OLD_PRODUCT_LEVEL_NAME','OLD_SHORT_PRODUCT_DESCRIPTION' ,'OLD_LONG_PRODUCT_DESCRIPTION', 'NEW_EXTERNAL_PRODUCT_IDENTIFIER', 'NEW_PRODUCT_LEVEL_NAME', 'NEW_SHORT_PRODUCT_DESCRIPTION', 'NEW_LONG_PRODUCT_DESCRIPTION' ]\\n\\n\\ncombined_cols = ['row_id']\\ndata = []\\nif chk_dq16:\\n  [combined_cols.append(i) for i in dq16_columns]\\n  dq16_val = ('MARKET_ID', 'SQL Validation KPI', \\\"MARKET_ID IS NULL\\\", '', 'false', 'Missing/Delivered Areas', 100 )\\n  data.append(dq16_val)\\nif chk_dq17:\\n  [combined_cols.append(i) for i in dq17_columns]\\n  dq17_val = ('PRODUCT_LEVEL_NAME', 'SQL Validation KPI', \\\"PRODUCT_LEVEL_NAME IS NULL\\\", '', 'false', 'Missing/Delivered Hierarchies', 100 )\\n  data.append(dq17_val)\\nif chk_dq18:\\n  [combined_cols.append(i) for i in dq18_columns]\\n  dq18_val = ('EXTERNAL_TIME_PERIOD_TAG', 'SQL Validation KPI', \\\"EXTERNAL_TIME_PERIOD_TAG IS NULL\\\", '', 'false', 'Unknown Time periods', 100 )\\n  data.append(dq18_val)\\nif chk_dq19:\\n  [combined_cols.append(i) for i in dq19_columns]\\n  dq19_val = ('NEW_EXTERNAL_PRODUCT_IDENTIFIER', 'SQL Validation KPI', \\\"NEW_EXTERNAL_PRODUCT_IDENTIFIER IS NULL\\\", '', 'false', 'Modified Product Description', 100 )\\n  data.append(dq19_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nschema = ['column', 'kpi_type', 'param_1', 'param_2','fail_on_error', 'check_description','target'  ]\\n\\nif ref_data_elig:\\n  df_ref_data = spark.createDataFrame(data, schema)\\nelse:\\n  df_empty\\n\\n\\n\\n#Reference Data Check Eligibility\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(ref_data_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"reference_data\\\",BooleanType(),True),\\n    StructField(\\\"reference_data_elig\\\",StringType(),True)\\n  ])\\n \\ndf_ref_data_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_ref_data_eligibility = df_ref_data_eligibility.withColumn('reference_data_elig', when(col('reference_data'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine_ref_data'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_ref_data'] = {\\\"df_object\\\" :df_ref_data}\\ndict_all_dfs['df_ref_data_eligibility'] = {\\\"df_object\\\" :df_ref_data_eligibility}\\n\\ndf_output_dict['df_combine_ref_data'] = df_combine\\ndf_output_dict['df_ref_data'] = df_ref_data\\ndf_output_dict['df_ref_data_eligibility'] = df_ref_data_eligibility\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier2_mkt_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_fact_stgng\"\n    },\n    {\n      \"name\": \"df_fact_raw\"\n    },\n    {\n      \"name\": \"df_mkt_xref\"\n    },\n    {\n      \"name\": \"df_prod_xref\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_ref_data\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_ref_data_eligibility\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference data chekcs v2",
      "predecessorName": "Reference data chekcs v1",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\ncntrt_id =  <<CNTRT_ID>>\\nsrce_sys_id = <<SRCE_SYS_ID>>\\nTIER2_FACT_TYPE_CODE = '<<FACT_TYPE_CODE>>'\\ntime_perd_type_code = '<<PERIOD_TYPE>>'\\nt2_cat_id = '<<CATEGORY_ID>>'\\n\\n\\nchk_dq16 = ('<<CHK_DQ16>>' =='true')\\nchk_dq17 = ('<<CHK_DQ17>>' =='true')\\nchk_dq18 = ('<<CHK_DQ18>>' =='true')\\nchk_dq19 = ('<<CHK_DQ19>>' =='true')\\n\\n\\n\\ntier2_mkt_mtrlz_tbl = dict_all_dfs['tier2_mkt_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_prod_mtrlz_tbl = dict_all_dfs['tier2_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ntier2_fact_stgng = dict_all_dfs['tier2_fact_stgng'][\\\"df_object\\\"]\\ntier2_fact_mtrlz_tbl = dict_all_dfs['tier2_fact_stgng'][\\\"df_object\\\"]\\ndf_fact_raw = dict_all_dfs['df_fact_raw'][\\\"df_object\\\"]\\n#df_mkt_xref = dict_all_dfs['df_mkt_xref'][\\\"df_object\\\"]\\ndf_prod_xref = dict_all_dfs['df_prod_xref'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import col, lit\\n\\ndf_prod_sel = tier2_prod_mtrlz_tbl\\n\\n##################################################\\n\\nfrom pyspark.sql.functions import col\\ndf_mkt_dim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_DIM_VW')\\ndf_mkt_sdim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_SDIM_VW')\\ndf_fact = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_TP_<<TIME_PERD_CLASS_CODE>>_FCT/part_srce_sys_id=<<SRCE_SYS_ID>>/part_cntrt_id=<<CNTRT_ID>>')\\n\\ntier2_mkt_mtrlz_tbl.createOrReplaceTempView('TIER2_MKT_MTRLZ_TBL')\\ntier2_fact_mtrlz_tbl.createOrReplaceTempView('TIER2_FACT_MTRLZ_TBL')\\n\\n#dpf_run_vw = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/unrefined/NNIT/tradepanel/adw/DPF_ALL_RUN_VW')\\n\\ndpf_run_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROCESS_RUN_LKP_VW\\\")\\ndpf_run_vw = dpf_run_vw.filter(f'cntrt_id = {cntrt_id}')\\ndpf_run_vw.createOrReplaceTempView('DPF_ALL_RUN_VW')\\n\\n#dpf_cntrt_vw = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/unrefined/NNIT/tradepanel/adw/MM_CNTRT_LKP_VW')\\ndpf_cntrt_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CNTRT_LKP\\\")\\ndpf_cntrt_vw = dpf_cntrt_vw.filter(f'cntrt_id = {cntrt_id}')\\ndpf_cntrt_vw.createOrReplaceTempView('MM_CNTRT_LKP')\\n\\ndf_mm_run_mkt_plc = spark.read.format('parquet').load('/mnt/<<PUBLISH_PATH>>/MM_RUN_MKT_PLC/')\\ndf_mm_run_mkt_plc.createOrReplaceTempView('MM_RUN_MKT_PLC')\\n\\n#mm_run_prttn_plc = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/tables/mm_run_prttn_plc.csv')\\nmm_run_prttn_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PRTTN_PLC/')\\nmm_run_prttn_plc.createOrReplaceTempView('MM_RUN_PRTTN_PLC')\\n\\ndf_fact.createOrReplaceTempView('MM_TP_MTH_FACT')\\n\\ndf_mkt_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_MKT_XREF')\\ndf_mkt_xref.createOrReplaceTempView('MM_MKT_XREF')\\n\\nfrom pyspark.sql.functions import col\\ndf_prod_dim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_DIM_VW')\\ndf_prod_sdim = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_SDIM_VW')\\n\\ndf_prod_dim.createOrReplaceTempView('MM_PROD_DIM')\\n\\ntier2_prod_mtrlz_tbl.createOrReplaceTempView('TIER2_PROD_MTRLZ_TBL')\\n\\n#df_mm_run_prod_plc = spark.read.format('csv').option('header', True).load('/mnt/unrefined/NNIT/tradepanel/cloudpanel-test-unref/test/dvm/file_struct/MM_RUN_PROD_PLC1.csv')\\ndf_mm_run_prod_plc = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_RUN_PROD_PLC')\\ndf_mm_run_prod_plc.createOrReplaceTempView('MM_RUN_PROD_PLC')\\n\\ndf_prod_xref = spark.read.format('parquet').option(\\\"ignoreCorruptFiles\\\", True).option('ignoreMissingFiles', True).load('/mnt/<<PUBLISH_PATH>>/MM_PROD_XREF')\\ndf_prod_xref.createOrReplaceTempView('MM_PROD_XREF')\\n\\n\\n#Markets delivered in file\\ndq16_query1 = \\\"\\\"\\\" SELECT src.EXTRN_MKT_ID,\\n                src.MKT_NAME AS EXTRN_MKT_NAME,\\n                src.MKT_SKID\\n         FROM   TIER2_MKT_MTRLZ_TBL src\\\"\\\"\\\"\\ndf_mkt_delivery = spark.sql(dq16_query1)\\ndf_mkt_delivery.createOrReplaceTempView('mkt_delivery')\\n\\n#Missing Markets in Fact File\\ndq16_query2 = \\\"\\\"\\\"SELECT mkt.EXTRN_MKT_ID,\\n                         mkt.MKT_SKID\\n         FROM   mkt_delivery mkt\\n                LEFT OUTER JOIN TIER2_FACT_MTRLZ_TBL fct\\n                             ON mkt.EXTRN_MKT_ID = fct.EXTRN_MKT_ID\\n         WHERE  fct.EXTRN_MKT_ID IS NULL\\\"\\\"\\\"\\ndf_miss_mkt_fct_delivery = spark.sql(dq16_query2)\\ndf_miss_mkt_fct_delivery.createOrReplaceTempView('miss_mkt_fct_delivery')\\n\\n#Completed runs of the contract\\ndq16_query3 = f\\\"\\\"\\\"SELECT RUN_ID,\\n                start_date_time as START_TIME_STAMP\\n         FROM   DPF_ALL_RUN_VW\\n         WHERE  CNTRT_ID = (SELECT CNTRT_ID\\n                            FROM   MM_CNTRT_LKP\\n                            WHERE  CNTRT_ID = {cntrt_id})\\n            AND process_status = 'FINISHED'\\n            ORDER BY START_TIME_STAMP DESC\\\"\\\"\\\"\\n\\ndf_cntrt_finished_runs = spark.sql(dq16_query3)\\ndf_cntrt_finished_runs.createOrReplaceTempView('cntrt_finished_runs')\\n\\n# Latest run prior to current run id\\ndq16_query4 = f\\\"\\\"\\\" SELECT RUN_ID\\n         FROM   cntrt_finished_runs\\n         WHERE  START_TIME_STAMP < (SELECT start_date_time\\n                                    FROM   DPF_ALL_RUN_VW\\n                                    WHERE  RUN_ID = {run_id})\\n            \\\"\\\"\\\"\\ndf_last_run = spark.sql(dq16_query4).limit(1)\\ndf_last_run.createOrReplaceTempView('last_run')\\n\\n# Markets delivered in last run\\ndq16_query5=\\\"\\\"\\\"SELECT plc.EXTRN_MKT_ID,\\n                plc.EXTRN_MKT_NAME,\\n                COALESCE(mkt_delivery.MKT_SKID, plc.MKT_SKID) AS MKT_SKID\\n         FROM   (SELECT plc.EXTRN_MKT_ID,\\n                        plc.EXTRN_MKT_NAME,\\n                        MKT_SKID\\n                 FROM   MM_RUN_MKT_PLC plc\\n                 WHERE  RUN_ID = (SELECT RUN_ID\\n                                  FROM   last_run)\\n                 AND  EXTRN_MKT_ID IS NOT NULL) plc\\n                 LEFT OUTER JOIN mkt_delivery\\n                              ON plc.EXTRN_MKT_ID = mkt_delivery.EXTRN_MKT_ID\\\"\\\"\\\"\\ndf_mkt_plc_last_run = spark.sql(dq16_query5)\\ndf_mkt_plc_last_run.createOrReplaceTempView('mkt_plc_last_run')\\n\\n# Time periods and Prod Prttns from last run\\ndq16_query6 = f\\\"\\\"\\\"SELECT RUN_ID,\\n                MM_TIME_PERD_END_DATE AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM   MM_RUN_PRTTN_PLC\\n         WHERE  RUN_ID = (SELECT RUN_ID\\n                          FROM   last_run)\\n            AND TIME_PERD_CLASS_CODE = 'MTH'\\n            AND CNTRT_ID = {cntrt_id}\\\"\\\"\\\"\\ndf_last_run_prttn = spark.sql(dq16_query6)\\ndf_last_run_prttn.createOrReplaceTempView('last_run_prttn')\\n\\n#Facts from last run\\ndq16_query7 = \\\"\\\"\\\"SELECT  DISTINCT fct.SRCE_SYS_ID,\\n                                  fct.MKT_SKID\\n         FROM   MM_TP_MTH_FACT fct\\n                JOIN last_run_prttn tp\\n                  ON fct.RUN_ID = tp.RUN_ID\\n                     AND fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE\\\"\\\"\\\"\\ndf_fct_last_run = spark.sql(dq16_query7)\\ndf_fct_last_run.createOrReplaceTempView('fct_last_run')\\n\\n# I think we can use Market SDIM\\ndq16_query8 = f\\\"\\\"\\\"SELECT mkt_skid,MAX(EXTRN_MKT_ID) EXTRN_MKT_ID, MAX(EXTRN_MKT_NAME) EXTRN_MKT_NAME\\n       FROM mm_mkt_xref\\n       WHERE srce_sys_id = {srce_sys_id}\\n       GROUP BY mkt_skid\\\"\\\"\\\"\\ndf_mkt_skid_slkp = spark.sql(dq16_query8)\\ndf_mkt_skid_slkp.createOrReplaceTempView('mkt_skid_slkp')\\n\\ndq16_query9 = \\\"\\\"\\\"SELECT  m.EXTRN_MKT_ID,\\n                         m.EXTRN_MKT_NAME,\\n                         fct.MKT_SKID\\n         FROM   fct_last_run fct left outer join mkt_skid_slkp m on m.mkt_skid= fct.mkt_skid\\\"\\\"\\\"\\ndf_mkt_fct_last_run = spark.sql(dq16_query9)\\ndf_mkt_fct_last_run.createOrReplaceTempView('mkt_fct_last_run')\\n\\ndq16_query10 = \\\"\\\"\\\"SELECT *\\n         FROM   mkt_plc_last_run\\n         UNION ALL\\n         SELECT *\\n         FROM   mkt_fct_last_run\\n         WHERE  (SELECT COUNT(*)\\n                 FROM   mkt_plc_last_run) = 0\\\"\\\"\\\"\\ndf_mkt_last_run = spark.sql(dq16_query10)\\ndf_mkt_last_run.createOrReplaceTempView('mkt_last_run')\\n\\ndq16_query11 = f\\\"\\\"\\\"SELECT CASE\\n     \\n                    WHEN '{time_perd_type_code}' like 'WK%' THEN\\n                        ADD_MONTHS(TIME_PERD_END_DATE, -1)\\n     \\n                    ELSE TIME_PERD_END_DATE\\n                END AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM (SELECT MAX(MM_TIME_PERD_END_DATE) AS TIME_PERD_END_DATE,\\n                      MAX(SRCE_SYS_ID) AS SRCE_SYS_ID,\\n                      MAX(CNTRT_ID) AS CNTRT_ID,\\n                      MAX(FACT_TYPE_CODE) AS FACT_TYPE_CODE,\\n                      MAX(PROD_PRTTN_CODE) AS PROD_PRTTN_CODE\\n               FROM   MM_RUN_PRTTN_PLC plc\\n              WHERE  RUN_ID IN (SELECT RUN_ID\\n                                FROM   cntrt_finished_runs)\\n                      AND TIME_PERD_CLASS_CODE = 'MTH'\\n                AND CNTRT_ID = '||in_cntrt_id||')\\\"\\\"\\\"\\ndf_last_period_prttn = spark.sql(dq16_query11)\\ndf_last_period_prttn.createOrReplaceTempView('last_period_prttn')\\n\\ndq16_query12 = \\\"\\\"\\\"SELECT DISTINCT fct.SRCE_SYS_ID,\\n                                   fct.MKT_SKID\\n         FROM   MM_TP_MTH_FACT fct\\n                JOIN last_period_prttn tp\\n                  ON fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE\\\"\\\"\\\"\\ndf_fct_last_period = spark.sql(dq16_query12)\\ndf_fct_last_period.createOrReplaceTempView('fct_last_period')\\n\\ndq16_query13 = \\\"\\\"\\\"SELECT  m.EXTRN_MKT_ID,\\n                          m.EXTRN_MKT_NAME,\\n                          fct.MKT_SKID\\n         FROM   fct_last_period fct left outer join mkt_skid_slkp m on m.mkt_skid= fct.mkt_skid\\\"\\\"\\\"\\ndf_mkt_fct_last_period = spark.sql(dq16_query13)\\ndf_mkt_fct_last_period.createOrReplaceTempView('mkt_fct_last_period')\\n\\ndq16_query14 = \\\"\\\"\\\"SELECT COALESCE(mkt_delivery.EXTRN_MKT_ID, mkt_last_run.EXTRN_MKT_ID) AS EXTRN_MKT_ID,\\n                                   COALESCE(mkt_delivery.EXTRN_MKT_NAME, mkt_last_run.EXTRN_MKT_NAME) AS EXTRN_MKT_NAME,\\n                                   COALESCE(mkt_delivery.MKT_SKID, mkt_last_run.MKT_SKID) AS MKT_SKID,\\n                                   NVL2(mkt_delivery.MKT_SKID, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(mkt_last_run.MKT_SKID, 'N', 'Y') AS NEW_IND\\n         FROM   mkt_delivery\\n                FULL OUTER JOIN mkt_last_run\\n                             ON mkt_delivery.MKT_SKID = mkt_last_run.MKT_SKID\\\"\\\"\\\"\\ndf_mkt_delivery_vs_last_run = spark.sql(dq16_query14)\\ndf_mkt_delivery_vs_last_run.createOrReplaceTempView('mkt_delivery_vs_last_run')\\n\\ndq16_query15 = \\\"\\\"\\\"SELECT /*+ materialize */ COALESCE(mkt_delivery.EXTRN_MKT_ID, mkt_fct_last_period.EXTRN_MKT_ID) AS EXTRN_MKT_ID,\\n                                   COALESCE(mkt_delivery.EXTRN_MKT_NAME, mkt_fct_last_period.EXTRN_MKT_NAME) AS EXTRN_MKT_NAME,\\n                                   COALESCE(mkt_delivery.MKT_SKID, mkt_fct_last_period.MKT_SKID) AS MKT_SKID,\\n                                   NVL2(mkt_delivery.MKT_SKID, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(mkt_fct_last_period.MKT_SKID, 'N', 'Y') AS NEW_IND\\n         FROM   mkt_delivery\\n                FULL OUTER JOIN mkt_fct_last_period\\n                             ON mkt_delivery.MKT_SKID = mkt_fct_last_period.MKT_SKID\\\"\\\"\\\"\\ndf_mkt_delivery_vs_last_period = spark.sql(dq16_query15)\\ndf_mkt_delivery_vs_last_period.createOrReplaceTempView('mkt_delivery_vs_last_period')\\n\\ndq16_query16 = \\\"\\\"\\\"SELECT  COALESCE(mkt_delivery_vs_last_run.EXTRN_MKT_ID, mkt_delivery_vs_last_period.EXTRN_MKT_ID) AS EXTRN_MKT_ID,\\n                                   COALESCE(mkt_delivery_vs_last_run.EXTRN_MKT_NAME, mkt_delivery_vs_last_period.EXTRN_MKT_NAME) AS EXTRN_MKT_NAME,\\n                                   COALESCE(mkt_delivery_vs_last_run.MKT_SKID, mkt_delivery_vs_last_period.MKT_SKID) AS MKT_SKID,\\n                                   mkt_delivery_vs_last_run.MISSING_IND AS LAST_RUN_MISSING_IND,\\n                                   mkt_delivery_vs_last_run.NEW_IND AS LAST_RUN_NEW_IND,\\n                                   mkt_delivery_vs_last_period.MISSING_IND AS LAST_PERIOD_MISSING_IND,\\n                                   mkt_delivery_vs_last_period.NEW_IND AS LAST_PERIOD_NEW_IND\\n         FROM   mkt_delivery_vs_last_run\\n                FULL OUTER JOIN mkt_delivery_vs_last_period\\n                             ON mkt_delivery_vs_last_run.MKT_SKID = mkt_delivery_vs_last_period.MKT_SKID\\\"\\\"\\\"\\ndf_mkt_delivery_vs_all_cases = spark.sql(dq16_query16)\\ndf_mkt_delivery_vs_all_cases.createOrReplaceTempView('mkt_delivery_vs_all_cases')\\n\\ndq16_query17 = \\\"\\\"\\\"SELECT  miss_mkt_fct_delivery.MKT_SKID,\\n                                   NVL2(mkt_fct_last_run.MKT_SKID, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_mkt_fct_delivery\\n                LEFT OUTER JOIN mkt_fct_last_run\\n                             ON miss_mkt_fct_delivery.MKT_SKID = mkt_fct_last_run.MKT_SKID\\\"\\\"\\\"\\ndf_miss_mkt_vs_last_run = spark.sql(dq16_query17)\\ndf_miss_mkt_vs_last_run.createOrReplaceTempView('miss_mkt_vs_last_run')\\n\\ndq16_query18 = \\\"\\\"\\\"SELECT  miss_mkt_fct_delivery.MKT_SKID,\\n                                   NVL2(mkt_fct_last_period.MKT_SKID, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_mkt_fct_delivery\\n                LEFT OUTER JOIN mkt_fct_last_period\\n                             ON miss_mkt_fct_delivery.MKT_SKID = mkt_fct_last_period.MKT_SKID\\\"\\\"\\\"\\ndf_miss_mkt_vs_last_period = spark.sql(dq16_query18)\\ndf_miss_mkt_vs_last_period.createOrReplaceTempView('miss_mkt_vs_last_period')\\n\\n#Final Select Query with necessary columns\\n\\ndq16_query19 = \\\"\\\"\\\"SELECT\\n               *,\\n               \\n                       \\n                           (  CASE\\n                                    WHEN LAST_RUN_MISS_VAL = 'Y'\\n                                    AND    LAST_RUN_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_RUN_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END + \\n                             CASE\\n                                    WHEN LAST_PERIOD_MISS_VAL = 'Y'\\n                                    AND    LAST_PERIOD_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_PERIOD_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END\\n                      )\\n                AS VAL\\n         FROM   (SELECT\\n                        CASE\\n                          WHEN mkt_delivery_vs_all_cases.LAST_RUN_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_RUN_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_RUN_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_RUN_STATUS,\\n                        CASE\\n                          WHEN mkt_delivery_vs_all_cases.LAST_PERIOD_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_PERIOD_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN mkt_delivery_vs_all_cases.LAST_PERIOD_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_PERIOD_STATUS,\\n                        miss_mkt_vs_last_run.MISSING_IND AS LAST_RUN_MISS_VAL,\\n                        miss_mkt_vs_last_period.MISSING_IND AS LAST_PERIOD_MISS_VAL,\\n                        mkt_delivery.MKT_SKID AS MKT_SKID,\\n                        mkt_delivery.EXTRN_MKT_ID AS EXTRN_MKT_ID,\\n                        mkt_delivery.EXTRN_MKT_NAME AS EXTRN_MKT_NAME,\\n                        mkt_delivery_vs_all_cases.EXTRN_MKT_NAME AS MKT_NAME\\n                        \\n                 FROM   mkt_delivery\\n                        FULL OUTER JOIN mkt_delivery_vs_all_cases\\n                                     ON mkt_delivery.MKT_SKID = mkt_delivery_vs_all_cases.MKT_SKID\\n                        LEFT OUTER JOIN miss_mkt_vs_last_run\\n                                     ON mkt_delivery.MKT_SKID = miss_mkt_vs_last_run.MKT_SKID\\n                        LEFT OUTER JOIN miss_mkt_vs_last_period\\n                                     ON mkt_delivery.MKT_SKID = miss_mkt_vs_last_period.MKT_SKID\\n                 WHERE  COALESCE(mkt_delivery.EXTRN_MKT_ID, mkt_delivery_vs_all_cases.EXTRN_MKT_ID) NOT IN ('Market Tag', 'Markets Tag'))\\\"\\\"\\\"\\n\\ndf_dq16_1 = spark.sql(dq16_query19).filter('VAL > 0')\\n\\n#DQ17\\n\\n\\n\\ndq17_query = f\\\"\\\"\\\"WITH prod_dlvr\\n     AS (SELECT src.EXTRN_PROD_ID,\\n                src.PROD_LVL_NAME\\n         FROM   TIER2_PROD_MTRLZ_TBL src),\\n     prod_hier_dlvr\\n     AS (SELECT  DISTINCT PROD_LVL_NAME\\n         FROM   PROD_DLVR\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     miss_prod_hier_fct_dlvr\\n     AS (SELECT  PROD_LVL_NAME\\n         FROM (SELECT PROD_LVL_NAME FROM prod_hier_dlvr\\n               MINUS\\n               SELECT DISTINCT prod.PROD_LVL_NAME\\n               FROM   PROD_DLVR prod\\n                      JOIN TIER2_FACT_MTRLZ_TBL fct\\n                        ON prod.EXTRN_PROD_ID = fct.EXTRN_PROD_ID)),\\n      \\n      cntrt_finished_runs\\n     AS (SELECT RUN_ID,\\n                start_date_time as START_TIME_STAMP\\n         FROM   DPF_ALL_RUN_VW\\n         WHERE  CNTRT_ID = (SELECT CNTRT_ID\\n                            FROM   MM_CNTRT_LKP\\n                            WHERE  CNTRT_ID = {cntrt_id})\\n            AND process_status = 'FINISHED'\\n            ORDER BY START_TIME_STAMP DESC),\\n     last_run\\n     AS (SELECT RUN_ID\\n         FROM   cntrt_finished_runs\\n         WHERE  START_TIME_STAMP < (SELECT start_date_time\\n                                    FROM   DPF_ALL_RUN_VW\\n                                    WHERE  RUN_ID = {run_id})\\n            LIMIT 1),\\n      \\n      prod_last_run\\n     AS (SELECT plc.EXTRN_PROD_ID,\\n                dim.PROD_LVL_NAME\\n         FROM   (SELECT *\\n                 FROM   MM_RUN_PROD_PLC\\n                 WHERE  RUN_ID = (SELECT RUN_ID\\n                                  FROM   LAST_RUN)) plc\\n                JOIN MM_PROD_DIM dim\\n                  ON plc.PROD_SKID = dim.PROD_SKID\\n                     AND plc.SRCE_SYS_ID = dim.SRCE_SYS_ID\\n                     AND plc.PROD_PRTTN_CODE = dim.PROD_PRTTN_CODE),\\n       \\n       prod_hier_plc_last_run\\n     AS (SELECT DISTINCT PROD_LVL_NAME\\n         FROM   prod_last_run\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     last_run_prttn\\n     AS (SELECT RUN_ID,\\n                MM_TIME_PERD_END_DATE AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM   MM_RUN_PRTTN_PLC\\n         WHERE  RUN_ID = (SELECT RUN_ID\\n                          FROM   last_run)\\n            AND TIME_PERD_CLASS_CODE = 'MTH'\\n            AND CNTRT_ID = {cntrt_id}),\\n      \\n      fct_last_run\\n     AS (SELECT  DISTINCT fct.SRCE_SYS_ID,\\n                          fct.CNTRT_ID,\\n                          fct.PROD_SKID,\\n                          fct.PROD_PRTTN_CODE\\n         FROM   MM_TP_MTH_FCT fct\\n                JOIN last_run_prttn tp\\n                  ON fct.RUN_ID = tp.RUN_ID\\n                     AND fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE),\\n       \\n       prod_hier_fct_last_run\\n     AS (SELECT  DISTINCT PROD_LVL_NAME\\n         FROM   fct_last_run fct\\n                JOIN MM_PROD_DIM dim\\n                  ON fct.PROD_SKID = dim.PROD_SKID\\n                     AND fct.SRCE_SYS_ID = dim.SRCE_SYS_ID\\n                     AND fct.PROD_PRTTN_CODE = dim.PROD_PRTTN_CODE\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     prod_hier_last_run\\n     AS (SELECT *\\n         FROM   prod_hier_plc_last_run\\n         UNION ALL\\n         SELECT *\\n         FROM   prod_hier_fct_last_run\\n         WHERE  (SELECT COUNT(*)\\n                 FROM   prod_hier_plc_last_run) = 0),\\n                 \\n     last_period_prttn\\n     AS (SELECT CASE\\n     \\n                    WHEN '{time_perd_type_code}' like 'WK%' THEN\\n                        ADD_MONTHS(TIME_PERD_END_DATE, -1)\\n     \\n                    ELSE TIME_PERD_END_DATE\\n                END AS TIME_PERD_END_DATE,\\n                SRCE_SYS_ID,\\n                CNTRT_ID,\\n                FACT_TYPE_CODE,\\n                PROD_PRTTN_CODE\\n         FROM (SELECT MAX(MM_TIME_PERD_END_DATE) AS TIME_PERD_END_DATE,\\n                      MAX(SRCE_SYS_ID) AS SRCE_SYS_ID,\\n                      MAX(CNTRT_ID) AS CNTRT_ID,\\n                      MAX(FACT_TYPE_CODE) AS FACT_TYPE_CODE,\\n                      MAX(PROD_PRTTN_CODE) AS PROD_PRTTN_CODE\\n               FROM   MM_RUN_PRTTN_PLC plc\\n              WHERE  RUN_ID IN (SELECT RUN_ID\\n                                FROM   cntrt_finished_runs)\\n                      AND TIME_PERD_CLASS_CODE = 'MTH'\\n                AND CNTRT_ID = {cntrt_id})),\\n                \\n      fct_last_period\\n     AS (SELECT  DISTINCT fct.SRCE_SYS_ID,\\n                          fct.PROD_SKID,\\n                          fct.PROD_PRTTN_CODE\\n         FROM   MM_TP_MTH_FCT fct\\n                JOIN last_period_prttn tp\\n                  ON fct.MM_TIME_PERD_END_DATE = tp.TIME_PERD_END_DATE\\n                     AND fct.SRCE_SYS_ID = tp.SRCE_SYS_ID\\n                     AND fct.CNTRT_ID = tp.CNTRT_ID\\n                     AND fct.FACT_TYPE_CODE = tp.FACT_TYPE_CODE\\n                     AND fct.PROD_PRTTN_CODE = tp.PROD_PRTTN_CODE),\\n                     \\n       prod_hier_fct_last_period\\n     AS (SELECT  DISTINCT PROD_LVL_NAME\\n         FROM   fct_last_period fct\\n                JOIN MM_PROD_DIM dim\\n                  ON fct.PROD_SKID = dim.PROD_SKID\\n                     AND fct.SRCE_SYS_ID = dim.SRCE_SYS_ID\\n                     AND fct.PROD_PRTTN_CODE = dim.PROD_PRTTN_CODE\\n         WHERE  PROD_LVL_NAME IS NOT NULL),\\n         \\n     prod_hier_dlvr_vs_last_run\\n     AS (SELECT  COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_last_run.PROD_LVL_NAME) AS PROD_LVL_NAME,\\n                                   NVL2(prod_hier_dlvr.PROD_LVL_NAME, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(prod_hier_last_run.PROD_LVL_NAME, 'N', 'Y') AS NEW_IND\\n         FROM   prod_hier_dlvr\\n                FULL OUTER JOIN prod_hier_last_run\\n                             ON prod_hier_dlvr.PROD_LVL_NAME = prod_hier_last_run.PROD_LVL_NAME),\\n                             \\n     prod_hier_dlvr_vs_last_period\\n     AS (SELECT  COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_fct_last_period.PROD_LVL_NAME) AS PROD_LVL_NAME,\\n                                   NVL2(prod_hier_dlvr.PROD_LVL_NAME, 'N', 'Y') AS MISSING_IND,\\n                                   NVL2(prod_hier_fct_last_period.PROD_LVL_NAME, 'N', 'Y') AS NEW_IND\\n         FROM   prod_hier_dlvr\\n                FULL OUTER JOIN prod_hier_fct_last_period\\n                             ON prod_hier_dlvr.PROD_LVL_NAME = prod_hier_fct_last_period.PROD_LVL_NAME),\\n                             \\n      prod_hier_dlvr_vs_all_cases\\n     AS (SELECT COALESCE(prod_hier_dlvr_vs_last_run.PROD_LVL_NAME, prod_hier_dlvr_vs_last_period.PROD_LVL_NAME) AS PROD_LVL_NAME,\\n                                   prod_hier_dlvr_vs_last_run.MISSING_IND AS LAST_RUN_MISSING_IND,\\n                                   prod_hier_dlvr_vs_last_run.NEW_IND AS LAST_RUN_NEW_IND,\\n                                   prod_hier_dlvr_vs_last_period.MISSING_IND AS LAST_PERIOD_MISSING_IND,\\n                                   prod_hier_dlvr_vs_last_period.NEW_IND AS LAST_PERIOD_NEW_IND\\n         FROM   prod_hier_dlvr_vs_last_run\\n                FULL OUTER JOIN prod_hier_dlvr_vs_last_period\\n                             ON prod_hier_dlvr_vs_last_run.PROD_LVL_NAME = prod_hier_dlvr_vs_last_period.PROD_LVL_NAME),\\n                             \\n     miss_prod_hier_vs_last_run\\n     AS (SELECT /*+ materialize */ miss_prod_hier_fct_dlvr.PROD_LVL_NAME,\\n                                   NVL2(prod_hier_fct_last_run.PROD_LVL_NAME, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_prod_hier_fct_dlvr\\n                LEFT OUTER JOIN prod_hier_fct_last_run\\n                             ON miss_prod_hier_fct_dlvr.PROD_LVL_NAME = prod_hier_fct_last_run.PROD_LVL_NAME),\\n     \\n     miss_prod_hier_vs_last_period\\n     AS (SELECT /*+ materialize */ miss_prod_hier_fct_dlvr.PROD_LVL_NAME,\\n                                   NVL2(prod_hier_fct_last_period.PROD_LVL_NAME, 'Y', 'N') AS MISSING_IND\\n         FROM   miss_prod_hier_fct_dlvr\\n                LEFT OUTER JOIN prod_hier_fct_last_period\\n                             ON miss_prod_hier_fct_dlvr.PROD_LVL_NAME = prod_hier_fct_last_period.PROD_LVL_NAME),\\n     \\nret\\n     AS (SELECT\\n                           *,\\n                       \\n                             (CASE\\n                                    WHEN LAST_RUN_MISS_VAL = 'Y'\\n                                    AND    LAST_RUN_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_RUN_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END) + \\n                            ( CASE\\n                                    WHEN LAST_PERIOD_MISS_VAL = 'Y'\\n                                    AND    LAST_PERIOD_STATUS = 'CONTINUE' THEN 1\\n                                    WHEN LAST_PERIOD_STATUS = 'MISSING' THEN 1\\n                                    ELSE 0\\n                             END)\\n                      \\n                AS VAL\\n         FROM   (SELECT\\n                        CASE\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_RUN_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_RUN_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_RUN_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_RUN_STATUS,\\n                        CASE\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_PERIOD_MISSING_IND = 'Y' THEN 'MISSING'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_PERIOD_NEW_IND = 'Y' THEN 'NEW'\\n                          WHEN prod_hier_dlvr_vs_all_cases.LAST_PERIOD_MISSING_IND IS NULL THEN 'MISSING'\\n                          ELSE 'CONTINUE'\\n                        END\\n                        AS LAST_PERIOD_STATUS,\\n                        miss_prod_hier_vs_last_run.MISSING_IND AS LAST_RUN_MISS_VAL,\\n                        miss_prod_hier_vs_last_period.MISSING_IND AS LAST_PERIOD_MISS_VAL,\\n                        prod_hier_dlvr.PROD_LVL_NAME AS PROD_LVL_NAME\\n                 FROM   prod_hier_dlvr\\n                        FULL OUTER JOIN prod_hier_dlvr_vs_all_cases\\n                                     ON prod_hier_dlvr.PROD_LVL_NAME = prod_hier_dlvr_vs_all_cases.PROD_LVL_NAME\\n                        LEFT OUTER JOIN miss_prod_hier_vs_last_run\\n                                     ON prod_hier_dlvr.PROD_LVL_NAME = miss_prod_hier_vs_last_run.PROD_LVL_NAME\\n                        LEFT OUTER JOIN miss_prod_hier_vs_last_period\\n                                     ON prod_hier_dlvr.PROD_LVL_NAME = miss_prod_hier_vs_last_period.PROD_LVL_NAME\\n                 WHERE  UPPER(COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_dlvr_vs_all_cases.PROD_LVL_NAME)) NOT IN ('LEVEL', 'PG LEVEL', 'HOLD LEVEL')\\n                        and UPPER(COALESCE(prod_hier_dlvr.PROD_LVL_NAME, prod_hier_dlvr_vs_all_cases.PROD_LVL_NAME)) not like '%DRILL%'))\\n\\nSELECT PROD_LVL_NAME AS miss_prod_lvl_name, LAST_RUN_STATUS AS miss_last_run_status, LAST_PERIOD_STATUS AS miss_last_period_status ,val AS miss_val FROM ret where val > 0 \\\"\\\"\\\"\\n\\ndf_dq17_1 = spark.sql(dq17_query)\\n\\n#DQ18\\ndf_dq_miss_time_pd = tier2_fact_stgng.filter('extrn_time_perd_id IS NULL').select('extrn_time_perd_id')\\n\\n#DQ19\\n\\nfrom pyspark.sql.functions import col, expr, regexp_replace, concat\\n\\n\\ndf_prod_sel = df_prod_sel.filter(expr(\\\"extrn_prod_id NOT LIKE '%PRODUCT%TAG%' \\\")).select('prod_lvl_name','long_prod_desc_txt','short_prod_desc_txt','extrn_prod_id','srce_sys_id','prod_prttn_code')\\n\\ndf_prod_sel = df_prod_sel.withColumn('combine_prod_desc', concat(col('short_prod_desc_txt'), col('long_prod_desc_txt')) ).withColumn('combine_prod_desc', regexp_replace('combine_prod_desc', ' ', '')).select('extrn_prod_id','combine_prod_desc','prod_lvl_name', 'short_prod_desc_txt', 'long_prod_desc_txt')\\n\\ndf_prod_dim_sel = df_prod_dim.filter(col('srce_sys_id')==srce_sys_id)\\ndf_prod_xref = df_prod_xref.filter(col('prod_prttn_code')== t2_cat_id)\\n\\ndf_prod_xref = df_prod_xref.withColumnRenamed('srce_sys_id', 'srce_sys_id1').withColumnRenamed('prod_skid', 'prod_skid1').withColumnRenamed('prod_prttn_code', 'prod_prttn_code1')\\n\\ndf_join = df_prod_dim_sel.join(df_prod_xref, ((df_prod_dim_sel.srce_sys_id == df_prod_xref.srce_sys_id1) & (df_prod_dim_sel.prod_skid == df_prod_xref.prod_skid1) & (df_prod_dim_sel.prod_prttn_code == df_prod_xref.prod_prttn_code1)  ) , 'inner').select('extrn_prod_id','prod_prttn_code','prod_skid','srce_sys_id','prod_lvl_name','short_prod_desc_txt', 'long_prod_desc_txt')\\n\\ndf_join = df_join.withColumn('old_combine_prod_desc', concat(col('short_prod_desc_txt'), col('long_prod_desc_txt')) ).withColumn('old_combine_prod_desc', regexp_replace('old_combine_prod_desc', ' ', '')).select('extrn_prod_id','old_combine_prod_desc','prod_lvl_name', 'short_prod_desc_txt','long_prod_desc_txt' )\\n\\ndf_join = df_join.withColumnRenamed('extrn_prod_id', 'old_extrn_prod_id').withColumnRenamed('prod_lvl_name', 'old_prod_lvl_name').withColumnRenamed('short_prod_desc_txt', 'old_short_prod_desc_txt').withColumnRenamed('long_prod_desc_txt', 'old_long_prod_desc_txt')\\n\\ndf_dq19 = df_prod_sel.join(df_join, ((df_prod_sel.extrn_prod_id==df_join.old_extrn_prod_id) & (~(df_prod_sel.combine_prod_desc==df_join.old_combine_prod_desc)) ), 'inner' )\\n\\ndf_dq19 = df_dq19.withColumnRenamed('extrn_prod_id', 'new_extrn_prod_id').withColumnRenamed('prod_lvl_name', 'new_prod_lvl_name').withColumnRenamed('combine_prod_desc', 'new_combine_prod_desc').withColumnRenamed('short_prod_desc_txt', 'new_short_prod_desc_txt').withColumnRenamed('long_prod_desc_txt', 'new_long_prod_desc_txt')\\n\\n##############\\n\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\nif chk_dq16:\\n  df_dq16_1 = df_dq16_1.withColumn('DQ16', lit('Missing/Delivered Areas')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq16_1 = df_dq16_1.select('row_id','DQ16',df_dq16_1.MKT_SKID.alias('MARKET_ID'), df_dq16_1.EXTRN_MKT_ID.alias('MARKET_NAME'), df_dq16_1.EXTRN_MKT_NAME.alias('SUPPLIER_TAG'), df_dq16_1.MKT_NAME.alias('SUPPLIER_DESCRIPTION'), df_dq16_1.LAST_RUN_STATUS.alias('STATUS_VS_LAST_LOADED_FILE'), df_dq16_1.LAST_PERIOD_STATUS.alias('STATUS_VS_LAST_TIME_PERIOD_IN_FACT_TABLE'))\\nelse:\\n  df_dq16_1 = df_empty\\n\\nif chk_dq17:\\n  df_dq17_1 = df_dq17_1.withColumn('DQ17', lit('Missing/Delivered Hierarchies')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq17_1 = df_dq17_1.select('row_id','DQ17',df_dq17_1.miss_prod_lvl_name.alias('PRODUCT_LEVEL_NAME'), df_dq17_1.miss_last_run_status.alias('STATUS_VS_LAST_LOAD_FILE'), df_dq17_1.miss_last_period_status.alias('STATUS_VS_LAST_TIME_PERD_IN_FACT_TABLE'))\\nelse:\\n  df_dq17_1 = df_empty\\n\\nif chk_dq18:\\n  df_dq_miss_time_pd = df_dq_miss_time_pd.withColumn('DQ18', lit('Unknown Time periods')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq_miss_time_pd = df_dq_miss_time_pd.select('row_id', 'DQ18', df_dq_miss_time_pd.extrn_time_perd_id.alias('EXTERNAL_TIME_PERIOD_TAG'))\\nelse:\\n  df_dq_miss_time_pd = df_empty\\n\\nif chk_dq19:\\n  df_dq19 = df_dq19.withColumn('DQ19', lit('Modified Product Description')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n  df_dq19 = df_dq19.select('row_id', 'DQ19', df_dq19.new_extrn_prod_id.alias('NEW_EXTERNAL_PRODUCT_IDENTIFIER'), df_dq19.new_prod_lvl_name.alias('NEW_PRODUCT_LEVEL_NAME'),  df_dq19.new_short_prod_desc_txt.alias('NEW_SHORT_PRODUCT_DESCRIPTION'), df_dq19.new_long_prod_desc_txt.alias('NEW_LONG_PRODUCT_DESCRIPTION'), df_dq19.old_extrn_prod_id.alias('OLD_EXTERNAL_PRODUCT_IDENTIFIER'), df_dq19.old_prod_lvl_name.alias('OLD_PRODUCT_LEVEL_NAME'), df_dq19.old_short_prod_desc_txt.alias('OLD_SHORT_PRODUCT_DESCRIPTION') , df_dq19.old_long_prod_desc_txt.alias('OLD_LONG_PRODUCT_DESCRIPTION') )\\nelse:\\n  df_dq19 = df_empty  \\n\\nref_data_elig = chk_dq16 | chk_dq17 | chk_dq18 | chk_dq19\\n\\nif ref_data_elig:\\n  df_combine = df_dq16_1.join(df_dq17_1,['row_id'] , 'full').join(df_dq_miss_time_pd,['row_id'] , 'full').join(df_dq19,['row_id'] , 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\nelse:\\n  df_empty\\n\\n\\n# df_combine = df_dq16_1.join(df_dq17_1,['row_id'] , 'full').join(df_dq_miss_time_pd,['row_id'] , 'full').join(df_dq19,['row_id'] , 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\n\\nfrom pyspark.sql.functions import when\\n\\n# KPI information\\ndq16_columns = ['DQ16','MARKET_ID','MARKET_NAME','SUPPLIER_TAG', 'SUPPLIER_DESCRIPTION','STATUS_VS_LAST_LOADED_FILE','STATUS_VS_LAST_TIME_PERIOD_IN_FACT_TABLE']\\ndq17_columns = ['DQ17','PRODUCT_LEVEL_NAME', 'STATUS_VS_LAST_LOAD_FILE','STATUS_VS_LAST_TIME_PERD_IN_FACT_TABLE']\\ndq18_columns = ['DQ18', 'EXTERNAL_TIME_PERIOD_TAG']\\ndq19_columns = [ 'DQ19','OLD_EXTERNAL_PRODUCT_IDENTIFIER','OLD_PRODUCT_LEVEL_NAME','OLD_SHORT_PRODUCT_DESCRIPTION' ,'OLD_LONG_PRODUCT_DESCRIPTION', 'NEW_EXTERNAL_PRODUCT_IDENTIFIER', 'NEW_PRODUCT_LEVEL_NAME', 'NEW_SHORT_PRODUCT_DESCRIPTION', 'NEW_LONG_PRODUCT_DESCRIPTION' ]\\n\\n\\ncombined_cols = ['row_id']\\ndata = []\\nif chk_dq16:\\n  [combined_cols.append(i) for i in dq16_columns]\\n  dq16_val = ('MARKET_ID', 'SQL Validation KPI', \\\"MARKET_ID IS NULL\\\", '', 'false', 'Missing/Delivered Areas', 100 )\\n  data.append(dq16_val)\\nif chk_dq17:\\n  [combined_cols.append(i) for i in dq17_columns]\\n  dq17_val = ('PRODUCT_LEVEL_NAME', 'SQL Validation KPI', \\\"PRODUCT_LEVEL_NAME IS NULL\\\", '', 'false', 'Missing/Delivered Hierarchies', 100 )\\n  data.append(dq17_val)\\nif chk_dq18:\\n  [combined_cols.append(i) for i in dq18_columns]\\n  dq18_val = ('EXTERNAL_TIME_PERIOD_TAG', 'SQL Validation KPI', \\\"EXTERNAL_TIME_PERIOD_TAG IS NULL\\\", '', 'false', 'Unknown Time periods', 100 )\\n  data.append(dq18_val)\\nif chk_dq19:\\n  [combined_cols.append(i) for i in dq19_columns]\\n  dq19_val = ('NEW_EXTERNAL_PRODUCT_IDENTIFIER', 'SQL Validation KPI', \\\"NEW_EXTERNAL_PRODUCT_IDENTIFIER IS NULL\\\", '', 'false', 'Modified Product Description', 100 )\\n  data.append(dq19_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\nif ref_data_elig:\\n  df_ref_data = spark.createDataFrame(data, schema_for_kpi)\\nelse:\\n  df_empty\\n\\n\\n\\n#Reference Data Check Eligibility\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\ndata2 = [(ref_data_elig,\\\"false\\\")\\n  ]\\n\\nschema = StructType([ \\n    StructField(\\\"reference_data\\\",BooleanType(),True),\\n    StructField(\\\"reference_data_elig\\\",StringType(),True)\\n  ])\\n \\ndf_ref_data_eligibility = spark.createDataFrame(data=data2,schema=schema)\\ndf_ref_data_eligibility = df_ref_data_eligibility.withColumn('reference_data_elig', when(col('reference_data'), lit('true')).otherwise(lit('false')))\\n\\n\\ndict_all_dfs['df_combine_ref_data'] = {\\\"df_object\\\" :df_combine}\\ndict_all_dfs['df_ref_data'] = {\\\"df_object\\\" :df_ref_data}\\ndict_all_dfs['df_ref_data_eligibility'] = {\\\"df_object\\\" :df_ref_data_eligibility}\\n\\ndf_output_dict['df_combine_ref_data'] = df_combine\\ndf_output_dict['df_ref_data'] = df_ref_data\\ndf_output_dict['df_ref_data_eligibility'] = df_ref_data_eligibility\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier2_mkt_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier2_fact_stgng\"\n    },\n    {\n      \"name\": \"df_fact_raw\"\n    },\n    {\n      \"name\": \"df_mkt_xref\"\n    },\n    {\n      \"name\": \"df_prod_xref\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_ref_data\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_ref_data_eligibility\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "reference data KPI",
      "predecessorName": "Reference data chekcs v2",
      "jsonSpecification": "{\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_ref_data.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_ref_data\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": false
    },
    {
      "operationName": "[GEN] - Update delivery lookup",
      "predecessorName": "reference data KPI",
      "jsonSpecification": "{\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\n\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\nrows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 3, 1, 4, <<PROCESS_RUN_KEY>>]]\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"parquet\\\").mode(\\\"append\\\").save(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-refined/MM_DLVRY_RUN_LKP\\\")\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_dummy\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "DQ  Reference Data",
      "predecessorName": "[GEN] - Update delivery lookup",
      "jsonSpecification": "{\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_ref_data.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_chk\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference data - Report",
      "predecessorName": "DQ  Reference Data",
      "jsonSpecification": "{\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"saveToCSV\": \"true\",\n  \"generateHTMLReport\": \"true\",\n  \"generatePDFReport\": \"false\",\n  \"includeDetailedValidationResults\": \"failed rows only\",\n  \"numberOfRowsToDisplay\": 100,\n  \"reportTemplate\": \"default\"\n}",
      "operationVersionName": "DataQualityReport",
      "overridableIndicator": false
    },
    {
      "operationName": "Stop Calc",
      "predecessorName": "Reference data - Report",
      "jsonSpecification": "{\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf = dict_all_dfs['df_chk'][\\\"df_object\\\"]\\n\\nresult = df.columns[4]\\ndf = df.withColumnRenamed(result, 'result')\\n\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\n\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\n\\ncnt = df.filter(\\\"result = 'Fail' \\\" ).count()\\n\\nif (cnt>0):\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 11, 1, 4, <<PROCESS_RUN_KEY>>]]\\nelse:\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 4, 1, 3, <<PROCESS_RUN_KEY>>]]\\n\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"parquet\\\").mode(\\\"append\\\").save(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-refined/MM_DLVRY_RUN_LKP\\\")\\n\\ndict_all_dfs['df_chk'] = {\\\"df_object\\\" :df}\\ndf_output_dict['df_chk'] = df\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_chk\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_chk\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Conditional Stop Calc",
      "predecessorName": "Stop Calc",
      "jsonSpecification": "{\n  \"expression\": \"(result = 'Fail')\",\n  \"processStatus\": \"DQ_ISSUE\",\n  \"conditionValue\": \"false\",\n  \"milestone\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_chk\"\n    }\n  ]\n}",
      "operationVersionName": "ConditionalStop",
      "overridableIndicator": false
    }
  ],
  "graphName": "cdl_t2_reference_data_v2"
}