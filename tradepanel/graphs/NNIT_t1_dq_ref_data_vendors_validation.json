{
  "applicationName": "TURBINE_INTERNAL",
  "nodes": [
    {
      "operationName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"manualSchema\": \"true\",\n  \"transformations\": [\n    {\n      \"columnType\": \"string\",\n      \"columnName\": \"test\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_dummy\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "CreateSchema",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference Data Vendors - Validations",
      "predecessorName": "dummy",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = 1\\ntier1_srce_sys_id = 3\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = 165761\\ntier1_cntry_id = 'PL'\\n\\npath = 'bf/unrefined/cloudpanel-test/cloudpanel-test-unref/Tier1/5000/'\\n\\n# Dataframes from Prior Steps\\n\\n\\ntier1_prod_mtrlz_tbl = dict_all_dfs['tier1_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_fact_mtrlz_tbl = dict_all_dfs['tier1_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_prod_gav = dict_all_dfs['tier1_prod_gav'][\\\"df_object\\\"]\\n\\ntier1_prod_mtrlz_tbl.createOrReplaceTempView('tier1_prod_mtrlz_tbl')\\ntier1_fact_mtrlz_tbl.createOrReplaceTempView('tier1_fact_mtrlz_tbl')\\ntier1_prod_gav.createOrReplaceTempView('tier1_prod_gav')\\n\\nmm_prod_xref = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_XREF/')\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\n\\nmm_prod_sdim = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_SDIM_VW/')\\nmm_prod_sdim.createOrReplaceTempView('mm_prod_sdim')\\n\\n\\n# tables from Postgres\\n\\n\\n\\n# mm_strct_lkp\\nmm_strct_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LKP\\\")\\nmm_strct_lkp.createOrReplaceTempView('mm_strct_lkp')\\n\\n# mm_strct_lvl_lkp\\nmm_strct_lvl_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LVL_LKP\\\")\\nmm_strct_lvl_lkp.createOrReplaceTempView('mm_strct_lvl_lkp')\\n\\n# mm_categ_strct_assoc\\nmm_categ_strct_assoc = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ASSOC\\\")\\nmm_categ_strct_assoc.createOrReplaceTempView('mm_categ_strct_assoc')\\n\\n# mm_categ_strct_attr_assoc_vw\\nmm_categ_strct_attr_assoc_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ATTR_ASSOC_VW\\\")\\nmm_categ_strct_attr_assoc_vw.createOrReplaceTempView('mm_categ_strct_attr_assoc_vw')\\n\\n#UDFs\\n\\n# Ignore Bad ASCII Symbols\\nspark.sql('''create or replace TEMPORARY FUNCTION FN_GET_MAX_LVL_NUM (\\nstrct_id int ) RETURNs int deterministic RETURN \\nSELECT MAX (lvl_num)\\n                              FROM mm_strct_lvl_lkp lvl\\n                             WHERE lvl.strct_id = strct_id''')\\n\\n\\n#Validations\\n\\n#Unknown hierarchy for product\\n\\nquery1 = f\\\"\\\"\\\"select tier1_prod_mtrlz_tbl.line_num as dq1_line_num, tier1_prod_mtrlz_tbl.extrn_code as dq1_extrn_code, tier1_prod_mtrlz_tbl.extrn_name as dq1_extrn_name, tier1_prod_mtrlz_tbl.attr_code_list as dq1_attr_code_list, tier1_prod_mtrlz_tbl.attr_code_0 as dq1_invalid_hier_num\\n  from tier1_prod_mtrlz_tbl \\n  left join mm_categ_strct_assoc on tier1_prod_mtrlz_tbl.attr_code_1=mm_categ_strct_assoc.categ_id \\n  and tier1_prod_mtrlz_tbl.attr_code_0 =  mm_categ_strct_assoc.strct_num\\n  where (mm_categ_strct_assoc.strct_num is null and strct_id is null)  \\n  order by line_num \\n  limit 100\\\"\\\"\\\"\\ndf_unk_hier_prod = spark.sql(query1)\\n\\n# Too many attributes for product hierarchy\\nquery2 = f\\\"\\\"\\\"WITH prod AS (SELECT line_num, extrn_name, extrn_code, attr_code_list, lvl_num, attr_code_0, attr_Code_1 FROM tier1_prod_mtrlz_tbl)\\nSELECT attr_code_1 as dq2_attr_code_1, lvl_num as dq2_lvl_num,attr_code_list as dq2_attr_code_list,line_num as dq2_line_num, extrn_code as dq2_extrn_code, extrn_name as dq2_extrn_name, strct_id as dq2_strct_id, dmnsn_id as dq2_dmnsn_id, strct_code as dq2_strct_code, strct_name as dq2_strct_name, strct_desc as dq2_strct_desc   FROM (SELECT *\\n  FROM prod JOIN mm_strct_lkp strct\\n\\t\\t\\t\\tON prod.attr_Code_1||'_H'||prod.attr_code_0 = strct.strct_code and strct.dmnsn_id=2\\n\\t\\t\\t\\t\\t AND prod.lvl_num > FN_GET_MAX_LVL_NUM(strct.strct_id))\\n  ORDER BY dq2_line_num\\n  LIMIT 100\\\"\\\"\\\"\\n\\ndf_attr_prod_hier = spark.sql(query2)\\n\\n# No parent in product hierarchy\\nquery3 = f\\\"\\\"\\\"WITH src0 AS (\\nSELECT prod.ATTR_CODE_2, prod.ATTR_CODE_LIST, prod.prod_skid, prod.extrn_name, fct.prod_extrn_code, fct.time_extrn_code, fct.MKT_EXTRN_CODE\\nFROM tier1_fact_mtrlz_tbl fct JOIN tier1_prod_gav prod\\n  ON fct.prod_extrn_code = prod.extrn_code),\\nsrc_dis AS (\\nSELECT DISTINCT mkt_extrn_code, prod_extrn_code, time_extrn_code, prod_skid, extrn_name, src0.ATTR_CODE_2, src0.ATTR_CODE_LIST FROM src0 \\n),\\nsrc_parent AS (\\nSELECT  mkt_extrn_code, time_extrn_code, prod_extrn_code, src_dis.prod_skid, src_dis.extrn_name, src_dis.ATTR_CODE_2, src_dis.ATTR_CODE_LIST, \\nsubstr(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\s)'),1, LENGTH(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\s)'))-1) AS PARENT_ATTR_CODE_LIST\\nFROM src_dis \\n)\\nSELECT src_parent.mkt_extrn_code as dq3_mkt_extrn_code, src_parent.time_extrn_code as dq3_time_extrn_code, src_parent.prod_extrn_code as dq3_prod_extrn_code, src_parent.ATTR_CODE_LIST as dq3_attr_code_list,\\nsrc_parent.prod_skid as dq3_prod_skid, src_parent.extrn_name as dq3_extrn_name\\nFROM src_parent LEFT JOIN src_dis \\nON PARENT_ATTR_CODE_LIST = src_dis.ATTR_CODE_LIST\\nAND src_parent.mkt_extrn_code = src_dis.mkt_extrn_code\\nAND src_parent.time_extrn_code = src_dis.time_extrn_code\\nWHERE src_dis.MKT_EXTRN_CODE IS NULL\\nAND src_parent.ATTR_CODE_2 IS NOT NULL LIMIT 199\\\"\\\"\\\"\\ndf_nopr_prod_hier = spark.sql(query3)\\n\\n# Duplicated product after mapping\\n\\nquery4 = f\\\"\\\"\\\"WITH data AS (\\nSELECT src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST <> xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST AND src.extrn_code <> xref.extrn_prod_id\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\n),\\nskid AS (\\nSELECT prod_skid FROM data \\nGROUP BY prod_skid\\nHAVING COUNT(DISTINCT line_num)>1\\n)\\nSELECT line_num as dq4_line_num, vendor_tag as dq4_vendor_tag, prod_codes_new as dq4_prod_codes_new, prod_skid as dq4_prod_skid, prod_name as dq4_prod_name FROM (SELECT DISTINCT line_num, extrn_code vendor_tag, EXTRN_PROD_ATTR_VAL_LIST prod_codes_new, data.PROD_SKID, PROD_NAME FROM data\\n   JOIN skid on data.prod_skid = skid.prod_skid) ORDER BY dq4_line_num LIMIT 500\\\"\\\"\\\"\\ndf_dup_prod_map = spark.sql(query4)\\n\\n# External code change\\n\\nquery5 = f\\\"\\\"\\\"WITH src AS (SELECT * FROM tier1_prod_mtrlz_tbl),\\n  lvl AS (SELECT * FROM mm_categ_strct_attr_assoc_vw),\\n  xref AS (SELECT  extrn_prod_id, EXTRN_PROD_ATTR_VAL_LIST, prod_skid \\n\\t         FROM mm_prod_xref WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id}),\\n  sd AS (SELECT  prod_skid, prod_lvl_name, prod_lvl_id \\n\\t       from mm_prod_sdim WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id})\\nSELECT extrn_prod_id as dq5_extrn_prod_id, vendor_tag_old as dq5_vendor_tag_old, prod_codes_new as dq5_prod_codes_new, attribute_level_new as dq5_attribute_level_new, prod_codes_old as dq5_prod_codes_old, attribute_level_old as dq5_attribute_level_old FROM (SELECT \\n   xref.extrn_prod_id,\\n   src.extrn_prod_id AS vendor_tag_old,\\n       src.EXTRN_PROD_ATTR_VAL_LIST prod_codes_new,\\n       lvl.attr_name attribute_level_new,\\n       xref.EXTRN_PROD_ATTR_VAL_LIST prod_codes_old,\\n       sd.prod_lvl_name attribute_level_old\\n    FROM src JOIN lvl\\n          ON (    src.ATTR_CODE_1 = lvl.CATEG_ID\\n             AND src.ATTR_CODE_0 = CAST (lvl.STRCT_NUM as string)\\n             AND src.LVL_NUM = lvl.LVL_NUM\\n          )\\n       JOIN xref ON xref.extrn_prod_id = src.extrn_prod_id \\n       JOIN sd ON sd.prod_skid = xref.prod_skid\\n       JOIN MM_STRCT_LVL_LKP lvl_dim ON sd.prod_lvl_id= lvl_dim.strct_lvl_id\\n     WHERE     src.EXTRN_PROD_ATTR_VAL_LIST != xref.EXTRN_PROD_ATTR_VAL_LIST\\n       AND lvl.lvl_num != lvl_dim.lvl_num) LIMIT 500\\\"\\\"\\\"\\ndf_ext_code_chng = spark.sql(query5)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\ncolumns = StructType([StructField('row_id', IntegerType(), True)])\\ndf_empty = spark.createDataFrame(data=[], schema=columns)\\n\\ndf_unk_hier_prod = df_unk_hier_prod.withColumn('DQ1', lit('Unknown hierarchy for product')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ndf_attr_prod_hier = df_attr_prod_hier.withColumn('DQ2', lit('Too many attributes for product hierarchy')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ndf_nopr_prod_hier = df_nopr_prod_hier.withColumn('DQ3', lit('No parent in product hierarchy')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ndf_dup_prod_map = df_dup_prod_map.withColumn('DQ4', lit('Duplicated product after mapping')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\ndf_ext_code_chng = df_ext_code_chng.withColumn('DQ5', lit('External code change')).withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id())))\\n\\n# Combine the dataframes\\ndf_combine = df_unk_hier_prod.join(df_attr_prod_hier,['row_id'] , 'full').join(df_nopr_prod_hier,['row_id'] , 'full').join(df_dup_prod_map,['row_id'] , 'full').join(df_ext_code_chng,['row_id'] , 'full').drop('row_id').withColumn(\\\"row_id\\\",row_number().over(Window.orderBy(monotonically_increasing_id()))).orderBy(col('row_id'))\\n\\n# KPI information\\ndq1_columns = ['DQ1','dq1_line_num','dq1_extrn_code','dq1_extrn_name','dq1_attr_code_list','dq1_invalid_hier_num']\\ndq2_columns = ['DQ2','dq2_attr_code_1','dq2_lvl_num','dq2_attr_code_list','dq2_line_num','dq2_extrn_code','dq2_extrn_name','dq2_strct_id','dq2_dmnsn_id','dq2_strct_code','dq2_strct_name','dq2_strct_desc']\\ndq3_columns = ['DQ3','dq3_mkt_extrn_code','dq3_time_extrn_code','dq3_prod_extrn_code','dq3_attr_code_list','dq3_prod_skid','dq3_extrn_name']\\ndq4_columns = ['DQ4','dq4_line_num','dq4_vendor_tag','dq4_prod_codes_new','dq4_prod_skid','dq4_prod_name']\\ndq5_columns = ['DQ5','dq5_extrn_prod_id','dq5_vendor_tag_old','dq5_prod_codes_new','dq5_attribute_level_new','dq5_prod_codes_old','dq5_attribute_level_old']\\n\\ncombined_cols = ['row_id']\\ndata = []\\n[combined_cols.append(i) for i in dq1_columns]\\n[combined_cols.append(i) for i in dq2_columns]\\n[combined_cols.append(i) for i in dq3_columns]\\n[combined_cols.append(i) for i in dq4_columns]\\n[combined_cols.append(i) for i in dq5_columns]\\n\\n#CASE WHEN ret.NR = 0 THEN 'PASSED' ELSE 'FAILED' END AS RESULT\\n\\ndq1_val = ('dq1_line_num', 'SQL Validation KPI', \\\"dq1_line_num IS NULL\\\", '', 'false', 'Unknown hierarchy for product', 100 )\\ndata.append(dq1_val)\\ndq2_val = ('dq2_line_num', 'SQL Validation KPI', \\\"dq2_line_num IS NULL\\\", '', 'false', 'Too many attributes for product hierarchy', 100 )\\ndata.append(dq2_val)\\ndq3_val = ('dq3_mkt_extrn_code', 'SQL Validation KPI', \\\"dq3_mkt_extrn_code IS NULL\\\", '', 'false', 'No parent in product hierarchy', 100 )\\ndata.append(dq3_val)\\ndq4_val = ('dq4_line_num', 'SQL Validation KPI', \\\"dq4_line_num IS NULL\\\", '', 'false', 'Duplicated product after mapping', 100 )\\ndata.append(dq4_val)\\ndq5_val = ('dq5_extrn_prod_id', 'SQL Validation KPI', \\\"dq5_extrn_prod_id IS NULL\\\", '', 'false', 'External code change', 100 )\\ndata.append(dq5_val)\\n\\ndf_combine = df_combine.select(*combined_cols)\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\ndf_ref_data_vendors = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\ndict_all_dfs['df_ref_data_vendors'] = {\\\"df_object\\\" :df_ref_data_vendors}\\ndf_output_dict['df_ref_data_vendors'] = df_ref_data_vendors\\n\\ndict_all_dfs['df_combine_ref_data_vendors'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_ref_data_vendors'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier1_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_prod_gav\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference Data Vendors - Validations - Update",
      "predecessorName": "Reference Data Vendors - Validations",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = <<VENDOR_ID>>\\ntier1_srce_sys_id = <<SRCE_SYS_ID>>\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = <<CNTRT_ID>>\\ntier1_cntry_id = '<<ISO_CRNCY_CODE>>'\\n\\npath = 'refined/NNIT/tradepanel/prod-tp-lightrefined/'\\n\\n# Dataframes from Prior Steps\\n\\n\\ntier1_prod_mtrlz_tbl = dict_all_dfs['tier1_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_fact_mtrlz_tbl = dict_all_dfs['tier1_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_prod_gav = dict_all_dfs['tier1_prod_gav'][\\\"df_object\\\"]\\n\\ntier1_prod_mtrlz_tbl.createOrReplaceTempView('tier1_prod_mtrlz_tbl')\\ntier1_fact_mtrlz_tbl.createOrReplaceTempView('tier1_fact_mtrlz_tbl')\\ntier1_prod_gav.createOrReplaceTempView('tier1_prod_gav')\\n\\nmm_prod_xref = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_XREF/')\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\n\\nmm_prod_sdim = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_SDIM_VW/')\\nmm_prod_sdim.createOrReplaceTempView('mm_prod_sdim')\\n\\n\\n# tables from Postgres\\n\\n\\n\\n# mm_strct_lkp\\nmm_strct_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LKP\\\")\\nmm_strct_lkp.createOrReplaceTempView('mm_strct_lkp')\\n\\n# mm_strct_lvl_lkp\\nmm_strct_lvl_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LVL_LKP\\\")\\nmm_strct_lvl_lkp.createOrReplaceTempView('mm_strct_lvl_lkp')\\n\\n# mm_categ_strct_assoc\\nmm_categ_strct_assoc = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ASSOC\\\")\\nmm_categ_strct_assoc.createOrReplaceTempView('mm_categ_strct_assoc')\\n\\n# mm_categ_strct_attr_assoc_vw\\nmm_categ_strct_attr_assoc_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ATTR_ASSOC_VW\\\")\\nmm_categ_strct_attr_assoc_vw.createOrReplaceTempView('mm_categ_strct_attr_assoc_vw')\\n\\n#UDFs\\n\\n# Ignore Bad ASCII Symbols\\nspark.sql('''create or replace TEMPORARY FUNCTION FN_GET_MAX_LVL_NUM (\\nstrct_id int ) RETURNs int deterministic RETURN \\nSELECT MAX (lvl_num)\\n                              FROM mm_strct_lvl_lkp lvl\\n                             WHERE lvl.strct_id = strct_id''')\\n\\n\\n#Validations\\n\\n#Unknown hierarchy for product\\n\\nquery1 = f\\\"\\\"\\\"select (1) DQ, tier1_prod_mtrlz_tbl.line_num as line_num, tier1_prod_mtrlz_tbl.extrn_code as extrn_code, tier1_prod_mtrlz_tbl.extrn_name as extrn_name, tier1_prod_mtrlz_tbl.attr_code_list as attr_code_list, tier1_prod_mtrlz_tbl.attr_code_0 as invalid_hier_num\\n  from tier1_prod_mtrlz_tbl \\n  left join mm_categ_strct_assoc on tier1_prod_mtrlz_tbl.attr_code_1=mm_categ_strct_assoc.categ_id \\n  and tier1_prod_mtrlz_tbl.attr_code_0 =  mm_categ_strct_assoc.strct_num\\n  where (mm_categ_strct_assoc.strct_num is null and strct_id is null)  \\n  order by line_num \\n  limit 100\\\"\\\"\\\"\\ndf_unk_hier_prod = spark.sql(query1)\\n\\n# Too many attributes for product hierarchy\\nquery2 = f\\\"\\\"\\\"WITH prod AS (SELECT line_num, extrn_name, extrn_code, attr_code_list, lvl_num, attr_code_0, attr_Code_1 FROM tier1_prod_mtrlz_tbl)\\nSELECT (2) DQ, attr_code_1 as attr_code_1, lvl_num as lvl_num,attr_code_list as attr_code_list,line_num as line_num, extrn_code as extrn_code, extrn_name as extrn_name, strct_id as strct_id, dmnsn_id as dmnsn_id, strct_code as strct_code, strct_name as strct_name, strct_desc as strct_desc   FROM (SELECT *\\n  FROM prod JOIN mm_strct_lkp strct\\n\\t\\t\\t\\tON prod.attr_Code_1||'_H'||prod.attr_code_0 = strct.strct_code and strct.dmnsn_id=2\\n\\t\\t\\t\\t\\t AND prod.lvl_num > FN_GET_MAX_LVL_NUM(strct.strct_id))\\n  ORDER BY line_num\\n  LIMIT 100\\\"\\\"\\\"\\n\\ndf_attr_prod_hier = spark.sql(query2)\\n\\n# No parent in product hierarchy\\nquery3 = f\\\"\\\"\\\"WITH src0 AS (\\nSELECT prod.ATTR_CODE_2, prod.ATTR_CODE_LIST, prod.prod_skid, prod.extrn_name, fct.prod_extrn_code, fct.time_extrn_code, fct.MKT_EXTRN_CODE\\nFROM tier1_fact_mtrlz_tbl fct JOIN tier1_prod_gav prod\\n  ON fct.prod_extrn_code = prod.extrn_code),\\nsrc_dis AS (\\nSELECT DISTINCT mkt_extrn_code, prod_extrn_code, time_extrn_code, prod_skid, extrn_name, src0.ATTR_CODE_2, src0.ATTR_CODE_LIST FROM src0 \\n),\\nsrc_parent AS (\\nSELECT  mkt_extrn_code, time_extrn_code, prod_extrn_code, src_dis.prod_skid, src_dis.extrn_name, src_dis.ATTR_CODE_2, src_dis.ATTR_CODE_LIST, \\nsubstr(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\s)'),1, LENGTH(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\s)'))-1) AS PARENT_ATTR_CODE_LIST\\nFROM src_dis \\n)\\nSELECT (3) DQ,src_parent.mkt_extrn_code as mkt_extrn_code, src_parent.time_extrn_code as time_extrn_code, src_parent.prod_extrn_code as prod_extrn_code, src_parent.ATTR_CODE_LIST as attr_code_list,\\nsrc_parent.prod_skid as prod_skid, src_parent.extrn_name as extrn_name\\nFROM src_parent LEFT JOIN src_dis \\nON PARENT_ATTR_CODE_LIST = src_dis.ATTR_CODE_LIST\\nAND src_parent.mkt_extrn_code = src_dis.mkt_extrn_code\\nAND src_parent.time_extrn_code = src_dis.time_extrn_code\\nWHERE src_dis.MKT_EXTRN_CODE IS NULL\\nAND src_parent.ATTR_CODE_2 IS NOT NULL LIMIT 199\\\"\\\"\\\"\\ndf_nopr_prod_hier = spark.sql(query3)\\n\\n# Duplicated product after mapping\\n\\nquery4 = f\\\"\\\"\\\"WITH data AS (\\nSELECT src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST <> xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST AND src.extrn_code <> xref.extrn_prod_id\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\n),\\nskid AS (\\nSELECT prod_skid FROM data \\nGROUP BY prod_skid\\nHAVING COUNT(DISTINCT line_num)>1\\n)\\nSELECT (4) DQ,line_num as line_num, vendor_tag as vendor_tag, prod_codes_new as prod_codes_new, prod_skid as prod_skid, prod_name as prod_name FROM (SELECT DISTINCT line_num, extrn_code vendor_tag, EXTRN_PROD_ATTR_VAL_LIST prod_codes_new, data.PROD_SKID, PROD_NAME FROM data\\n   JOIN skid on data.prod_skid = skid.prod_skid) ORDER BY line_num LIMIT 500\\\"\\\"\\\"\\ndf_dup_prod_map = spark.sql(query4)\\n\\n# External code change\\n\\nquery5 = f\\\"\\\"\\\"WITH src AS (SELECT * FROM tier1_prod_mtrlz_tbl),\\n  lvl AS (SELECT * FROM mm_categ_strct_attr_assoc_vw),\\n  xref AS (SELECT  extrn_prod_id, EXTRN_PROD_ATTR_VAL_LIST, prod_skid \\n\\t         FROM mm_prod_xref WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id}),\\n  sd AS (SELECT  prod_skid, prod_lvl_name, prod_lvl_id \\n\\t       from mm_prod_sdim WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id})\\nSELECT (5) DQ,extrn_prod_id as extrn_prod_id, vendor_tag_old as vendor_tag_old, prod_codes_new as prod_codes_new, attribute_level_new as attribute_level_new, prod_codes_old as prod_codes_old, attribute_level_old as attribute_level_old FROM (SELECT \\n   xref.extrn_prod_id,\\n   src.extrn_prod_id AS vendor_tag_old,\\n       src.EXTRN_PROD_ATTR_VAL_LIST prod_codes_new,\\n       lvl.attr_name attribute_level_new,\\n       xref.EXTRN_PROD_ATTR_VAL_LIST prod_codes_old,\\n       sd.prod_lvl_name attribute_level_old\\n    FROM src JOIN lvl\\n          ON (    src.ATTR_CODE_1 = lvl.CATEG_ID\\n             AND src.ATTR_CODE_0 = CAST (lvl.STRCT_NUM as string)\\n             AND src.LVL_NUM = lvl.LVL_NUM\\n          )\\n       JOIN xref ON xref.extrn_prod_id = src.extrn_prod_id \\n       JOIN sd ON sd.prod_skid = xref.prod_skid\\n       JOIN MM_STRCT_LVL_LKP lvl_dim ON sd.prod_lvl_id= lvl_dim.strct_lvl_id\\n     WHERE     src.EXTRN_PROD_ATTR_VAL_LIST != xref.EXTRN_PROD_ATTR_VAL_LIST\\n       AND lvl.lvl_num != lvl_dim.lvl_num) LIMIT 500\\\"\\\"\\\"\\ndf_ext_code_chng = spark.sql(query5)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\n\\ndq1 = df_unk_hier_prod\\ndq2 = df_attr_prod_hier\\ndq3 = df_nopr_prod_hier\\ndq4 = df_dup_prod_map\\ndq5 = df_ext_code_chng\\n\\n# Combine the dataframes\\ndf_combine = dq1.unionByName(dq2, True).unionByName(dq3, True).unionByName(dq4, True).unionByName(dq4, True).unionByName(dq5, True)\\n\\ndata = []\\n\\ndq1_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 1 \\\", '', 'false', 'Unknown hierarchy for product', 100 )\\ndata.append(dq1_val)\\ndq2_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 2 \\\", '', 'false', 'Too many attributes for product hierarchy', 100 )\\ndata.append(dq2_val)\\ndq3_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 3 \\\", '', 'false', 'No parent in product hierarchy', 100 )\\ndata.append(dq3_val)\\ndq4_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 4 \\\", '', 'false', 'Duplicated product after mapping', 100 )\\ndata.append(dq4_val)\\ndq5_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 5 \\\", '', 'false', 'External code change', 100 )\\ndata.append(dq5_val)\\n\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\ndf_ref_data_vendors = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\ndict_all_dfs['df_ref_data_vendors'] = {\\\"df_object\\\" :df_ref_data_vendors}\\ndf_output_dict['df_ref_data_vendors'] = df_ref_data_vendors\\n\\ndict_all_dfs['df_combine_ref_data_vendors'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_ref_data_vendors'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier1_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_prod_gav\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference Data Vendors - Validations - Update 1",
      "predecessorName": "Reference Data Vendors - Validations - Update",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = <<VENDOR_ID>>\\ntier1_srce_sys_id = <<SRCE_SYS_ID>>\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = <<CNTRT_ID>>\\ntier1_cntry_id = '<<ISO_CRNCY_CODE>>'\\n\\npath = 'refined/NNIT/tradepanel/prod-tp-lightrefined/'\\n\\n# Dataframes from Prior Steps\\n\\n\\ntier1_prod_mtrlz_tbl = dict_all_dfs['tier1_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_fact_mtrlz_tbl = dict_all_dfs['tier1_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_prod_gav = dict_all_dfs['tier1_prod_gav'][\\\"df_object\\\"]\\n\\ntier1_prod_mtrlz_tbl.createOrReplaceTempView('tier1_prod_mtrlz_tbl')\\ntier1_fact_mtrlz_tbl.createOrReplaceTempView('tier1_fact_mtrlz_tbl')\\ntier1_prod_gav.createOrReplaceTempView('tier1_prod_gav')\\n\\nmm_prod_xref = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_XREF/')\\nmm_prod_xref.createOrReplaceTempView('mm_prod_xref')\\n\\nmm_prod_sdim = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_SDIM_VW/')\\nmm_prod_sdim.createOrReplaceTempView('mm_prod_sdim')\\n\\n\\n# tables from Postgres\\n\\n\\n\\n# mm_strct_lkp\\nmm_strct_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LKP\\\")\\nmm_strct_lkp.createOrReplaceTempView('mm_strct_lkp')\\n\\n# mm_strct_lvl_lkp\\nmm_strct_lvl_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LVL_LKP\\\")\\nmm_strct_lvl_lkp.createOrReplaceTempView('mm_strct_lvl_lkp')\\n\\n# mm_categ_strct_assoc\\nmm_categ_strct_assoc = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ASSOC\\\")\\nmm_categ_strct_assoc.createOrReplaceTempView('mm_categ_strct_assoc')\\n\\n# mm_categ_strct_attr_assoc_vw\\nmm_categ_strct_attr_assoc_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ATTR_ASSOC_VW\\\")\\nmm_categ_strct_attr_assoc_vw.createOrReplaceTempView('mm_categ_strct_attr_assoc_vw')\\n\\n#UDFs\\n\\n# Ignore Bad ASCII Symbols\\nspark.sql('''create or replace TEMPORARY FUNCTION FN_GET_MAX_LVL_NUM (\\nstrct_id int ) RETURNs int deterministic RETURN \\nSELECT MAX (lvl_num)\\n                              FROM mm_strct_lvl_lkp lvl\\n                             WHERE lvl.strct_id = strct_id''')\\n\\n\\n#Validations\\n\\n#Unknown hierarchy for product\\n\\nquery1 = f\\\"\\\"\\\"select ('Unknown hierarchy for product') DQ, tier1_prod_mtrlz_tbl.line_num as line_num, tier1_prod_mtrlz_tbl.extrn_code as extrn_code, tier1_prod_mtrlz_tbl.extrn_name as extrn_name, tier1_prod_mtrlz_tbl.attr_code_list as attr_code_list, tier1_prod_mtrlz_tbl.attr_code_0 as invalid_hier_num\\n  from tier1_prod_mtrlz_tbl \\n  left join mm_categ_strct_assoc on tier1_prod_mtrlz_tbl.attr_code_1=mm_categ_strct_assoc.categ_id \\n  and tier1_prod_mtrlz_tbl.attr_code_0 =  mm_categ_strct_assoc.strct_num\\n  where (mm_categ_strct_assoc.strct_num is null and strct_id is null)  \\n  order by line_num \\n  limit 100\\\"\\\"\\\"\\ndf_unk_hier_prod = spark.sql(query1)\\n\\n# Too many attributes for product hierarchy\\nquery2 = f\\\"\\\"\\\"WITH prod AS (SELECT line_num, extrn_name, extrn_code, attr_code_list, lvl_num, attr_code_0, attr_Code_1 FROM tier1_prod_mtrlz_tbl)\\nSELECT ('Too many attributes for product hierarchy') DQ, attr_code_1 as attr_code_1, lvl_num as lvl_num,attr_code_list as attr_code_list,line_num as line_num, extrn_code as extrn_code, extrn_name as extrn_name, strct_id as strct_id, dmnsn_id as dmnsn_id, strct_code as strct_code, strct_name as strct_name, strct_desc as strct_desc   FROM (SELECT *\\n  FROM prod JOIN mm_strct_lkp strct\\n\\t\\t\\t\\tON prod.attr_Code_1||'_H'||prod.attr_code_0 = strct.strct_code and strct.dmnsn_id=2\\n\\t\\t\\t\\t\\t AND prod.lvl_num > FN_GET_MAX_LVL_NUM(strct.strct_id))\\n  ORDER BY line_num\\n  LIMIT 100\\\"\\\"\\\"\\n\\ndf_attr_prod_hier = spark.sql(query2)\\n\\n# No parent in product hierarchy\\nquery3 = f\\\"\\\"\\\"WITH src0 AS (\\nSELECT prod.ATTR_CODE_2, prod.ATTR_CODE_LIST, prod.prod_skid, prod.extrn_name, fct.prod_extrn_code, fct.time_extrn_code, fct.MKT_EXTRN_CODE\\nFROM tier1_fact_mtrlz_tbl fct JOIN tier1_prod_gav prod\\n  ON fct.prod_extrn_code = prod.extrn_code),\\nsrc_dis AS (\\nSELECT DISTINCT mkt_extrn_code, prod_extrn_code, time_extrn_code, prod_skid, extrn_name, src0.ATTR_CODE_2, src0.ATTR_CODE_LIST FROM src0 \\n),\\nsrc_parent AS (\\nSELECT  mkt_extrn_code, time_extrn_code, prod_extrn_code, src_dis.prod_skid, src_dis.extrn_name, src_dis.ATTR_CODE_2, src_dis.ATTR_CODE_LIST, \\nsubstr(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\s)'),1, LENGTH(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\s)'))-1) AS PARENT_ATTR_CODE_LIST\\nFROM src_dis \\n)\\nSELECT ('No parent in product hierarchy') DQ,src_parent.mkt_extrn_code as mkt_extrn_code, src_parent.time_extrn_code as time_extrn_code, src_parent.prod_extrn_code as prod_extrn_code, src_parent.ATTR_CODE_LIST as attr_code_list,\\nsrc_parent.prod_skid as prod_skid, src_parent.extrn_name as extrn_name\\nFROM src_parent LEFT JOIN src_dis \\nON PARENT_ATTR_CODE_LIST = src_dis.ATTR_CODE_LIST\\nAND src_parent.mkt_extrn_code = src_dis.mkt_extrn_code\\nAND src_parent.time_extrn_code = src_dis.time_extrn_code\\nWHERE src_dis.MKT_EXTRN_CODE IS NULL\\nAND src_parent.ATTR_CODE_2 IS NOT NULL LIMIT 199\\\"\\\"\\\"\\ndf_nopr_prod_hier = spark.sql(query3)\\n\\n# Duplicated product after mapping\\n\\nquery4 = f\\\"\\\"\\\"WITH data AS (\\nSELECT src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST <> xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST AND src.extrn_code <> xref.extrn_prod_id\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\n),\\nskid AS (\\nSELECT prod_skid FROM data \\nGROUP BY prod_skid\\nHAVING COUNT(DISTINCT line_num)>1\\n)\\nSELECT ('Duplicated product after mapping') DQ,line_num as line_num, vendor_tag as vendor_tag, prod_codes_new as prod_codes_new, prod_skid as prod_skid, prod_name as prod_name FROM (SELECT DISTINCT line_num, extrn_code vendor_tag, EXTRN_PROD_ATTR_VAL_LIST prod_codes_new, data.PROD_SKID, PROD_NAME FROM data\\n   JOIN skid on data.prod_skid = skid.prod_skid) ORDER BY line_num LIMIT 500\\\"\\\"\\\"\\ndf_dup_prod_map = spark.sql(query4)\\n\\n# External code change\\n\\nquery5 = f\\\"\\\"\\\"WITH src AS (SELECT * FROM tier1_prod_mtrlz_tbl),\\n  lvl AS (SELECT * FROM mm_categ_strct_attr_assoc_vw),\\n  xref AS (SELECT  extrn_prod_id, EXTRN_PROD_ATTR_VAL_LIST, prod_skid \\n\\t         FROM mm_prod_xref WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id}),\\n  sd AS (SELECT  prod_skid, prod_lvl_name, prod_lvl_id \\n\\t       from mm_prod_sdim WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id})\\nSELECT ('External code change') DQ,extrn_prod_id as extrn_prod_id, vendor_tag_old as vendor_tag_old, prod_codes_new as prod_codes_new, attribute_level_new as attribute_level_new, prod_codes_old as prod_codes_old, attribute_level_old as attribute_level_old FROM (SELECT \\n   xref.extrn_prod_id,\\n   src.extrn_prod_id AS vendor_tag_old,\\n       src.EXTRN_PROD_ATTR_VAL_LIST prod_codes_new,\\n       lvl.attr_name attribute_level_new,\\n       xref.EXTRN_PROD_ATTR_VAL_LIST prod_codes_old,\\n       sd.prod_lvl_name attribute_level_old\\n    FROM src JOIN lvl\\n          ON (    src.ATTR_CODE_1 = lvl.CATEG_ID\\n             AND src.ATTR_CODE_0 = CAST (lvl.STRCT_NUM as string)\\n             AND src.LVL_NUM = lvl.LVL_NUM\\n          )\\n       JOIN xref ON xref.extrn_prod_id = src.extrn_prod_id \\n       JOIN sd ON sd.prod_skid = xref.prod_skid\\n       JOIN MM_STRCT_LVL_LKP lvl_dim ON sd.prod_lvl_id= lvl_dim.strct_lvl_id\\n     WHERE     src.EXTRN_PROD_ATTR_VAL_LIST != xref.EXTRN_PROD_ATTR_VAL_LIST\\n       AND lvl.lvl_num != lvl_dim.lvl_num) LIMIT 500\\\"\\\"\\\"\\ndf_ext_code_chng = spark.sql(query5)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\n\\ndq1 = df_unk_hier_prod\\ndq2 = df_attr_prod_hier\\ndq3 = df_nopr_prod_hier\\ndq4 = df_dup_prod_map\\ndq5 = df_ext_code_chng\\n\\n# Combine the dataframes\\ndf_combine = dq1.unionByName(dq2, True).unionByName(dq3, True).unionByName(dq4, True).unionByName(dq4, True).unionByName(dq5, True)\\n\\ndata = []\\n\\ndq1_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'Unknown hierarchy for product' \\\", '', 'false', 'Unknown hierarchy for product', 100 )\\ndata.append(dq1_val)\\ndq2_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'Too many attributes for product hierarchy' \\\", '', 'false', 'Too many attributes for product hierarchy', 100 )\\ndata.append(dq2_val)\\ndq3_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'No parent in product hierarchy' \\\", '', 'false', 'No parent in product hierarchy', 100 )\\ndata.append(dq3_val)\\ndq4_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'Duplicated product after mapping' \\\", '', 'false', 'Duplicated product after mapping', 100 )\\ndata.append(dq4_val)\\ndq5_val = ('DQ', 'SQL Validation KPI', \\\"DQ <> 'External code change' \\\", '', 'false', 'External code change', 100 )\\ndata.append(dq5_val)\\n\\n\\n#Prepare KPI\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"column\\\",StringType(),True),\\n    StructField(\\\"kpi_type\\\",StringType(),True),\\n    StructField(\\\"param_1\\\",StringType(),True),\\n    StructField(\\\"param_2\\\",StringType(),True),\\n    StructField(\\\"fail_on_error\\\",StringType(),True),\\n    StructField(\\\"check_description\\\",StringType(),True),\\n    StructField(\\\"target\\\",StringType(),True)\\n  ])\\n\\ndf_ref_data_vendors = spark.createDataFrame(data, schema_for_kpi)\\n\\n\\ndict_all_dfs['df_ref_data_vendors'] = {\\\"df_object\\\" :df_ref_data_vendors}\\ndf_output_dict['df_ref_data_vendors'] = df_ref_data_vendors\\n\\ndict_all_dfs['df_combine_ref_data_vendors'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_ref_data_vendors'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier1_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_prod_gav\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "[FL]MM_PROD_SDIM_VW",
      "operationDescription": "18-dec-23 --changed this step to load prod sdim from part_srce_sys_id",
      "predecessorName": "Reference Data Vendors - Validations - Update 1",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"fileType\": \"parquet\",\n  \"inferSchema\": \"false\",\n  \"path\": \"refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROD_SDIM_VW/part_srce_sys_id=<<SRCE_SYS_ID>>/\",\n  \"addInputFileName\": \"false\",\n  \"semaphoreOption\": \"shared\",\n  \"createIfNotExist\": \"false\",\n  \"mergeSchema\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_prod_sdim_vw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": false
    },
    {
      "operationName": "release mm_prod_sdim",
      "operationDescription": "18-dec-23 --changed this step to release semaphore prod sdim from part_srce_sys_id",
      "predecessorName": "[FL]MM_PROD_SDIM_VW",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"actionType\": \"release\",\n  \"itemType\": \"path\",\n  \"itemPath\": \"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROD_SDIM_VW/part_srce_sys_id=<<SRCE_SYS_ID>>/\"\n}",
      "operationVersionName": "SemaphoreOperation",
      "overridableIndicator": false
    },
    {
      "operationName": "[cet] add part srce sys id",
      "operationDescription": "18-dec-23 --new step to add part_srce_sys_id partition column",
      "predecessorName": "release mm_prod_sdim",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"AddAllSourceColumns\": \"true\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_prod_sdim_vw\"\n    }\n  ],\n  \"transformations\": [\n    {\n      \"transformation\": \"round(srce_sys_id,0)\",\n      \"columnName\": \"part_srce_sys_id\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_prod_sdim_vw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "ColumnExpressionTransformation",
      "overridableIndicator": false
    },
    {
      "operationName": "[Gen] cc mm_prod_sdim",
      "operationDescription": "18-dec-23 --new step to cc mm_prod_sdim with schema",
      "predecessorName": "[cet] add part srce sys id",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_prod_sdim_vw = dict_all_dfs['df_prod_sdim_vw'][\\\"df_object\\\"]\\ndf_mm_prod_sdim_vw_schema = dict_all_dfs['df_mm_prod_sdim_vw_schema'][\\\"df_object\\\"]\\n\\nfrom pyspark.sql.functions import col\\n\\nlkp_cols = df_prod_sdim_vw.columns\\nsdim_cols = df_mm_prod_sdim_vw_schema.columns\\n \\nfrom pyspark.sql.functions import lit\\nadd_cols = list(set(sdim_cols)-set(lkp_cols))\\nfor i in add_cols:\\n  df_prod_sdim_vw = df_prod_sdim_vw.withColumn(i,lit(None).cast('string'))\\n\\ndf_prod_sdim_vw = df_prod_sdim_vw.select(*sdim_cols)\\ncols = df_prod_sdim_vw.columns\\n\\nfor j in cols:\\n  if dict(df_prod_sdim_vw.dtypes)[j] != dict(df_mm_prod_sdim_vw_schema.dtypes)[j]:\\n    df_prod_sdim_vw = df_prod_sdim_vw.withColumn(j, col(j).cast(dict(df_mm_prod_sdim_vw_schema.dtypes)[j]))\\n\\ndict_all_dfs['df_prod_sdim_vw'] = {\\\"df_object\\\" :df_prod_sdim_vw}\\ndf_output_dict['df_prod_sdim_vw'] = df_prod_sdim_vw\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_prod_sdim_vw\"\n    },\n    {\n      \"name\": \"df_mm_prod_sdim_vw_schema\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_prod_sdim_vw\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "[fl] mm_prod_xref",
      "predecessorName": "[Gen] cc mm_prod_sdim",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"fileType\": \"parquet\",\n  \"inferSchema\": \"false\",\n  \"path\": \"refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROD_XREF/\",\n  \"addInputFileName\": \"false\",\n  \"semaphoreOption\": \"shared\",\n  \"createIfNotExist\": \"false\",\n  \"mergeSchema\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_prod_xref\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "FileLoaderTabular",
      "overridableIndicator": false
    },
    {
      "operationName": "[semaphore] to release mm_prod_xref",
      "predecessorName": "[fl] mm_prod_xref",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"actionType\": \"release\",\n  \"itemType\": \"path\",\n  \"itemPath\": \"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROD_XREF/\"\n}",
      "operationVersionName": "SemaphoreOperation",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference Data Vendors - Validations and Report Generation",
      "predecessorName": "[semaphore] to release mm_prod_xref",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf_file_struct_summary = dict_all_dfs['df_file_struct_summary'][\\\"df_object\\\"]\\n\\n#Variables\\n\\ntier1_vendr_id = <<VENDOR_ID>>\\ntier1_srce_sys_id = <<SRCE_SYS_ID>>\\ntier1_fact_type_code = 'TP'\\ntier1_cntrt_id = <<CNTRT_ID>>\\ntier1_cntry_id = '<<ISO_CRNCY_CODE>>'\\n\\npath = 'refined/NNIT/tradepanel/prod-tp-lightrefined/'\\n\\n# Dataframes from Prior Steps\\n\\n\\ntier1_prod_mtrlz_tbl = dict_all_dfs['tier1_prod_mtrlz_tbl'][\\\"df_object\\\"]\\ndf_prod_sdim_vw = dict_all_dfs['df_prod_sdim_vw'][\\\"df_object\\\"]\\ndf_prod_xref = dict_all_dfs['df_prod_xref'][\\\"df_object\\\"]\\ntier1_fact_mtrlz_tbl = dict_all_dfs['tier1_fact_mtrlz_tbl'][\\\"df_object\\\"]\\ntier1_prod_gav = dict_all_dfs['tier1_prod_gav'][\\\"df_object\\\"]\\n\\ntier1_prod_mtrlz_tbl.createOrReplaceTempView('tier1_prod_mtrlz_tbl')\\ntier1_fact_mtrlz_tbl.createOrReplaceTempView('tier1_fact_mtrlz_tbl')\\ntier1_prod_gav.createOrReplaceTempView('tier1_prod_gav')\\n\\n#mm_prod_xref = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_XREF/')\\ndf_prod_xref.createOrReplaceTempView('mm_prod_xref')\\n\\n#mm_prod_sdim = spark.read.format('parquet').load(f'/mnt/{path}/MM_PROD_SDIM_VW/')\\ndf_prod_sdim_vw.createOrReplaceTempView('mm_prod_sdim')\\n\\n\\n# tables from Postgres\\n\\n\\n\\n# mm_strct_lkp\\nmm_strct_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LKP\\\")\\nmm_strct_lkp.createOrReplaceTempView('mm_strct_lkp')\\n\\n# mm_strct_lvl_lkp\\nmm_strct_lvl_lkp = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_STRCT_LVL_LKP\\\")\\nmm_strct_lvl_lkp.createOrReplaceTempView('mm_strct_lvl_lkp')\\n\\n# mm_categ_strct_assoc\\nmm_categ_strct_assoc = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ASSOC\\\")\\nmm_categ_strct_assoc.createOrReplaceTempView('mm_categ_strct_assoc')\\n\\n# mm_categ_strct_attr_assoc_vw\\nmm_categ_strct_attr_assoc_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_CATEG_STRCT_ATTR_ASSOC_VW\\\")\\nmm_categ_strct_attr_assoc_vw.createOrReplaceTempView('mm_categ_strct_attr_assoc_vw')\\n\\n#UDFs\\n\\n# Ignore Bad ASCII Symbols\\nspark.sql('''create or replace TEMPORARY FUNCTION FN_GET_MAX_LVL_NUM (\\nstrct_id int ) RETURNs int deterministic RETURN \\nSELECT MAX (lvl_num)\\n                              FROM mm_strct_lvl_lkp lvl\\n                             WHERE lvl.strct_id = strct_id''')\\n\\n\\n#Validations\\n\\n#Unknown hierarchy for product\\n\\nquery1 = f\\\"\\\"\\\"select ('Unknown hierarchy for product') DQ, tier1_prod_mtrlz_tbl.line_num as line_num, tier1_prod_mtrlz_tbl.extrn_code as extrn_code, tier1_prod_mtrlz_tbl.extrn_name as extrn_name, tier1_prod_mtrlz_tbl.attr_code_list as attr_code_list, tier1_prod_mtrlz_tbl.attr_code_0 as invalid_hier_num\\n  from tier1_prod_mtrlz_tbl \\n  left join mm_categ_strct_assoc on tier1_prod_mtrlz_tbl.attr_code_1=mm_categ_strct_assoc.categ_id \\n  and tier1_prod_mtrlz_tbl.attr_code_0 =  mm_categ_strct_assoc.strct_num\\n  where (mm_categ_strct_assoc.strct_num is null and strct_id is null)  \\n  order by line_num \\n  limit 100\\\"\\\"\\\"\\ndf_unk_hier_prod = spark.sql(query1)\\n\\n# Too many attributes for product hierarchy\\nquery2 = f\\\"\\\"\\\"WITH prod AS (SELECT line_num, extrn_name, extrn_code, attr_code_list, lvl_num, attr_code_0, attr_Code_1 FROM tier1_prod_mtrlz_tbl)\\nSELECT ('Too many attributes for product hierarchy') DQ, attr_code_1 as attr_code_1, lvl_num as lvl_num,attr_code_list as attr_code_list,line_num as line_num, extrn_code as extrn_code, extrn_name as extrn_name, strct_id as strct_id, dmnsn_id as dmnsn_id, strct_code as strct_code, strct_name as strct_name, strct_desc as strct_desc   FROM (SELECT *\\n  FROM prod JOIN mm_strct_lkp strct\\n\\t\\t\\t\\tON prod.attr_Code_1||'_H'||prod.attr_code_0 = strct.strct_code and strct.dmnsn_id=2\\n\\t\\t\\t\\t\\t AND prod.lvl_num > FN_GET_MAX_LVL_NUM(strct.strct_id))\\n  ORDER BY line_num\\n  LIMIT 100\\\"\\\"\\\"\\n\\ndf_attr_prod_hier = spark.sql(query2)\\n\\n# No parent in product hierarchy\\nquery3 = f\\\"\\\"\\\"WITH src0 AS (\\nSELECT prod.ATTR_CODE_2, prod.ATTR_CODE_LIST, prod.prod_skid, prod.extrn_name, fct.prod_extrn_code, fct.time_extrn_code, fct.MKT_EXTRN_CODE\\nFROM tier1_fact_mtrlz_tbl fct JOIN tier1_prod_gav prod\\n  ON fct.prod_extrn_code = prod.extrn_code),\\nsrc_dis AS (\\nSELECT DISTINCT mkt_extrn_code, prod_extrn_code, time_extrn_code, prod_skid, extrn_name, src0.ATTR_CODE_2, src0.ATTR_CODE_LIST FROM src0 \\n),\\nsrc_parent AS (\\nSELECT  mkt_extrn_code, time_extrn_code, prod_extrn_code, src_dis.prod_skid, src_dis.extrn_name, src_dis.ATTR_CODE_2, src_dis.ATTR_CODE_LIST, \\nsubstr(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\\\\\\\\\s)'),1, LENGTH(regexp_extract(src_dis.ATTR_CODE_LIST, '(^.+\\\\\\\\\\\\s)'))-1) AS PARENT_ATTR_CODE_LIST\\nFROM src_dis \\n)\\nSELECT ('No parent in product hierarchy') DQ,src_parent.mkt_extrn_code as mkt_extrn_code, src_parent.time_extrn_code as time_extrn_code, src_parent.prod_extrn_code as prod_extrn_code, src_parent.ATTR_CODE_LIST as attr_code_list,\\nsrc_parent.prod_skid as prod_skid, src_parent.extrn_name as extrn_name\\nFROM src_parent LEFT JOIN src_dis \\nON PARENT_ATTR_CODE_LIST = src_dis.ATTR_CODE_LIST\\nAND src_parent.mkt_extrn_code = src_dis.mkt_extrn_code\\nAND src_parent.time_extrn_code = src_dis.time_extrn_code\\nWHERE src_dis.MKT_EXTRN_CODE IS NULL\\nAND (src_parent.ATTR_CODE_2 IS NOT NULL and src_parent.ATTR_CODE_2 <> '')\\\"\\\"\\\"\\ndf_nopr_prod_hier = spark.sql(query3)\\n\\n# Duplicated product after mapping\\n\\nquery4 = f\\\"\\\"\\\"WITH data AS (\\nSELECT src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.extrn_code = xref.extrn_prod_id AND src.EXTRN_PROD_ATTR_VAL_LIST <> xref.EXTRN_PROD_ATTR_VAL_LIST\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\nUNION ALL\\nSELECT  src.line_num, src.extrn_code, src.EXTRN_PROD_ATTR_VAL_LIST, xref.EXTRN_PROD_ID, xref.prod_skid, src.prod_name\\nFROM tier1_prod_gav src JOIN mm_prod_xref xref ON \\nsrc.EXTRN_PROD_ATTR_VAL_LIST = xref.EXTRN_PROD_ATTR_VAL_LIST AND src.extrn_code <> xref.extrn_prod_id\\nAND xref.srce_sys_id = 3 \\nAND xref.cntrt_id = {tier1_cntrt_id} \\nAND src.lvl_num < 9\\nAND src.PROD_LVL_NAME <> 'ITEM'\\n),\\nskid AS (\\nSELECT prod_skid FROM data \\nGROUP BY prod_skid\\nHAVING COUNT(DISTINCT line_num)>1\\n)\\nSELECT ('Duplicated product after mapping') DQ,line_num as line_num, vendor_tag as vendor_tag, prod_codes_new as prod_codes_new, prod_skid as prod_skid, prod_name as prod_name FROM (SELECT DISTINCT line_num, extrn_code vendor_tag, EXTRN_PROD_ATTR_VAL_LIST prod_codes_new, data.PROD_SKID, PROD_NAME FROM data\\n   JOIN skid on data.prod_skid = skid.prod_skid) ORDER BY line_num LIMIT 500\\\"\\\"\\\"\\ndf_dup_prod_map = spark.sql(query4)\\n\\n# External code change\\n\\nquery5 = f\\\"\\\"\\\"WITH src AS (SELECT * FROM tier1_prod_mtrlz_tbl),\\n  lvl AS (SELECT * FROM mm_categ_strct_attr_assoc_vw),\\n  xref AS (SELECT  extrn_prod_id, EXTRN_PROD_ATTR_VAL_LIST, prod_skid \\n\\t         FROM mm_prod_xref WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id}),\\n  sd AS (SELECT  prod_skid, prod_lvl_name, prod_lvl_id \\n\\t       from mm_prod_sdim WHERE cntrt_id = {tier1_cntrt_id} and srce_sys_id = {tier1_srce_sys_id})\\nSELECT ('External code change') DQ,extrn_prod_id as extrn_prod_id, vendor_tag_old as vendor_tag_old, prod_codes_new as prod_codes_new, attribute_level_new as attribute_level_new, prod_codes_old as prod_codes_old, attribute_level_old as attribute_level_old FROM (SELECT \\n   xref.extrn_prod_id,\\n   src.extrn_prod_id AS vendor_tag_old,\\n       src.EXTRN_PROD_ATTR_VAL_LIST prod_codes_new,\\n       lvl.attr_name attribute_level_new,\\n       xref.EXTRN_PROD_ATTR_VAL_LIST prod_codes_old,\\n       sd.prod_lvl_name attribute_level_old\\n    FROM src JOIN lvl\\n          ON (    src.ATTR_CODE_1 = lvl.CATEG_ID\\n             AND src.ATTR_CODE_0 = CAST (lvl.STRCT_NUM as string)\\n             AND src.LVL_NUM = lvl.LVL_NUM\\n          )\\n       JOIN xref ON xref.extrn_prod_id = src.extrn_prod_id \\n       JOIN sd ON sd.prod_skid = xref.prod_skid\\n       JOIN MM_STRCT_LVL_LKP lvl_dim ON sd.prod_lvl_id= lvl_dim.strct_lvl_id\\n     WHERE     src.EXTRN_PROD_ATTR_VAL_LIST != xref.EXTRN_PROD_ATTR_VAL_LIST\\n       AND lvl.lvl_num != lvl_dim.lvl_num) LIMIT 500\\\"\\\"\\\"\\ndf_ext_code_chng = spark.sql(query5)\\n\\n# Combine\\nfrom pyspark.sql.functions import lit, row_number, monotonically_increasing_id, col, when\\nfrom pyspark.sql.window import Window\\nfrom pyspark.sql.types import *\\n\\n\\ndq1 = df_unk_hier_prod\\ndq2 = df_attr_prod_hier\\ndq3 = df_nopr_prod_hier\\ndq4 = df_dup_prod_map\\ndq5 = df_ext_code_chng\\n\\n# Combine the dataframes\\ndf_combine = dq1.unionByName(dq2, True).unionByName(dq3, True).unionByName(dq4, True).unionByName(dq4, True).unionByName(dq5, True)\\n\\n#Report Generation\\n\\nimport subprocess\\nimport os\\nimport shutil\\nimport subprocess\\nimport sys\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\nimport pandas as pd\\nsubprocess.check_call([sys.executable,\\\"-m\\\",\\\"pip\\\",\\\"install\\\",\\\"xlsxwriter\\\"])\\nsubprocess.check_call([sys.executable,\\\"-m\\\",\\\"pip\\\",\\\"install\\\",\\\"openpyxl\\\"])\\nimport xlsxwriter\\nimport openpyxl\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\nrpt_path = 'refined/NNIT/tradepanel/prod-tp-lightrefined/'\\n\\nwriter = pd.ExcelWriter(f'tp_dvm_rprt_{run_id}.xlsx', engine='xlsxwriter')\\n\\n# Prepare KPI\\ndata = []\\n\\ndq_ref_vendor_val = ('Reference Data Vendors Validations','','')\\ndata.append(dq_ref_vendor_val)\\n\\nif dq1.count()==0:\\n  dq_val=('Unknown hierarchy for product', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Unknown hierarchy for product', 'FAILED', '=HYPERLINK(\\\"#REFV_VAL1!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq2.count()==0:\\n  dq_val=('Too many attributes for product hierarchy', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Too many attributes for product hierarchy', 'FAILED', '=HYPERLINK(\\\"#REFV_VAL2!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\nif dq3.count()==0:\\n  dq_val=('No parent in product hierarchy', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('No parent in product hierarchy', 'FAILED', '=HYPERLINK(\\\"#REFV_VAL3!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\n\\nif dq4.count()==0:\\n  dq_val=('Duplicated product after mapping', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('Duplicated product after mapping', 'FAILED', '=HYPERLINK(\\\"#REFV_VAL4!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\n  \\nif dq5.count()==0:\\n  dq_val=('External code change', 'PASSED', '' )\\n  data.append(dq_val)\\nelse:\\n  dq_val=('External code change', 'FAILED', '=HYPERLINK(\\\"#REFV_VAL5!A1\\\",\\\"click_here\\\")' )\\n  data.append(dq_val)\\n\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\n#Prepare Summary Report dataframe\\n\\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType\\n\\nschema_for_kpi = StructType([ \\n    StructField(\\\"Validation\\\",StringType(),True),\\n    StructField(\\\"Result\\\",StringType(),True),\\n    StructField(\\\"Details\\\",StringType(),True)\\n  ])\\n  \\n# Creation of Summary Tab\\n\\ndf_ref_vendors_summary = spark.createDataFrame(data, schema_for_kpi)\\ndf_ref_vendors_summary = df_ref_vendors_summary.orderBy('Result')\\nsummary = df_file_struct_summary.unionByName(df_ref_vendors_summary, True)\\n\\nSUMMARY = summary.toPandas()\\nSUMMARY.to_excel(writer,sheet_name=\\\"SUMMARY\\\",index=False)\\n\\n# Creation of other tabs\\n\\nlst_dfs = [dq1, dq2, dq3, dq4, dq5]\\nc = 1\\nfor i in lst_dfs:\\n  if i.count()>0:\\n    i = i.toPandas()\\n    i.to_excel(writer,sheet_name=f\\\"REFV_VAL{c}\\\",index=False)\\n  c= c+1\\n\\n# Close Excel Report and Save\\nwriter.close()\\n\\nfiles = [f for f in os.listdir('.') if os.path.isfile(f)]\\nfor f in files:\\n  if f==f'tp_dvm_rprt_{run_id}.xlsx':\\n    shutil.copyfile(f, f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}.xlsx')\\n\\n# Read and combine the report\\nwriter = pd.ExcelWriter(f'tp_dvm_rprt_{run_id}_summary.xlsx', engine='xlsxwriter')\\n\\n# Creation of Summary Tab Once again\\n\\ndf_file_struct_summary = df_file_struct_summary.orderBy('Result')\\ndf_ref_vendors_summary = df_ref_vendors_summary.orderBy('Result')\\nsummary = df_file_struct_summary.unionByName(df_ref_vendors_summary, True)\\n\\nSUMMARY = summary.toPandas()\\nSUMMARY.to_excel(writer,sheet_name=\\\"SUMMARY\\\",index=False)\\n\\n#Read Summary Report\\ndf_rpt_summary = pd.read_excel(f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}_summary.xlsx', sheet_name=None)\\n\\nfor key, value in df_rpt_summary.items():\\n  if (key!='SUMMARY'):\\n    df_i = df_rpt_summary[key]\\n    df_i.to_excel(writer,sheet_name=f\\\"{key}\\\",index=False)\\n\\n#Read Current report\\ndf_rpt_curr = pd.read_excel(f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}.xlsx', sheet_name=None)\\n\\nfor key, value in df_rpt_curr.items():\\n  if (key!='SUMMARY'):\\n    df_i = df_rpt_curr[key]\\n    df_i.to_excel(writer,sheet_name=f\\\"{key}\\\",index=False)\\n\\nwriter.close()\\n\\nfiles = [f for f in os.listdir('.') if os.path.isfile(f)]\\nfor f in files:\\n  if f==f'tp_dvm_rprt_{run_id}_summary.xlsx':\\n    shutil.move(f, f'/dbfs/mnt/{rpt_path}/tp_dvm_rpt/tp_dvm_rprt_{run_id}_summary.xlsx')\\n\\ndict_all_dfs['df_ref_vendors_summary'] = {\\\"df_object\\\" :df_ref_vendors_summary}\\ndf_output_dict['df_ref_vendors_summary'] = df_ref_vendors_summary\\n\\ndict_all_dfs['df_combine_ref_data_vendors'] = {\\\"df_object\\\" :df_combine}\\ndf_output_dict['df_combine_ref_data_vendors'] = df_combine\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"tier1_prod_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_fact_mtrlz_tbl\"\n    },\n    {\n      \"name\": \"tier1_prod_gav\"\n    },\n    {\n      \"name\": \"df_file_struct_summary\"\n    },\n    {\n      \"name\": \"df_prod_sdim_vw\"\n    },\n    {\n      \"name\": \"df_prod_xref\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_ref_vendors_summary\",\n      \"cache\": \"materialize\"\n    },\n    {\n      \"name\": \"df_combine_ref_data_vendors\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Update Delivery Details - Precommand",
      "predecessorName": "Reference Data Vendors - Validations and Report Generation",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\ndf_ref_vendors_summary = dict_all_dfs['df_ref_vendors_summary'][\\\"df_object\\\"]\\nref_data_vendors_fails = df_ref_vendors_summary.filter(\\\" Result = 'FAILED' \\\").count()\\n\\n\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\nif (ref_data_vendors_fails == 0):\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 4, 1, 2, <<PROCESS_RUN_KEY>>]]\\nelse:\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 10, 1, 4, <<PROCESS_RUN_KEY>>]]\\n\\ncolumns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\njdbcDF2 = spark_session.createDataFrame(rows, columns)\\njdbcDF2.write.format(\\\"parquet\\\").mode(\\\"append\\\").save(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-refined/MM_DLVRY_RUN_LKP\\\")\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_ref_vendors_summary\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_ref_vendors_summary\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Mail Sender",
      "predecessorName": "Update Delivery Details - Precommand",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\nfrom pyspark.sql.functions import *\\n\\n\\n# Run information from Postgres\\nrun_id = <<PROCESS_RUN_KEY>>\\n\\n\\n\\n# mm_process_run_lkp_vw\\nmm_process_run_lkp_vw = spark.read.format(\\\"parquet\\\").load(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-lightrefined/MM_PROCESS_RUN_LKP_VW where run_id = {run_id}\\\")\\nmm_process_run_lkp_vw.createOrReplaceTempView('mm_process_run_lkp_vw')\\n\\nmm_process_run_lkp_vw = mm_process_run_lkp_vw.select('file_name', 'start_date_time')\\nrun_info = mm_process_run_lkp_vw.collect()\\n\\n\\n#Mail Sender\\n\\nimport smtplib\\nfrom email.mime.text import MIMEText\\nfrom email.mime.multipart import MIMEMultipart\\nfrom email.mime.base import MIMEBase\\nfrom email import encoders\\n# Setup port number and server name\\n\\nsmtp_port = 587                 # Standard secure SMTP port\\nsmtp_server = \\\"smtp.office365.com\\\"  # Google SMTP Server\\n\\ncontacts = '<<CONTACTS>>'\\n#lst_of_contacts = [i.strip() for i in contacts.split(\\\";\\\")]\\n#lst_of_contacts = [\\\"michalska.mb@pg.com\\\", \\\"ulbrych.b@pg.com\\\"]\\nlst_of_contacts = [\\\"michalska.mb@pg.com\\\", \\\"ulbrych.b@pg.com\\\", \\\"kumtepe.o@pg.com\\\"]\\n\\n# Set up the email lists\\nemail_from = \\\"cpnotification.im@pg.com\\\"\\n\\n#email_list = [\\\"Gopi.C@lntinfotech.com\\\", \\\"jagdish.sahu@lntinfotech.com\\\", \\\"poltoratskyi.i@pg.com\\\"]\\nemail_list = [\\\"Gopi.C@lntinfotech.com\\\"]\\n\\nfor c in lst_of_contacts:\\n  email_list.append(c)\\n\\npswd = dbutils.secrets.get('tp_dpf2cdl', 'cpnotification-password')\\n\\nrun_id = <<PROCESS_RUN_KEY>>\\nfile_name = run_info[0]['file_name']\\nfile_timestamp= run_info[0]['start_date_time']\\nreport_name = f\\\"tp_dvm_rprt_{run_id}_summary.xlsx\\\"\\nrpt_path = 'refined/NNIT/tradepanel/prod-tp-lightrefined/tp_dvm_rpt/'\\n\\n# name the email subject\\nsubject = f\\\"TP Reference Data Vendors DVM Validation Report for run {run_id}\\\"\\n\\n# Define the email function (dont call it email!)\\ndef send_emails(email_list):\\n\\n    for person in email_list:\\n\\n        # Make the body of the email\\n        body = f\\\"\\\"\\\"\\n\\nValidation summary:\\n\\nSource File Name: {file_name} \\n\\\\nRun id: {run_id} \\n\\nPlease find the attachments for detailed validation report\\n\\nRegards,\\nTradepanel Team    \\n        \\\"\\\"\\\"\\n        print(body)\\n        # make a MIME object to define parts of the email\\n        msg = MIMEMultipart()\\n        msg['From'] = email_from\\n        msg['To'] = person\\n        msg['Subject'] = subject\\n\\n        # Attach the body of the message\\n        msg.attach(MIMEText(body, 'plain'))\\n\\n        # Define the file to attach\\n        filename_path = f\\\"/dbfs/mnt/{rpt_path}/tp_dvm_rprt_{run_id}_summary.xlsx\\\"\\n\\n        # Open the file in python as a binary\\n        attachment= open(filename_path, 'rb')  # r for read and b for binary\\n\\n        # Encode as base 64\\n        attachment_package = MIMEBase('application', 'octet-stream')\\n        attachment_package.set_payload((attachment).read())\\n        encoders.encode_base64(attachment_package)\\n        attachment_package.add_header('Content-Disposition', \\\"attachment; filename= \\\" + report_name)\\n        msg.attach(attachment_package)\\n\\n        # Cast as string\\n        text = msg.as_string()\\n\\n        # Connect with the server\\n        print(\\\"Connecting to server...\\\")\\n        TIE_server = smtplib.SMTP(smtp_server, smtp_port)\\n        TIE_server.starttls()\\n        TIE_server.login(email_from, pswd)\\n        print(\\\"Succesfully connected to server\\\")\\n        print()\\n\\n\\n        # Send emails to \\\"person\\\" as list is iterated\\n        print(f\\\"Sending email to: {person}...\\\")\\n        TIE_server.sendmail(email_from, person, text)\\n        print(f\\\"Email sent to: {person}\\\")\\n        print()\\n\\n    # Close the port\\n    TIE_server.quit()\\n\\n\\n# Run the function\\nsend_emails(email_list)\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_ref_vendors_summary\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_ref_vendors_summary\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Stop the process If Reference Data Validation Fails",
      "predecessorName": "Mail Sender",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"expression\": \"DQ is not Null\",\n  \"processStatus\": \"DQ_ISSUE\",\n  \"conditionValue\": \"true\",\n  \"milestone\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data_vendors\"\n    }\n  ]\n}",
      "operationVersionName": "ConditionalStop",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference Data Vendors Checks Eligible KPIs",
      "predecessorName": "Stop the process If Reference Data Validation Fails",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"semaphoreOption\": \"none\",\n  \"format\": \"csv\",\n  \"disableSuccessFile\": \"false\",\n  \"shouldDeleteSuccess\": \"false\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_vendors.csv\",\n  \"mode\": \"overwrite\",\n  \"compression\": \"None\",\n  \"coalesceByNumber\": 1,\n  \"repartitionByColumn\": [],\n  \"columnToDrop\": [],\n  \"partitionByColumn\": [],\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_ref_data_vendors\"\n    }\n  ]\n}",
      "operationVersionName": "FilePublisher",
      "overridableIndicator": false
    },
    {
      "operationName": "Reference Data Vendors - DQ Check",
      "predecessorName": "Reference Data Vendors Checks Eligible KPIs",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"inputType\": \"Input using uploaded file\",\n  \"path\": \"bf/unrefined/adw-reference-bf/KPI/<<PROCESS_RUN_KEY>>_vendors.csv\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data_vendors\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data_vendors_chk\",\n      \"cache\": \"materialize\"\n    }\n  ]\n}",
      "operationVersionName": "DataQualityValidation",
      "overridableIndicator": false
    },
    {
      "operationName": "Update Delivery Details - post command",
      "predecessorName": "Reference Data Vendors - DQ Check",
      "jsonSpecification": "{\n  \"active\": \"true\",\n  \"separateSparkSession\": \"false\",\n  \"milestone\": \"false\",\n  \"saveOutputDfsToTempTable\": \"false\",\n  \"customCode\": \"spark = self.spark_session\\nmyLogger = self.log\\n\\nfrom IPython import get_ipython\\ndbutils = get_ipython().user_ns[\\\"dbutils\\\"]\\n\\ndf = dict_all_dfs['df_combine_ref_data_vendors'][\\\"df_object\\\"]\\n\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.types import *\\n\\n\\n\\nspark_session = SparkSession.builder.appName('Spark_Session').getOrCreate()\\n\\ncnt = df.count()\\n\\nif (cnt>0):\\n  rows = [['null', <<CNTRT_ID>>, <<PROCESS_RUN_KEY>>, 10, 1, 4, <<PROCESS_RUN_KEY>>]]\\n  columns = ['cmmnt_txt', 'cntrt_id', 'dlvry_id', 'dlvry_phase_id', 'dlvry_run_seq_num', 'dlvry_sttus_id', 'run_id']\\n  jdbcDF2 = spark_session.createDataFrame(rows, columns)\\n  jdbcDF2.write.format(\\\"parquet\\\").mode(\\\"append\\\").save(\\\"/mnt/refined/NNIT/tradepanel/prod-tp-refined/MM_DLVRY_RUN_LKP\\\")\",\n  \"inputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data_vendors\"\n    }\n  ],\n  \"outputDataframes\": [\n    {\n      \"name\": \"df_combine_ref_data_vendors\",\n      \"cache\": \"none\"\n    }\n  ]\n}",
      "operationVersionName": "Generic",
      "overridableIndicator": false
    },
    {
      "operationName": "Report Generation",
      "predecessorName": "Update Delivery Details - post command",
      "jsonSpecification": "{\n  \"active\": \"false\",\n  \"documentation\": \"https://jira-pg-ds.atlassian.net/wiki/spaces/CDLBOK/pages/3676897284/Turbine+DQ+Operations\",\n  \"saveToCSV\": \"true\",\n  \"generateHTMLReport\": \"true\",\n  \"generatePDFReport\": \"false\",\n  \"includeDetailedValidationResults\": \"failed rows only\",\n  \"numberOfRowsToDisplay\": 100,\n  \"reportTemplate\": \"default\"\n}",
      "operationVersionName": "DataQualityReport",
      "overridableIndicator": false
    }
  ],
  "graphName": "NNIT_t1_dq_ref_data_vendors_validation"
}